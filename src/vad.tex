\ifsolo
    ~

    \vspace{1cm}

    \begin{center}
        \textbf{\LARGE Variables aléatoires discrètes} \\[1em]
    \end{center}
    \tableofcontents
\else
    \chapter{Variables aléatoires discrètes}

    \minitoc
\fi
\thispagestyle{empty}

\section{Tribu}
\subsection{Définition}

Dans tout le chapitre, $\Omega$ désigne un ensemble non vide.

\begin{dfn}
    Un ensemble $\mathcal A$ de parties de $\Omega$ est une \textbf{tribu}\index{tribu} si \begin{itemize}
        \item $ \mathcal A$ est stable par passage au complémentaire
        \item $\emptyset \in \mathcal A$
        \item $ \mathcal A$ est stable par union dénombrable
    \end{itemize}
    On dit alors que $(\Omega, \mathcal A)$ est un \textbf{espace probabilisable}\index{espace probabilisable}  et les éléments de $\mathcal A$ sont appelés  \textbf{événements} \index{evenement@événement}
\end{dfn}


\subsection{Propriétés ensemblistes}

\begin{prop}
    Soit $(\Omega, \mathcal A)$ un espace probabilisable. Si $(A_n)\in \mathcal A^{\N}$ alors \[
        \bigcap_{n\in\N}A_n \in \mathcal A
    \]
\end{prop}

\begin{proof}
    C'est une union dénombrable de complémentaires
\end{proof}

\begin{notation}[HP]
    \[
        \underline{\lim}A_n=\bigcup_{n\in\N} \left( \bigcap_{m\geq n} A_m \right) \qquad \qquad \overline{\lim} A_n=\bigcap_{n\in\N} \left( \bigcup_{m\geq n}A_m \right) 
    \]
\end{notation}

\subsection{Opérations sur les tribus}

Soit $(\mathcal A_i)_{i\in I}$ une famille de tribus. L'ensemble \[
    \bigcap_{i\in I}\mathcal A_i
\]
est une tribu (facile).

\begin{rem}
    On définit ainsi la notion de tribu engendrée: la tribu engendrée par $\mathcal E\subset \mathcal \P(\Omega)$ est l'intersection de toutes les tribus contenant $\mathcal E$.
\end{rem}

\begin{dfn}
    Soit $(\Omega, \mathcal A)$ un espace probabilisable. \begin{itemize}
        \item $A,B\in \mathcal A$ sont \textbf{incompatible} si $A \cap B=\emptyset$
        \item Pour $A\in \mathcal A$, on appelle  \textbf{événement contraire} l'événement $A^c$
        \item On dit que $(A_i)_{i\in I} \in  \mathcal A^I$ est un \textbf{système complet d'événements}\index{système complet d'événements, SCE} si et seulement si $(A_i)_{i\in I}$ partitionne $\Omega$.
    \end{itemize}
\end{dfn}

\section{Espace probabilisé}

\begin{dfn}
    Soit $(\Omega, \mathcal A)$ un espace probabilisable. On appelle \textbf{probabilité}\index{probabilité} sur $(\Omega, \mathcal A)$ une application $\P:\mathcal A \to  [0, 1]$ telle que \begin{itemize}
        \item $\P(\Omega)=1$
        \item $\P$ est $\sigma$-additive, c'est-à-dire que pour toute suite $(A_i)$ d'événements deux à deux incompatibles, \[
                \P \left( \bigcup_{n\in\N}A_n \right) =\sum_{n\geq 0}\P(A_n)
        \]
    \end{itemize}
    Si $\P$ convient, on dira que $(\Omega, \mathcal A, \P)$ est un \textbf{espace probabilisé}\index{espace probabilisé}.
\end{dfn}

\begin{rem}
    \begin{itemize}
        \item La suite constante égale à $\emptyset$ donne $\P(\emptyset)=0$.
        \item Si $A$ et $B$ sont incompatibles, alors $\P(A\cup B)=\P(A)+\P(B)$ 
        \item $\P(A^c)=\P(A\cup A^c)-\P(A)=1-\P(A)$
        \item $A\subset B \implies \P(B)=\P(A)+\P(B\setminus A)\geq \P(A)$
    \end{itemize}
\end{rem}

\begin{exo}[Inégalité d'Edith Kosmanek\index{Kosmanek (inégalité de -- )}]
    Soit $(\Omega, \mathcal A, \P)$ un espace probabilisé, $A, B$ des événements. Montrer que \[
        |\P(A\cap B)-\P(A)\P(B)|\leq \frac{1}{4}
    \]
\end{exo}

\begin{proof}[Résolution]
    $(A\cap B, A\cap B^c, A^c\cap B, A^c\cap B^c)$ est un SCE et un peu de calcul donne \[
        \P(A\cap B)-\P(A)\P(B)=\P(A\cap B)\P(A^c\cap B^c)-\P(A^c\cap B)\P(A\cap B^c)
    \]
    Puis si $a,b\geq 0$ sont tels que $a+b\leq 1$, on a $ab=\sqrt{ab}^2\leq \left( \frac{a+b}{2} \right) ^2\leq \frac{1}{4}$ donc les deux produits sont $\leq \frac{1}{4}$, ce qui conclut.
\end{proof}

\begin{exo}
    $(\Omega, \mathcal A, \P)$ probabilisé, $A, B \in \mathcal A$. Montrer que $|\P(A)-\P(B)| \leq \P(A\Delta B)$
\end{exo}

\section{Propriétés élémentaires des probabilités}

\begin{thm}[Continuité croissante\index{Continuité croissante (probabilités)}]
    \Hyp $(\Omega, \mathcal A, \P)$ probabilisé, $(A_n)_{n\in \N}$ suite croissante d'événements
    \Conc \[
        \P\left(\bigcup_{n\in \N}A_n \right)=\lim_{n \to \infty} \P(A_n).
    \]
\end{thm}

\begin{proof} Par $\sigma$-additivité, en posant $A_{-1}=\emptyset$, on a
    \[
        \P\left( \bigcup_{n\in N} A_n \right) =\sum_{n\geq 0}\P(A_n\setminus A_{n-1})= \lim_{n \to \infty} \sum_{k=0}^{n} \underbrace{\P(A_k\setminus A_{k-1})}_{=\P(A_k)-\P(A_{k-1})}=\lim_{n \to \infty} \P(A_n).
    \]
\end{proof}

\begin{thm}[Continuité décroissante]
    \Hyp $(\Omega, \mathcal A, \P)$ probabilisé, $(A_n)_{n\in \N}$ suite décroissante d'événements
    \Conc \[
        \P\left(\bigcap_{n\in \N}A_n \right)=\lim_{n \to \infty} \P(A_n).
    \]
\end{thm}

\begin{proof}
    C'est la continuité croissante en passant au complémentaire.
\end{proof}

\begin{thm}[Sous-additivité]
    \Hyp $(\Omega, \mathcal A, \P)$ espace probabilisé, $(A_n)$ suite d'événements
    \Conc Avec la convention qu'une somme de série divergente à terme général positif vaut $+\infty$, \[
        \P\left( \bigcup_{n \in  \N}A_n  \right) \leq \sum_{n=0}^{+\infty} \P(A_n).
    \] 
\end{thm}

\begin{proof}
    On note \[
    X_n=\bigcup_{k\in \N}A_k 
    \] 
    de sorte que $(X_n)$ est une suite croissante d'événements. Pour deux événements $A\subset B$, on a \[
        \P(A\cup B)=\P(A\cup(B\setminus A))=\P(A)+\P(B\setminus A)\leq \P(A)+\P(B)
    \] 
    et en itérant, on trouve \[
        \forall n \in  \N, \qquad \P(X_n)\leq \sum_{k=0}^{n} \P(A_i)
    \] 
    donc \[
        \lim_{n \to \infty} \P(X_n) = \P\left( \bigcup_{n\in \N} A_n \right) \leq \sum_{n=0}^{+\infty} \P(A_n)
    \] 
\end{proof}

\begin{rem}
    Un événement $A\in \mathcal A$ est dit \textbf{négligeable} si $\P(A)=0$. Si $(A_n)$ est une suite d'événements négligeables, alors \[
        \P(\bigcup_{n\in \N}A_n )\leq \sum_{n=0}^{+\infty} \P(A_n)=0
    \] 
\end{rem}

\subsection{Formule de Poincaré}

Soit $(\Omega, \mathcal A, \P)$ un espace probabilisé, $A_1, \cdots, A_n \in  \mathcal A$. On va montrer que \[
    \P(A_1\cup \cdots \cup A_n)=\sum_{1\leq i_1\leq n}\P(A_{i_1})-\sum_{1\leq i_1<i_2\leq n}\P(A_{i_1}\cap A_{i_2}) +\cdots +(-1)^{n-1}\sum_{1\leq i_1<\cdots <i_n\leq n}\P(A_{i_1}\cap \cdots \cap A_{i_n})
\] 
On remarque pour cela \[
    \1_{(A_1\cup \cdots \cup A_n)^c}=1-\1_{A_1\cup \cdots \cup A_n}=\1_{A_1^c\cap \cdots \cap A_n^c}=\1_{A_1^c}\times \cdots \times \1_{A_n^c}=(1-\1_{A_1})\times \cdots \times (1-\1_{A_n})
\]
donc en développant, \[
    \1_{(A_1\cup \cdots \cup A_n)^c} = 1-\sum_{i=1}^n\1_{A_i}-\sum_{i_1<i_2}\1_{A_{i_1}\cap A_{i_2}}+\cdots +(-1)^{n-1}\sum_{i_1<\cdots <i_n}\1_{A_{i_1}\cap\cdots\cap A_{i_n}}
\]
Or (on le verra), $\E(\1_A)=\P(A)$ et $\E$ est linéaire, ce qui conclut.

\begin{ex}
    On place $r$ boules dans $n$ cases. Calculer la probabilité qu'aucune case ne soit vide. On note $A_i$ l'événement "La case n°$i$ est vide" et $B=A_1^c\cap \cdots \cap A_n^c$. On cherche $\P(B)$. On a \[
        \forall k \in  \llbracket 1, n\rrbracket, \qquad \P(A_{i_1}\cap \cdots \cap A_{i_k})= \left( 1-\frac{k}{n} \right) ^r.
    \] 
    Puis, \begin{align*}
        \P(B^c)=1-\P(B)&=\sum_{1\leq i_1\leq n} \left( 1-\frac{1}{n} \right) ^r-\sum_{i_1<i_2} \left( 1-\frac{2}{n} \right) ^r+\cdots +(-1)^{n-1} \sum_{i_1<\cdots <i_n} \left( 1-\frac{n}{n} \right) ^r\\
                       &=\sum_{k=0}^n(-1)^k\binom nk \left( 1-\frac{k}{n} \right) ^r
    \end{align*}
\end{ex}

\section{Probabilité conditionnelle}

\begin{dfn}
    Soit $(\Omega, \mathcal A, \P)$ un espace probabilisé et $B\in \mathcal A$ tel que $\P(B)>0$. On appelle probabilité de $A \in \mathcal A$ sachant $B$ le nombre  \[
        \P_B(A)=\P(A|B)\defeq \frac{\P(A\cap B)}{\P(B)}
    \] 
\end{dfn}

\begin{prop}
    \Hyp $(\Omega, \mathcal A, \P)$ un espace probabilisé, $A_1, \cdots , A_n \in  \mathcal A$ tels que $\P(A_1 \cap \cdots \cap A_n)>0$
    \Conc \[
        \P(A_1\cap \cdots \cap A_n)=\P(A_1)\P(A_2|A_1)\P(A_3|A_1\cap A_2)\cdots \P(A_n|A_1\cap\cdots\cap A_{n-1})
    \] 
\end{prop}

\begin{proof}
    La seule difficulté est de montrer que les probabilités utilisées existent. C'est le cas car \[
        \forall i\leq n, \qquad 0<\P(A_1\cap\cdots\cap A_n)\leq \P(A_1\cap \cdots \cap A_i)
    \] 
\end{proof}

\begin{ex}
    On se donne un sac dans lequel il y a une boule blanche et une boule noire. À chaque tour, on tire une boule et on en remet deux de la même couleur dans le sac. Quelle est la probabilité de tirer $n$ boules blanches lors des $n$ premiers tirages ?

    On note $A_n$ l'événement cherché et: \begin{itemize}
        \item $ \P(A_1)=\frac{1}{2}$
        \item $\P(A_n|A_{n-1})=\dfrac n{n+1}$
        \item \begin{align*}
                \P(A_n)&=\P(A_1\cap \cdots \cap A_n) \\ &= \P(A_1)\cdot \P(A_2|A_1)\cdots \P(A_n|A_1\cap\cdots \cap A_{n-1}) \\ &=\P(A_1)\P(A_2|A_1)\cdots \P(A_n|A_{n-1}) \\ &= \frac{1}{2} \times \frac{2}{3} \times \cdots \times \frac{n}{n+1}\\&=\frac{1}{n+1}
        \end{align*}
    \end{itemize}
\end{ex}

\begin{ex}[Monty Hall]
    Dans le jeu télévisé \emph{Let's make a deal}, un candidat doit choisir une porte parmi trois. Derrière deux d'entre elles, il y a une chèvre, et derrière la troisième il y a une voiture. On note $X$ la variable aléatoire qui correspond au premier choix. Une fois le choix effectué, le présentateur révèle une chèvre parmi les portes non choisies, et le candidat a l'occasion de changer son choix. On note alors $Y$ la variable aléatoire qui vaut $0$ si le candidat ne change pas son choix et $1$ s'il le change. Le candidat a-t-il intérêt à changer son choix ?

    On suppose que la porte gagnante est la porte numéro $1$ et on note $A$ l'événement qui correspond à une victoire du candidat. On a \begin{itemize}
        \item $\P(A|X=1,Y=1)=1$
        \item $\P(A|X=2,Y=1)=0$
        \item $\P(A|X=3,Y=1)=0$
        \item $\P(A|X=1,Y=0)=0$
        \item $\P(A|X=2,Y=0)=1$
        \item $\P(A|X=3,Y=0)=1$
    \end{itemize}
    Donc \[
        \P(A|Y=1)=\P(A|X=1,Y=1)+\P(A|X=2,Y=1)\P(X=2)+\P(A|X=3,Y=1)\P(X=3)=\frac{2}{3}
    \]
    Donc le candidat a intérêt à changer son choix.
\end{ex}

\begin{defprop}[Trace]
    \Hyp $(\Omega, \mathcal A, \P)$ est un espace probabilisé, $A\in \mathcal A$ un événement tel que $\P(A)>0$
    \begin{concenum}
        \item $\mathcal A_A\defeq\{A\cap B, \quad B\in \mathcal A\}$ est une tribu de $A$
        \item $(A,\mathcal A_A, \P_A)$ est un espace probabilisé, avec {\begin{align*}
                \P_A:\mathcal A_A  &\longrightarrow [0,1] \\
                B &\longmapsto \frac{\P(B)}{\P(A)}
        \end{align*}}
    \end{concenum}
\end{defprop}

\begin{prop}
    \Hyp $(\Omega, \mathcal A, \P)$ est un espace probabilisé, $A\in \mathcal A$ un événement tel que $\P(A)>0$
    \Conc $(\Omega, \mathcal A, \P_A)$ est un espace probabilisé avec { \begin{align*}
            \P_A: \mathcal A &\longrightarrow [0,1] \\
            B &\longmapsto \frac{\P(A\cap B)}{\P(A)}
    \end{align*} }
\end{prop}

\begin{thmdef}
    \Hyp $(\Omega, \mathcal A, \P)$ espace probabilisé
    \begin{concenum}
    \item On dira que $(A_n)\in \mathcal A^{\N}$ est un \textbf{système complet d'événements}\index{système complet d'événements} si \begin{itemize}
        \item $ \forall n\in \N,\quad A_n \neq \emptyset$
        \item Les $A_i$ sont deux à deux incompatibles
        \item $\bigcup_{i \in  \N}A_i  =\Omega$ 
    \end{itemize}
    \item Pour tout $A\in \mathcal A$ on a dans ce cas \[
            \P(A)=\sum_{n \in  \N} \P(A\cap A_n)=\sum_{n \in  \N} \P_{A_n}(A)\P(A_n)
    \] 
    en convenant que si $\P(A_n)=0$ alors $\P(A_n)\P_{A_n}(A)=0$
    \end{concenum}
\end{thmdef}

\begin{proof}
    C'est la $\sigma$-additivité.
\end{proof}

\begin{rem}
    On peut se dispenser de l'hypothèse $A_i\neq \emptyset$, et on peut se contenter de  $\P(\bigcup A_n )=1$ au lieu de $ \bigcup A_n=\Omega $. On parle alors de système pseudo-complet ou presque-complet.
\end{rem}

\section{Formule de Bayes}

\begin{thm}[Bayes]
    \Hyp $(\Omega, \mathcal A, \P)$ espace probabilisé
    \begin{concenum}
        \item Si $A,B\in \mathcal A$ sont tels que $\P(A), \P(B)>0$, alors \[
            \P(B|A)=\frac{\P(A|B)\P(B)}{\P(A)}
        \] 
    \item Si $(A_n)\in  \mathcal A^{\N}$ est un SCE, $B\in \mathcal A$ est tel que $\P(B)>0$, alors \[
            \P_B(A_i)=\frac{\P_{A_i}(B)\P(A_i)}{\sum_{n=0}^{+\infty} \P_{A_n}(B)\P(A_n)}
    \] 
    \end{concenum}
\end{thm}

\section{Germes de probabilités}

\begin{prop}[Germe de probabilité\index{germe de probabilité}]
    \Hyp $\Omega=\{x_i, \quad i \in  \N\}$, $\mathcal A=\mathcal \P(\Omega)$ et $(p_i)_{i \in \N} \in (\R^+)^{\N}$ telle que $\sum p_i=1$
    \Conc Il existe une unique probabilité $\P$ sur $(\Omega, \mathcal A)$ telle que $\forall i \in  \N,\quad \P(\{x_i\})=p_i$, et elle est donnée par \[
        \forall X \in  \mathcal A,\quad \P(X)=\sum_{\substack{i \in  \N\\x_i\in X}}p_i
    \] 
\end{prop}

\begin{proof}
    Unicité facile, existence facile.
\end{proof}

\section{Variables aléatoires discrètes}

\begin{dfn}
    Soit $(\Omega, \mathcal A, \P)$ un espace probabilisé et $E$ un ensemble. On appelle \textbf{variable aléatoire discrète}\index{variable aléatoire discrète} à valeurs dans $E$ une application $X:\Omega \to  E$ telle que \begin{itemize}
        \item $X(\Omega)$ est fini ou dénombrable
        \item  $ \forall x \in  E,\quad X^{-1}(\{x\} )\in  \mathcal A$
    \end{itemize}
\end{dfn}

\begin{ex}
    $A\in \mathcal A,\quad \1_A$ est une v.a.d
\end{ex}

\begin{rem}
    Si $E$ est dénombrable, alors $E$ s'écri t  $\{x_n,\quad n\in \N\} $ avec les $x_n$ deux à deux distincts. Si  $X:\Omega \to  E$ est une v.a.d alors \begin{itemize}
        \item $A_n=X^{-1}(\{x_n\} )$. Les $(A_n)_n$ forment un SCE (ou un quasi-SCE)
        \item $p_n=\P(A_n)$ définit un germe de probabilité
        \item Une probabilité sur $(E, \mathcal \P(E))$ est donnée par \[
                \forall B\subset E,\quad \sum_{\substack{n\geq 0\\ x_n\in  B}}p_n
        \] 
        et on a \[
            [X\in B]=\bigcup_{\substack{n\geq 0\\x_n\in  B}}X^{-1}(\{x_n\} )\in  \mathcal A 
        \] 
        donc \[
            \P(X\in  B)=\sum_{\substack{n\geq 0\\x_n\in B}}\P(A_n)=\P_\star (B)
        \] 
    \end{itemize}
    On a ainsi une probabilité sur $(E, \mathcal \P(E))$ à partir de $(\Omega, \mathcal A, \P)$ sans connaître $\Omega$. On a seulement besoin de connaître les $p_n$ (la loi de $X$)
\end{rem}

\begin{thm}
    \Hyp $E=\{x_n, \quad n\in \N\} $, les $x_n$ deux à deux distincts, $(p_n)_{n\in \N}\in (\R^+)^{\N}, \quad \sum p_n=1$
    \Conc Il existe $\Omega$, une tribu $\mathcal A$ de $\Omega$, $\P$ une probabilité telle que $(\Omega, \mathcal A,\P)$ est un espace probabilisé et $X:\Omega\to E$ est une v.a.d satisfaisant $\P(X=x_n)=p_n$
\end{thm}

\begin{proof}
    Admis(e) mais facile ($\Omega=E, \mathcal A=\mathcal \P(E)$)
\end{proof}

\section{Indépendance}

\subsection{Indépendance d'événements}

\begin{dfn}
    $(\Omega, \mathcal A, \P)$ un espace probabilisé. Deux événements $A,B\in \mathcal A$ sont dits \textbf{indépendants}\index{evenements independants@événements indépendants} si et seulement si $\P(A\cap B)=\P(A)\P(B)$. Si $\P(B)\neq 0$, c'est équivalent à $\P(A|B)=\P(A)$
\end{dfn}

\subsection{Indépendance de variables aléatoires}

\begin{dfn}
    Soit $(\Omega, \mathcal A, \P)$ un espace probabilisé. \begin{itemize}
        \item Si $X_1, \cdots, X_n$ sont des v.a.d sur $\Omega$, $E_i=X_i(\Omega)$, on dira que $X_1, \cdots,X_n$ sont \textbf{mutuellement indépendantes}\index{indépendance mutuelle} si \[
                \forall (A_1,\cdots,A_n)\in \mathcal \P(E_1)\times \cdots \times \mathcal \P(E_n), \qquad \P(X_1\in  A_1,\cdots,X_n\in A_n)=\P(X_1\in A_1)\cdots \P(X_n\in A_n)
        \] ou de manière équivalente \[
        \forall (x_1,\cdots,x_n)\in E_1\times \cdots \times E_n, \qquad \P(X_1=x_1,\cdots,X_n=x_n)=\P(X_1=x_1)\cdots \P(X_n=x_n)
        \] 
    \item Si $X, Y$ sont des v.a.d indépendantes, on note $X\indep Y$
    \end{itemize}
\end{dfn}

\begin{rem}
    En général, l'indépendance mutuelle implique l'indépendance des variables deux à deux, mais la réciproque est fausse.
\end{rem}

\subsection{Coalitions}

On note $(\Omega, \mathcal A, \P)$ un espace probabilisé, $X, Y, Z: \Omega \to  \N$ mutuellement indépendantes. On note $U=X+Y$.  $U(\Omega)$ est fini ou dénombrable. Pour  $n\in  \N$, \[
    [U=n]=\bigcup_{k=0}^{n}[X=k,Y=n-k]\in \mathcal A
\] 
donc $U$ est une variable aléatoire discrète. A-t-on  $U\indep Z$ ? On a  \[
    [U=n,Z=m]=\bigcup_{k=0}^{n}[X=k,Y=n-k,Z=m]
\]
donc \[
    \P(U=n,Z=m)=\P(Z=m) \sum_{k=0}^{n} \P(X=k)\P(Y=n-k)=\P(U=n)\P(Z=m)
\]
donc $U\indep Z$

\begin{lmm}[Coalitions\index{Coalitions (lemme des -- )}]
    \Hyp $X_1,\cdots, X_n$ des v.a.d mutuellement indépendantes à valeurs dans $\R$, $d\in \llbracket 1, n\rrbracket$, $f: \R^d\to  E,\quad g: \R^{n-d}\to F$
    \Conc $f(X_1,\cdots, X_d)\indep g(X_{d+1},\cdots ,X_n)$
\end{lmm}

\begin{proof}
    $X=f(X_1,\cdots ,X_d),Y=g(X_{d+1},\cdots ,X_n)$ sont des v.a.d car pour $x\in X(\Omega)$, \[
        X^{-1}(\{x\} )=\bigcup_{(x_1,\cdots ,x_n)\in f^{-1}(\{x\} )} [X_1=x_1,\cdots ,X_n=x_n]\in \mathcal A
    \] car $f^{-1}(\{x\} )\subset E_1\times \cdots \times E_d$. Soient $x\in X(\Omega),y\in Y(\Omega)$. \[
    [X=x,Y=y]=\;\;\;\;\smashoperator{\bigcup_{\substack{(x_1,\cdots ,x_d)\in E_1\times \cdots \times E_d\\(y_{d+1},\cdots ,y_n)\in E_{d+1}\times \cdots \times E_n\\ f(x_1,\cdots ,x_n)=x\\g(y_{d+1},\cdots ,y_n)=y}}} \;\;\;\;[X_1=x_1,\cdots ,X_d=x_d,Y_{d+1},\cdots ,X_n=y_n]
\] donc \begin{align*}
\P(X=x,Y=y)&=\;\;\;\;\smashoperator[lr]{\sum_{\substack{(x_1,\cdots ,x_d)\in E_1\times \cdots \times E_d\\(y_{d+1},\cdots ,y_n)\in E_{d+1}\times \cdots \times E_n\\ f(x_1,\cdots ,x_n)=x\\g(y_{d+1},\cdots ,y_n)=y}}}\;\;\;\; \P(X_1=x_1)\P(X_2=x_2)\cdots \P(X_d=x_d)\P(X_{d+1}=y_{d+1})\cdots \P(X_n=y_n) \\
           &= \;\;\;\;\smashoperator[l]{\sum_{\substack{(x_1,\cdots ,x_d)\in E_1\times \cdots \times E_d\\ f(x_1,\cdots ,x_n)=x}}}\;\;
           \smashoperator[r]{\sum_{\substack{(y_{d+1},\cdots ,y_n)\in E_{d+1}\times \cdots \times E_n\\ g(y_{d+1},\cdots ,y_n)=y}}}\;\;\;\; \P(X_1=x_1)\cdots \P(X_n=y_n) \\
           &= \P(X=x)\P(Y=y)
\end{align*}
ce qui conclut.
\end{proof}

\begin{rem}
    Le fait que les $X_i$ soient à valeur dans  $\R$ est sans importance. Par ailleurs, on peut généraliser le résultat en \[
        f_1(X_1,\cdots ,X_{d_1}),\cdots ,f_p(X_{d_{p-1}+1},\cdots ,X_n)
    \] 
    mutuellement indépendantes.
\end{rem}

\begin{dfn}
    Soit $(\Omega, \mathcal A, \P)$ un espace probabilisé et $(X_n)_{n\in \N}$ une suite de variables aléatoires discrètes. On dira que la famille $(X_n)_{n\in \N}$ est est mutuellement indépendante si pour toute partie finie $I\subset \N$, les $(X_i)_{i\in I}$ sont mutuellement indépendantes
\end{dfn}

\begin{thm}
    \Hyp $(E_n)_{n\in \N}$ une suite d'ensembles dénombrables, $(\P_n)_{n\in \N}$ une suite de probabilités sur $(E_n, \mathcal \P(E_n))$.
    \Conc Il existe un espace probabilisé $(\Omega, \mathcal A, \P)$ et une suite $(X_n)$ de variables aléatoires discrètes avec  $X_n:\Omega\to E_n$ telles que \begin{itemize}
        \item $\forall n\in \N,\forall x\in E_n,\quad \P(X_n=x)=\P_n(\{x\} )$
        \item $(X_n)_{n\in \N}$ sont mutuellement indépendantes.
    \end{itemize}
\end{thm}

\begin{proof}
    Admise.
\end{proof}

\section{Exemples de lois}

\begin{dfn}
    Soit $p\in ]0,1[$ et $(\Omega,\mathcal  A,\P)$ probabilisé. On dira que $X:\Omega\to \N^\star$ suit une loi géométrique de paramètre $p$ si \begin{itemize}
        \item $X$ est une v.a.r.d
        \item $\P(X=n)=q^{n-1}p$ où $q=p-1$.
    \end{itemize}
    On écrira $X\suit G(p)$\index{loi géométrique}
\end{dfn}

\begin{rem}
    $(q^{n-1}p)_n$ est un germe de probabilité donc il existe des v.a.d qui suivent cette loi.
\end{rem}

\begin{thm}
    \Hyp $(\Omega, \mathcal A, \P)$ espace probabilisé, $X\suit G(p)$
     \begin{concenum}
     \item Pour $n,k\in \N,\quad \P(X>n+k|X>n)=\P(X>k)$. On dit que $X$ est sans mémoire
     \item Si $X$ est une v.a.r.d à valeurs dans $\N^\star$ et si $\forall  n,k\in \N,\quad \P(X>n+k|X>n)=\P(X>k)$ alors $X\suit G(\P(X=1))$
    \end{concenum}
\end{thm}

\begin{proof}~
\begin{enumerate}
    \item \[
            \P(X>n)=\sum_{k\geq 1}\P(X=n+k)=\sum_{k\geq 1}q^{m+k-1}p= \frac{q^mp}{1-q}=q^m \neq 0
    \] et \[
    \P(X>n+k|X>k)= \frac{\P(X>n+k,X>n)}{\P(X>m)}=\frac{q^{n+k}}{q^m}=q^k=\P(X>k)
    \] 
\item On pose $F(n)=\P(X>n)$. L'hypothèse donne  $\forall  n,k \in  \N, F(n+k)=F(n)F(k)$. Avec $q=F(1)$, on a  $\forall  n\in \N^\star, F(n)=q^n$ et $F(0)=\P(X>0)=1=q^0$. Ainsi,  \[
        \forall  n\in \N,\quad \P(X=n)=F(n-1)-F(n)=q^{n-1}(1-q)
\] 
et $p\defeq1-q$. La suite des $[X>n]$ est décroissante donc par continuité,  \[
    \P(\emptyset)=0=\P \left( \bigcap_{n \in  \N} [X>n] \right) =\lim_{n\to+\infty}q^n\implies p,q\in ]0,1[
\] 
\end{enumerate}
\end{proof}

\begin{rem}
    On prend $(X_n)_n$ une suite de variables aléatoires discrètes indépendantes de loi $\mathcal  B(p)$ sur $(\Omega, \mathcal A, \P)$ espace probabilisé. Pour $\omega \in  \Omega$, on pose (avec la convention $\min \emptyset=+\infty$)\[
        T(\omega)=\min \{n\in  \N^\star,\quad X_n(\omega)=1\} 
    \]
    \begin{itemize}
        \item $T(\Omega)=\N^\star\cup \{+\infty\} $ est dénombrable
        \item On a \[
                [T=n]=[X_1=0,\cdots ,X_{n-1}=0,X_n=1]\in \mathcal  A
        \] 
        et \[
            [T=+\infty]= \left( \bigcup_{n\in \N^\star}[T=n]  \right)^c\in \mathcal  A.
        \] 
    \end{itemize}
    Ainsi, $T$ est une v.a.d. Pour $n\in \N$, \[
        \P(T=n)=\P(X_1=0,\cdots ,X_{n-1}=0,X_n=1)=q^{n-1}p
    \] 
    et \[
        \P(T=+\infty)=1-\sum_{n\geq 0}\P(T=n)=1-\frac{p}{1-q}=0
    \] 
    Par abus, on dit que $T\suit G(p)$ même si  $T$ est à valeurs dans $\N^\star\cup \{+\infty\} $
\end{rem}

\begin{exo}
Avec les mêmes notations, on note \[
    T_n=\min \{i\in \N^\star,\quad (X_1+\cdots +X_n)(\omega)=n\} 
\] 
Trouver la loi de $T_n$
\end{exo}

\begin{proof}[Résolution]
C'est bien une v.a.d et \[
    \P(T_n=j)=\sum_{\substack{\epsilon_1+\cdots +\epsilon_{j-1}=n-1\\\epsilon_i\in \{0,1\} }}\underbrace{\P(X_1=\epsilon_1)\cdots \P(X_{j-1}=\epsilon_{j-1})\P(X_j=1)}_{(1-q)^{n-1}q^{j-n}(1-q)}=\binom{j-1}{n-1}(1-p)^nq^{j-n}
\]
et puis \[
    1-\P(T_n=+\infty)=\P\left( \bigcup_{j\geq n} [T_n=j] \right) =\sum_{j\geq n}\binom{j-1}{n-1}q^{j-n}p^n=\sum_{j\geq n}\frac{(j-1)\cdots (j-n+1)}{(n-1)!}q^{j-1}p^n
\] 
Or en posant $f:z\in ]-1,1[ \longmapsto \frac{1}{1-z}$, \[
    f^{(n-1)}(q)=\sum_{j\geq n}(j-1)\cdots (j-n+1)q^{j-n}=\frac{(n-1)!}{(1-q)^n}
\] 
d'où \[
    \P(T_n=+\infty)=1-\frac{p^n}{(1-q)^n}=0
\]
\end{proof}

\begin{dfn}
    Soit $(\Omega, \mathcal  A, \P)$ un espace probabilisé. On dira que $X:\Omega\to \N$ suit une loi de Poisson\index{Poisson (loi de -- )}\index{loi de Poisson} de paramètre $\lambda>0$ si pour  $n\in \N$, $\P(X=n)=e^{-\lambda}\frac{\lambda^n}{n!}$. On écrira $X\suit \mathcal  \P(\lambda)$
\end{dfn}

\begin{rem}
    Il existe bien des v.a.r.d qui suivent cette loi car $\displaystyle\left(e^{-\lambda}\frac{\lambda^n}{n!}\right)$ est un germe de probabilité.
\end{rem}

\begin{rem}
    On va montrer que si $X_1\suit\mathcal \P(\lambda_1),X_2\suit \mathcal  \P(\lambda_2)$ avec $X_1\indep X_2$, alors $X_1+X_2 \suit \mathcal  \P(\lambda_1+\lambda_2)$. On note $X=X_1+X_2$. On a \begin{itemize}
        \item $X(\Omega)\subset \N$
        \item $\displaystyle [X=n]=\bigcup_{k=0}^n[X_1=k,X_2=n-k] $ 
    \end{itemize}
    Donc \begin{align*}
        \P(X=n)&=\sum_{k=0}^n\P(X_1=k,X_2=n-k)\\&= \sum_{k=0}^{n} e^{-\lambda_1}\frac{\lambda_1^k}{k!}e^{-\lambda_2}\frac{\lambda_2^{n-k}}{(n-k)!}\\&=\frac{e^{-\lambda_1-\lambda_2}}{n!} \sum_{k=0}^{n} \binom nk \lambda_1^k\lambda_2^{n-k}\\&=e^{-\lambda_1-\lambda_2}\frac{(\lambda_1+\lambda_2)^n}{n!}
    \end{align*}
    Donc $X\suit \mathcal  \P(\lambda_1+\lambda_2)$
\end{rem}

\endchapter

\ifsolo
    ~

    \vspace{1cm}

    \begin{center}
        \textbf{\LARGE Espérance -- Variance} \\[1em]
    \end{center}
    \tableofcontents
\else
    \chapter{Espérance -- Variance}

    \minitoc
\fi
\thispagestyle{empty}

\section{Espérance}

\begin{dfn}
    On note $(\Omega, \mathcal  A, \P)$ un espace probabilisé, $X$ une variable aléatoire \textbf{réelle} discrète. On dira que $X$ est d'\textbf{espérance fini}\index{espérance} si  $(x\P(X=x))_{x\in X(\Omega)}$ est sommable. Dans ce cas, on appelle \textbf{espérance} la quantité \[
        \E(X)= \sum_{x \in  X(\Omega)}x\P(X=x)
    \] 
\end{dfn}

\begin{rem}[Rappel]
    Si $X(\Omega)=\{x_n,\quad n\in \N\} $, les $x_n$ deux à deux distincts, alors  \[
        \sum_{n\in \N}|x_n|\P(X=x_n) \text{ CV } \iff \left( x_n\P(X=x_n) \right)_{n \in  \N}\text{ sommable }
    \]
\end{rem}

\begin{ex}~
    \begin{itemize}
        \item $X\suit G(p)$ est d'espérance finie car  $(nq^{n-1}p)_n$ est sommable, et \[
                \E(X)=\sum_{n\geq 0}nq^{n-1}p=\frac{1}{p}
        \] 
    \item $X\suit \mathcal  P(\lambda)$ pour $\lambda>0$, on a  $n\P(X=n)=e^{-\lambda}\frac{n\lambda^n}{n!}$ et $ \E(X)=\lambda$
    \item $X=\1_A$ pour  $A\in \mathcal  A$, $\E(X)=\P(X=1)=\P(A)$
    \end{itemize}
\end{ex}

\section{Formule de transfert}

\begin{thm}
    \Hyp $(\Omega, \mathcal  A, \P)$ un espace probabilisé, $X$ une v.a.r.d et  $f:X(\Omega)\to \R$
    \begin{concenum}
    \item $f(X)$ est une v.a.r.d
    \item  $f(X)$ est d'espérance finie  si et seulement si $(f(x)\P(X=x))_{x\in X(\Omega)}$ et dans ce cas, \[
            \E(f(X))=\sum_{x\in X(\Omega)}f(x)\P(X=x)
    \] 
    \end{concenum}
\end{thm}

\begin{proof}~
    \begin{enumerate}
        \item Lemme des coalitions
        \item Pour $y\in f(X(\Omega))$, on note $I_y=f^{-1}(\{y\} )$. On a \[
                [f(X)=y]=\bigcup_{x\in I_y}[X=x] 
        \] 
        et cette union est disjointe donc \[
            \P(f(X)=y)=\sum_{x\in I_y}\P(X=x).
        \]
        La famille $(I_y)_{y\in f(X(\Omega))}$ forme une partition de $X(\Omega)$. Puis  \[
            \sum_{x \in  I_y}\P(X=x)f(x)=y\sum_{x\in I_y}\P(f(X)=y)=y\P(f(X)=y).
        \] Si la famille $(f(x)\P(X=x))_{x \in  X(\Omega)}$ est sommable, alors pour tout $y\in f(X(\Omega))$, $(f(x)\P(X=x))_{x\in I_y}$ est sommable et $(I_y)$ est une partition donc la famille  \[
        \left( \sum_{x \in  I_y} P(X=x)f(x)\right) _{y\in f(X(\Omega))}
        \] 
        est sommable et \[
            \sum_{x\in X(\Omega)}f(x)\P(X=x)=\sum_{y\in f(X(\Omega))}\sum_{x \in  I_y}f(x)\P(X=x)=\sum_{y\in f(X(\Omega))}y\P(f(X)=y)=\E(f(X)).
        \] 
        Si $f(X)$ est d'espérance finie, alors $(|y|\P(f(X)=y))_{y\in  f(X(\Omega))}$ est sommable et \[
            |y|\P(f(X)=y)=\sum_{x \in  I_y}|f(x)|\P(X=x)
        \] 
        donc $(|f(x)|P(X=x))_{x\in X(\Omega)}$ est sommable.
    \end{enumerate}
\end{proof}

\begin{ex}
    $X\suit G(p)$, $Y=X^2$. La variable $Y$ est-elle d'espérance finie ? La famille $(n^2q^{n-1}p)_n$ est sommable donc $Y$ a une espérance finie et \[
        \E(Y)=\sum_{n\geq 1}n^2q^{n-1}p=\frac{2-p}{p^2}
    \] 
    On peut faire de même avec $Y=\exp (-X)$, ou encore si $X\suit \mathcal  P(\lambda)$, $Y=\cos  X$
\end{ex}

\section{Conséquences du théorème de transfert}

\begin{prop}
    \Hyp $(\Omega, \mathcal  A, \P)$ un espace probabilisé, $X,Y$ deux v.a.r.d, $Y$ d'espérance finie
    \Conc Si  $|X|\leq Y$, alors  $X$ est d'espérance finie et $\E(|X|)\leq \E(Y)$
\end{prop}

\begin{proof}
    On note $Z=(X,Y)$ qui est une v.a.d et  $f:\R^2\to \R,(x,y)\longmapsto x$ de sorte que $X=f(Z)$. Alors  \begin{align*}
        X\text{ d'espérance finie } &\iff (f(x, y)\P(X=x,Y=y))_{(x,y)\in X(\Omega)\times Y(\Omega)}\text{ sommable } \\ &\iff  (x\P(X=x,Y=y))_{(x,y)\in X(\Omega)\times Y(\Omega)}\text{ sommable }
    \end{align*}
    On a \[
        |x\P(X=x,Y=y)|\leq y\P(X=x,Y=y)\tag{$\star$}
    \] 
    Or \[
        [Y=y]=\bigcup_{x \in  X(\Omega)}[X=x,Y=y] \qquad \therefore \qquad  \P(Y=y)=\sum_{x \in  X(\Omega)}\P(X=x,Y=y)
    \]
    donc $(y\P(X=x,Y=y))_{x \in  X(\Omega)}$ est sommable. $Y$ est d'espérance finie donc  \[
        (y\P(Y=y))_{y\in Y(\Omega)}=\left( \sum_{x\in X(\Omega)}y\P(X=x,Y=y) \right) _{y \in  Y(\Omega)}
    \] 
    est sommable, donc $(y\P(X=x,Y=y))_{(x,y)\in  X(\Omega)\times Y(\Omega)}$ est sommable et $(\star)$ conclut sur la sommabilité. Puis  \begin{align*}
        \E(|X|)=\E(|f(Z)|)&=\sum_{\substack{x \in  X(\Omega)\\y\in Y(\Omega)}}|x|\P(X=x,Y=y)\\&\leq \sum_{\substack{x \in  X(\Omega)\\y\in Y(\Omega)}}y\P(X=x,Y=y)\\&\leq \sum_{y\in Y(\Omega)}\sum_{x\in X(\Omega)}y\P(X=x,Y=y)=\E(Y)
    \end{align*}
\end{proof}

\begin{thm}
    \Hyp $(\Omega, \mathcal A, \P)$ espace probabilisé, $\lambda\in \R$ est $X,Y$ des v.a.r.d. d'espérance finie.
     \begin{concenum}
     \item $X+Y$ et $\lambda X$ sont des v.a.r.d d'espérance finie et  $\E(X+Y)=\E(X)+\E(Y)$ et $ \E(\lambda X)=\lambda \E(X)$
     \item $X\geq 0 \implies \E(X)\geq 0$
     \item $X\geq Y\implies\E(X)\geq \E(Y)$
     \item Si $X$ et $Y$ sont indépendantes, $\E(XY)=\E(X)\E(Y)$
    \end{concenum}
\end{thm}

\begin{proof}~
\begin{enumerate}
    \item L'homogénéité est évidente. On montre l'additivité. On pose $Z=(X,Y)$ et  $f:(x,y)\longmapsto x+y$ de sorte que $T=f(Z)$ est une v.a.r.d par le lemme des coalitions.  $T$ est d'espérance finie  si et seulement si $((x+y)\P(X=x,Y=y))_{x,y}$ est sommable.

        La famille $(|x|\P(X=x,Y=y))_{y}$ est sommable car \[
            [X=x]=\bigcup_{y \in  Y(\Omega)} [X=x,Y=y] \qquad \therefore \qquad  \sum_{y \in  Y(\Omega)}|x|\P(X=x,Y=y)=|x|\P(X=x)
        \]
        sommable pour $x$ donc $(|x|\P(X=x,Y=y))_{x,y}$ est sommable, idem pour $(|y|\P(X=x,Y=y))_{x,y}$ donc $X+Y$ est d'espérance finie. Finalement,  \[
            \E(T)=\sum_x \left( \sum_y x\P(X=x,Y=y) \right) +\sum_y \left( \sum_x y\P(X=x,Y=y) \right) =\E(X)+\E(Y)
        \] 
    \item On applique la définition
    \item On utilise $2$ et $1$.
    \item $XY$ d'espérance finie  si et seulement si $(xy\P(X=x,Y=y))_{x,y}$ est sommable soit encore si la famille  $(x\P(X=x)y\P(Y=y))_{x,y}$ est sommable. Le théorème de Fubini discret donne la sommabilité et \[
            \sum_{x,y}xy\P(X=x)\P(Y=y)=\E(X)\E(Y)
    \]
\end{enumerate}
\end{proof}

\begin{ex}
    On note $(E, (\;|\;))$ un espace euclidien,  $v_1,\cdots ,v_n$ des vecteurs unitaires. On veut montrer qu'il existe $\epsilon_1,\cdots ,\epsilon_n\in \{-1,1\} $ tels que \[
        \|\epsilon_1v_1+\cdots +\epsilon_nv_n\|\leq \sqrt{n}
    \] 
    On se donne $X_1,\cdots ,X_n \quad  \indep$ qui suivent une loi de Rademacher\index{loi de Rademacher}, c'est-à-dire $\P(X_i=-1)=\P(X_i=1)=\frac{1}{2}$. On note \[
        N=\|X_1v_1+\cdots +X_nv_n\|^2
    \] 
    On a \[
        \E(N)=\sum_{i=1}^n\underbrace{\E(X_i^2)\|v_i\|^2}_{=1}+\sum_{i\neq  j}\underbrace{\E(X_iX_j)}_{=\E(X_i)\E(X_j)=0}(v_i|v_j)=n
    \]
    Donc $\P(N\leq n)\neq 0$ ce qui conclut.
\end{ex}

\section{Variance}

\begin{thmdef}
    \Hyp $(\Omega, \mathcal  A, \P)$ un espace probabilisé, $X$ une v.a.r.d
     \begin{concenum}
     \item Si $X^2$ est d'espérance finie, alors  $X$ est d'espérance finie et  $E(X)^2\leq E(X^2)$.
     \item Dans ce cas, on appelle \textbf{variance}\index{variance} la quantité $V(X)=\E((X-\E(X))^2)$ et \textbf{écart-type} la quantité $\sigma(X)=\sqrt{V(X)}$ 
     \item $V(X)=\E(X^2)-\E(X)^2$ (König-Huygens\index{Konig-Huygens@König-Huygens})
     \item $V(aX+b)=a^2V(X)$
    \end{concenum}
\end{thmdef}

\begin{proof}~
\begin{enumerate}
    \item $|X|\leq \frac{X^2+1}{2}$ et $\E((X+t)^2)=\E(X^2)+2t\E(X)+t^2\geq 0$ donc $\Delta\leq 0$ ie \conc
        \stepcounter{enumi}
    \item $\E((X-\E(X))^2)=\E(X^2-2X\E(X)+E(X)^2)=\E(X^2)-2\E(X)\E(X)+\E(X)^2=\E(X^2)-\E(X)^2$
\end{enumerate}
\end{proof}

\begin{rem}
On note $X$ une vard de variance non nulle. La variable \textbf{centrée réduite} \[
    Y=\frac{X-\E(X)}{\sigma(X)}
\] 
est telle que $\E(Y)=0$ et $V(Y)=1$
\end{rem}

\begin{rem}
    Si la quantité $\E(X^n)$ existe, on l'appelle \textbf{moment d'ordre $\bm n$} de $X$ \index{moment d'ordre supérieur} 
\end{rem}

\begin{exo}
    Pour $X\suit G(p)$, montrer  $\E(X)=\frac{1}{p}$ et $V(X)=\frac{q}{p^2}$. Pour $X\suit \mathcal  P(\lambda)$, montrer $ \E(X)=\lambda, \quad  V(X)=\lambda$
\end{exo}

\begin{thmdef}
    \Hyp $(\Omega, \mathcal  A, \P)$ un espace probabilisé, $X,Y$ des vard avec variance.
    \begin{concenum}
    \item $X,Y$ sont d'espérances finies et  \[
            \E(XY)^2\leq \E(X^2)\E(Y^2)
    \] 
\item Dans ce cas, on appelle covariance de $X$ et $Y$ la quantité \[
        \Cov(X,Y)=\E((X-\E(X))(Y-\E(Y)))
\] 
et on a \[
    \Cov(X,Y)=\E(XY)-\E(X)\E(Y)
\] 
\item Lorsqu'il existe ($\sigma(X)\sigma(Y)\neq 0$), on appelle coefficient de corrélation le réel \[
        P(X,Y)=\frac{\Cov(X,Y)}{\sigma(X)\sigma(Y)}\in [-1, 1]
\]
    \end{concenum}
\end{thmdef}

\begin{proof}~
    \begin{enumerate}
        \item $|XY|\leq \frac{X^2+Y^2}{2}$ d'où l'existence de l'espérance puis la négativité du discriminant dans $\E((X+tY)^2)\geq 0$ conclut, sauf si $\E(Y^2)=0$, auquel cas $\E((X+tY)^2)$ est une constante, ce qui donne $E(XY)=0$.
        \item C'est la linéarité de l'espérance.
        \item $|\Cov(X,Y)|^2\leq \E((X-\E(X))^2)\E((Y-\E(Y))^2)=\sigma^2(X)\sigma^2(Y)$
    \end{enumerate}
\end{proof}

\begin{rem}
    On note $(X_i)_{1\leq i\leq n}$ une suite de v.a.r.d de $(\Omega, \mathcal  A, \P)$, $S_n=X_1+\cdots +X_n$. On a \begin{itemize}
        \item $\displaystyle \E(S_n^2)= \sum_{i=1}^{n} \E(X_i^2)+2\sum_{i<j}\E(X_iX_j)$
        \item $\displaystyle \E(S_n)^2= \sum_{i=1}^{n} \E(X_i)^2+2\sum_{i<j}\E(X_i)\E(X_j)$
        \item $\displaystyle V(S_n)=\E(S_n^2)-E(S_n)^2= \sum_{i=1}^{n} V(X_i)+2\sum_{i<j}\Cov(X_i,X_j)$
    \end{itemize}
    Si les $X_i$ sont deux à deux indépendants, alors $V(S_n)=V(X_1)+\cdots +V(X_n)$
\end{rem}

\begin{ex}
    On munit $\mathfrak S_n $ de la probabilité uniforme. On note $S_n(\sigma)$ le nombre de points fixes de  $\sigma$. Donner l'espérance et la variance de  $S_n$.
\end{ex}

\begin{proof}[Résolution]
    On introduit $X_i:\sigma \longmapsto \delta_{i,\sigma(i)}$ de sorte que $S_n(\sigma)=X_1+\cdots +X_n$. Dans ce cas, on a  \[
        \P(X_i=1)=\frac{(n-1)!}{n!}=\frac{1}{n}
    \] 
    donc $ \E(X_i)=\frac{1}{n}$ (Bernoulli) et $\E(S_n)=1$. Puis, \[
        V(S_n)=\E(S_n^2)-E(S_n)^2= \sum_{i=1}^{n} V(X_i)+2\sum_{i<j}\Cov(X_i,X_j)=n\left( \frac{1}{n}-\frac{1}{n^2} \right) 2 \frac{n(n-1)}{2}\left( \frac{1}{n(n-1)}-\frac{1}{n^2} \right) =1
    \] 
\end{proof}

\begin{dfn}
    Soient $X,Y$ des v.a.d sur $(\Omega, \mathcal  A, \P)$ et $Z=(X,Y)$.  \begin{itemize}
        \item On appelle \textbf{loi} de $Z$ la donnée de  $(\P(X=x,Y=y))_{\substack{x\in X(\Omega)\\y\in Y(\Omega)}}$ appelée \textbf{loi conjointe}\index{loi conjointe} de $X$ et $Y$.
        \item On appelle \textbf{loi marginale}\index{loi marginale} de $Z$ la loi de $X$ et la loi de $Y$. Elles s'expriment à partir de la loi conjointe: \[
                \P(X=x)=\sum_{y \in  Y(\Omega)}\P(X=x,Y=y) \qquad \qquad \P(Y=y)=\sum_{x \in  X(\Omega)}\P(X=x,Y=y)
        \] 
    \end{itemize}
\end{dfn}
\begin{rem}
    Si $(a_{x,y})_{(x,y)\in  I\times J}$ est une famille sommable de $[0,1]$, alors il existe une v.a.r.d $Z$ à valeurs dans $I\times J$ telle que $ \forall (x,y)\in I\times J,\quad  \P(Z=(x,y))=a_{x,y}$ si et seulement si la somme de la famille vaut $1$
\end{rem}

\section{Inégalités classiques}

\begin{thm}[Markov, Tchebychev\index{Markov (inégalité de -- )}\index{Tchebychev!inégalité}]
    \Hyp $(\Omega, \mathcal  A, \P)$ espace probabilisé, $X$ une v.a.r.d
     \begin{concenum}
     \item Soit $a>0$. Si $X$ est d'espérance finie alors \[
             \P(|X|\geq a)\leq \frac{\E(|X|)}{a}\tag{Markov}
     \] 
 \item Soit $a>0$. Si  $X$ possède une variance alors  \[
         \P(|X-\E(X)|\geq a)\leq \frac{V(X)}{a^2}\tag{Tchebychev}
 \] 
    \end{concenum}
\end{thm}

\begin{proof}
~ \begin{enumerate}
    \item $a\1_{|X|\geq a}\leq |X|$ et croissance de l'espérance
    \item  $ \P(|X-\E(X)|\geq a)=\P((X-\E(X))^2\geq a^2)$ puis Markov.
\end{enumerate}
\end{proof}

\begin{ex}
    On note $(X_i)_{i\geq 1}$ une suite de v.a.r.d $\indep \quad \suit \mathcal  B(\frac{1}{2})$. On note $S_n=X_1+\cdots +X_n$ le nombre de succès en $n$ lancés. $\P(S_n\geq \frac{3}{4}n)$ ?
    \begin{itemize}
        \item Markov: $\displaystyle\P(S_n\geq \frac{3}{4}n)\leq \frac{\frac{n}{2}}{\frac{3n}{4}}=\frac{2}{3}$
        \item Tchebychev: $\displaystyle\P(S_n\geq \frac{3}{4})=\P(S_n-\E(S_n)\geq \frac{n}{4}) \leq \P(|S_n-\E(S_n)|\geq \frac{n}{4}\leq \frac{4}{n}$
    \end{itemize}
\end{ex}

\begin{ex}
    On note $X\suit G(\frac{1}{4})$. On va majorer $\P(X\geq n)$. \begin{itemize}
        \item Markov: $\P(X\geq n)\leq \dfrac{\E(X)}{n}=\dfrac{4}{n}$
        \item Tchebychev $\P(X\geq n)\leq \dfrac{V(X)}{(n-4)^2}=\dfrac{12}{(n-4)^2}$
        \item En posant $Y=\left( \dfrac{5}{4} \right) ^X$, on a $\E(Y)=5$ donc \[
                \P(X\geq n)\leq 5\left( \frac{4}{5} \right) ^n
        \] 
    \end{itemize}
\end{ex}

\section{Variables aléatoires à valeurs entières}

\begin{defprop}
    \Hyp $(\Omega, \mathcal A,\P)$ espace probabilisé, $X:\Omega\to \N$ une v.a.d
    \begin{concenum}
    \item Pour tout $t\in [-1,1]$, $t^X$ est d'espérance finie
    \item  Pour tout $t \in  [-1, 1]$, on note \[
            G_X(t)=\E(t^X)= \sum_{n=0}^{+\infty} \P(X=n)t^n
    \] 
    On appelle cette fonction la \textbf{fonction génératrice} de $X$.\index{fonction génératrice d'une v.a.d} \begin{enumerate}
        \item La série a un rayon de convergence $R\geq 1$
        \item Il y a convergence normale sur $ \mathcal  D_f(0,1)$
    \end{enumerate}
    \end{concenum}
\end{defprop}

\begin{proof}
    Pour $t\in [-1,1]$, $|t^X|\leq 1$ or  $1$ a une espérance finie, puis la formule de transfert justifie l'expression de  $\E(t^X)$. La série converge pour $t=1$ donc il y a bien CVN et  $R\geq 1$
\end{proof}

\begin{thm}
\Hyp $X:\Omega\to \N$ une v.a.d
\begin{concenum}
\item $X$ est d'espérance finie si et seulement si $\sum \P(X\geq n)$ converge et dans ce cas \[
        \E(X)=\sum_{n\geq 1}\P(X\geq n)
\] 
\item Il y a équivalence entre \begin{enumerate}
    \item $X$ a une espérance finie
    \item  $G_X$ est dérivable à gauche en  $1$
\end{enumerate}
Dans ce cas, $ \E(X)=G_X'(1^-)$
\end{concenum}
\end{thm}

\begin{proof}
~ \begin{enumerate}
    \item $X$ est d'espérance finie  si et seulement si $(k\P(X=k))_{k \in  \N^\star}$ est sommable. On note \[
            a_{i,k}\defeq \P(X=k)\1_{\llbracket 1,k \rrbracket }(i)
    \] 
    La famille $(a_{i,k})_{i\in \N^\star}$ est sommable et \[
        \sum_{i=1}^{+\infty} a_{i,k}=k\P(X=k)
    \] 
    et c'est le terme général d'une famille sommable, donc Fubini discret conclut.
\item $(a\implies b)$ $X$ d'espérance finie. Pour $t\in [0,1[$, \[
        \frac{G_X(t)-G_X(1)}{t-1} = \sum_{n=0}^{+\infty} \P(X=n)\frac{t^n-1}{t-1}= \sum_{n=1}^{+\infty} \P(X=n)(1+t+\cdots +t^{n-1})
\] 
et le terme général de cette série est inférieur en valeur absolu à $n\P(X=n)$ qui est le terme général d'une série convergente. Il y a donc convergence normale et le théorème de la double limite donne \[
    \lim_{t \to 1^-} \frac{G_X(t)-G_X(1)}{t-1}= \sum_{n=1}^{+\infty} \P(X=n)n=\E(X)
\] 

$(b\implies a)$ $G_X$ dérivable à gauche en $1$. Pour $N\in \N^\star$ fixé, \[
    \frac{G_X(t)-G_X(1)}{t-1}\geq \sum_{n=0}^{N} \P(X=n)(1+t+\cdots +t^{n-1}) \underset{t\to 1^-}\implies G_X'(1)\geq \sum_{n=0}^{N} n\P(X=n)
\]
Les sommes partielles sont majorées, donc $X$ est d'espérance finie.
\end{enumerate}
\end{proof}

\begin{ex}~
    \begin{center}
        \begin{tabular}{cccc}
            \hline \hline Loi de $X$ & $G_X$ & $R(G_X)$ & $ \E(X)$ \\
            \hline
            \rule{0pt}{2em} $\mathcal  B(p)$ &  $q+pt$ &  $+\infty$ &  $p$ \\
            \rule{0pt}{2em} $\mathcal B(n,p)$ &  $(q+pt)^n$ &  $+\infty$ & $np$ \\
            \rule{0pt}{2em} $\mathcal  P(\lambda)$ & $\exp (\lambda(t-1))$ & $+\infty$ & $\lambda$ \\
            \rule{0pt}{2em} $\mathcal  G(p)$ & $\dfrac{pt}{1-qt}$ &  $\dfrac{1}{q}$ & $\dfrac{1}{p}$ \\
            \rule{0pt}{2em} $\mathcal U(\llbracket 0,n-1 \rrbracket )$ & $\displaystyle \frac{1}{n} \frac{t^n-1}{t-1}$ & $+\infty$ &  $\dfrac{n-1}2$ \\[1em]
            \hline
        \end{tabular}
    \end{center}
\end{ex}

\begin{rem}
Si $ X_1,\cdots ,X_n$ sont des vard indépendantes à valeurs dans $ \N$, \[
    G_{X_1+\cdots +X_n}(t)=\E\left(t^{X_1+\cdots +X_n}\right)\underset\indep=G_{X_1}(t)\times \cdots \times G_{X_n}(t)
\] 
\end{rem}

\begin{thm}
    \Hyp $(\Omega,\mathcal A,\P)$ espace probabilisé, $X:\Omega\to \N$ une vad.
    \Conc Il y a équivalence entre \begin{enumerate}
        \item $X$ a une variance
        \item  $G_X$ est deux fois dérivable à gauche en  $1$.
    \end{enumerate}
    Dans ce cas, $G_X''(1^-)=\E(X(X-1))$
\end{thm}

\begin{proof}
    $(1\implies 2)$ \hyp $X$ a une variance donc  $X$ a une espérance et  \[
        G'_X(1)= \sum_{n=1}^{+\infty} n\P(X=n)
    \] 
    donc \[
        \frac{G_X'(t)-G_X'(1)}{t-1}=\sum_{n\geq 2}n\P(X=n) \frac{t^{n-1}-1}{t-1}
    \] 
    qui converge normalement ($X^2$ a une espérance finie  car $X$ a une variance), le théorème de la double limite conclut.

    $(2\implies 1)$ On a cette fois ci \[
        \frac{G_X'(t)-G_X'(1)}{t-1}\geq \sum_{k=0}^{N} k(k-1)\P(X=k)(1+t+\cdots +t^{n-1})
    \]
    donc avec $t\to 1^-$, \[
        G''_X(1)\geq  \sum_{n=0}^{N} n(n-1)\P(X=n)\geq 0
    \] 
    d'où la convergence de la série et \conc.
\end{proof}

\section{Somme aléatoire de v.a.r.d}

On note $(X_n)_{n\in \N^\star}$ une suite de vard sur $(\Omega, \mathcal  A, \P)$, mutuellement indépendantes à valeurs dans $\N$. On note $N:\Omega\to \N^\star$ une vard indépendante des $X_i$ et  \[
    S\defeq X_1+\cdots +X_N
\] 
On suppose que les $X_i$ ont même loi. On a  \[
    [S=n]=\bigcup_{k \in  \N^\star}[N=k,X_1+\cdots +X_k=n]
\] 
donc \[
    G_S(t)= \sum_{n\geq 0}\sum_{k\geq 1} \P(N=k)\P(X_1+\cdots +X_k=n)t^n=\sum_{k\geq 1}\P(N=k)G_X(t)^k =G_N\circ G_X(t)
\] 

\section{Résultats asymptotiques}

\begin{prop}
    \Hyp $(\Omega, \mathcal  A, \P)$ est un espace probabilisé, $X_n\suit \mathcal  B(n, p_n)$ avec $np_n \to  \lambda>0$
    \Conc Pour tout $ k\in \N$, \[
        \P(X_n=l) \xrightarrow[n\to+\infty]{}e^{-\lambda}\frac{\lambda^k}{k!}
    \]
\end{prop}

\begin{proof}
    Pas dur.
\end{proof}

\begin{thm}
    \Hyp $(\Omega, \mathcal  A, \P)$, $X_1,\cdots , X_n$ des vard $\indep$ de même loi avec variance (iid\footnotemark), $S_n=X_1+\cdots +X_n$ et $m=\E(X_1)$
    \Conc \[
        \forall  \epsilon>0,qa\P\left( \left| \frac{S_n}{n}-m\geq \epsilon \right| \right) \leq \frac{V(X_1)}{n\epsilon^2} \xrightarrow[n\to+\infty]{}0
    \] 
\end{thm}
\footnotetext{indépendantes et identiquement distribuées}

\begin{proof}
Tchebychev
\end{proof}

\section{Exemples}

\subsection{Loi du minimum}

On cherche la loi de $Y=\min(X_1,\cdots ,X_n)$ si $X_1,\cdots ,X_n\suit G(p)$ sont $\indep$. On a  \[
    \P(Y>k)=\P(X_1>k)^p=q^{kn}
\]
et \[
    \forall  k\geq 1, \qquad  \P(Y=k)=\P(Y>k-1)-\P(Y>k)=q^{kn-n}-q^{kn}=q^{(k-1)n}(1-q^n)
\] 
donc $Y\suit G(1-q^n)$ 

\subsection{Décomposabilité de la loi uniforme}

On note $Z$ une vard sur  $(\Omega, \mathcal A, \P)$ à valeurs dans $\N$. On dit que $Z$ est décomposable s'il existe  $X\indep Y$ presque sûrement non constantes telles que  $Z\sim X+Y$ (même loi). On va montrer que  $Z\suit \mathcal  U(\llbracket 0,n-1 \rrbracket )$ est indécomposable si et seulement si $n$ est premier.

\begin{itemize}
    \item Si $n=uv$ avec  $u,v\geq 2$, alors \[
            G_Z(t)=\frac{1}{uv} \frac{t^{uv}-1}{t-1}=\frac{1}{v} \frac{(t^u)^v-1}{t^u-1} \cdot \frac{1}{u} \frac{t^u-1}{t-1}.
    \]
    $\displaystyle \frac{1}{u} \frac{t^u-1}{t-1}$ est la fonction génératrice d'une vard qui suit une loi uniforme sur $\llbracket 0,u-1 \rrbracket $. Puis $\displaystyle \frac{1}{v}\frac{(t^u)^v-1}{t^u-1}$ est la fonction génératrice d'une vard qui suit une loi uniforme sur $u\cdot \llbracket 0,v-1 \rrbracket $
     Il existe donc $X\indep Y$ presque sûrement non constantes qui suivent des lois uniformes et telles que  $Z\sim X+Y$

 \item On suppose que $n=p$ est premier et que $Z$ est décomposable en $X+Y$, avec  $X\indep Y$ non constantes presque sûrement. On a alors \[
         G_Z(t)=G_X(t)G_Y(t)=\frac{1}{p} (1+t+\cdots +t^{p-1})
 \] 
 Si $G_X$ n'est pas polynômial, alors il y a une infinité de $k$ tels que $\P(X=k)t^k\neq 0$, et si $\P(Y=i)t^i\neq 0$, le coef de $t^{k+i}$ dans  $G_Z$ est non nul (produit de Cauchy avec des coefs positifs) donc  $G_Z$ n'est pas polynômial, ce qui est absurde. Donc $G_X$ et  $G_Y$ sont des polynômes.

 On suppose qu'il existe  $P, Q$ unitaires à coefficients positifs (les coefficients d'une série génératrice sont positifs) tels que  $P(t)Q(t)=1+\cdots +t^{p-1}$. Le polynôme $PQ$ divise $T^p-1$ donc ses racines sont unitaires et  \[
     P(T)=(T-a_1)\cdots (T-a_r)=\overline{P}(T)=\left( T-\frac{1}{a_1} \right) \cdots \left( T-\frac{1}{a_r} \right) = \frac{(-1)^r}{a_1\cdots a_r}T^rP\left( \frac{1}{T} \right) 
 \] 
 et vu les relations coefficients-racines, $\dfrac{(-1)^r}{a_1\cdots a_r}=1$ donc $P(T)=T^r P\left( \dfrac 1T \right) $ et $P$ est un polynôme réciproque. Par le même raisonnement,  $Q$ aussi. On note  $r=\deg P$ et  $s=\deg Q$. On suppose  $r\leq s$ et on a \[
     P(t)Q(t)=(t^r+\alpha_{r-1}t^{r-1}+\cdots +1)(t^s+\beta_{s-1}t^{s-1}+\cdots +1)=1+\cdots +t^{p-1}
 \] 
 donc (coef de $t^r$)\[
     \sum_{i+j=r}\alpha_i\beta_j= \underbrace{\alpha_r\beta_0}_{=1}+\underbrace{\alpha_{r-1}\beta_1}_{\geq 0}+\cdots +\underbrace{\alpha_0\beta_r}_{\geq 0}=1
 \] 
 donc $\alpha_{r-1}\beta_1=\cdots =\alpha_0\beta_r=0$ et $\beta_r=0$

 On va montrer que  $\beta_r$ ne peut pas être nul. On note  $\omega=\exp (\frac{2i\pi }{p})$ et les racines de $Q$ s'écrivent  $\omega^{i_1},\cdots ,\omega^{i_s}$. Le coefficient de $t^r$ dans  $Q$ vaut  \[
     (-1)^{s-r}\sum_{\substack{J\subset \llbracket 1,s \rrbracket \\ \#J=s-r}}\prod_{j\in J}\omega^{i_j}=(-1)^{s-r}(a_0+a_1\omega+\cdots +a_{p-1}\omega^{p-1})
 \]
 avec les $a_i$ non tous nuls. On note \[
     \hat{P} (X)=a_0+a_1X+\cdots +a_{p-1}X^{p-1} \in \Z[X]
 \]
 Le polynôme $1+X+\cdots +X^{p-1}$ est irréductible dans $\Z[X]$ (traité dans le chapitre de complément sur les polynômes), unitaire et annulateur de $\omega$ donc c'est son polynôme minimal.  $\hat{P}$ est donc proportionnel à ce polynôme (car on a supposé $\beta_r=0$) et $a_0=\cdots =a_{p-1}\in  \N^\star$. Puis $\hat{P}(1)=p\;\dom \hat{P}$ correspond aussi au nombre de termes dans la somme: \[
     p\;\dom \hat{P}=\binom s{s-r}
 \] 
 ce qui est absurde car $s<p$ donne  \[
     v_p\left( \binom s{s-r} \right) =v_p(s!)-v_p((s-r)!)-v_p(r!)=0
 \] 
\end{itemize}

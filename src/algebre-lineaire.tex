\ifsolo
~

\vspace{1cm}

\begin{center}
    \textbf{\LARGE Algèbre Linéaire} \\[1em]
\end{center}
\tableofcontents
\else
\chapter{Algèbre Linéaire}

\minitoc
\fi
\thispagestyle{empty}

\section{Rappels}

\begin{dfn}
    Pour un corps $\K$, $(E, +, \cdot)$ est un  $\K$-espace vectoriel\index{espace vectoriel} si \begin{itemize}
        \item $(E, +)$ est un groupe abélien
        \item $\forall \lambda \in \K,\quad \forall  x,y\in E,\quad \lambda(x+y)=\lambda x+\lambda y$
        \item $\forall \lambda,\mu \in  \K,\quad  \forall  x \in  E,\quad (\lambda+\mu)x=\lambda x+\mu x$ et $\lambda(\mu x)=(\lambda\mu)x$
        \item $\forall x \in  E, \quad  1_{\K}\cdot x=x$
    \end{itemize}
\end{dfn}

\subsection{Calculs dans un espace vectoriel}

\begin{rem}[Rappel]
    $(E, +, \cdot)$ un  $\K$-ev. Alors \begin{itemize}
        \item $\forall  x \in  E,\quad 0_{\K}=0_E$
        \item $\forall  \lambda \in  \K,\quad  \lambda \cdot 0_E=0_E$ et $\qquad \lambda x=0_E \iff \lambda=0_K$ ou $x=0_E$
        \end{itemize}
    \end{rem}

    \begin{dfn}
        On note $(E, +, \cdot)$ un  $\K$-ev et $I$ un ensemble non vide quelconque. On appelle  \textbf{famille de vecteurs}\index{famille de vecteurs} de $E$ indexée par  $I$ la donnée d'une fonction  $x:I\to E$. On la note $(x_i)_{i \in  I}$ avec $x_i=x(i)$

        On appelle \textbf{combinaison linéaire}\index{combinaison linéaire} de $(x_i)_{i \in  I}$ la donnée de $J\subset I$ fini, d'une famille  $(\lambda_j)_{j \in  J}$ de scalaires de $\K$ et du vecteur \[
            \sum_{j \in  J} \lambda_j x_j
        \]
    \end{dfn}

    \todo{Compléter cette section}

    \section{Les sous-espaces vectoriels}

    \subsection{Constructeurs d'espaces vectoriels}

    Si $(E, +, \cdot)$ et  $(F, +, \cdot)$ sont des  $\K$-ev alors $(E\times F, +, \cdot)$ est un  $\K$-ev avec les lois qui s'appliquent terme à terme.

    Si $X$ est un ensemble non vide, et $E$ est un  $\K$-ev, alors $\mathcal F(X,E)$ est un  $\K$-ev

    \subsection{Rappels}

    \begin{dfn}
        $F$ est un \textbf{sous-espace vectoriel}  du  $\K$-e.v. $E$  si et seulement si c'est un $\K$-e.v. pour les mêmes lois et qu'il est inclus dans $E$, ou bien de manière équivalente, $F$ est tel que  \begin{itemize}
            \item $F\subset E$
            \item  $F\neq \emptyset$
            \item  $ \forall  \lambda \in  \K, \quad  \forall  x, y \in  F, \qquad  \lambda x+y \in  F$
        \end{itemize}
    \end{dfn}

    \begin{defprop}
        \Hyp $E$ un $\K$-ev, $(F_i)_i$ une famille de sev de  $E$
        \begin{concenum}
        \item $\bigcap_{i \in I }\limits F_i $ sev de $E$
        \item On note $A$ une partie de  $E$, $\mathcal  E_A$ l'ensemble des sev de $E$ qui contiennent  $A$ \begin{enumerate}
            \item $\bigcap_{F \in  \mathcal  E_A}\limits F$ est le sous-espace engendré par $A$, on le note aussi  $\Vect(A)$ ou  $\langle A\rangle$
            \item  $F \in  \mathcal  E_A \implies  \Vect(A)\subset F$
            \item $\Vect(A)= \left\{ \sum_{i=1}^n\limits \lambda_ia_i,\qquad  n \in  \N, \lambda_i \in  \K,a_i \in  A \right\}$
        \end{enumerate}
    \end{concenum}
\end{defprop}

\begin{rem}
    Pour montrer qu'un ensemble est un sev on peut: revenir à la définition, reconnaître un espace engendré, montrer que c'est le noyau ou l'image d'une application linéaire.
\end{rem}

\subsection{Union de sous-espaces vectoriels}

On note $E$ un  $\K$-espace vectoriel. On va montrer que si $\K$ est infini, alors la réunion d'un nombre fini de sous espaces stricts de $E$ ne peut pas valoir  $E$.

Soient $E_1, \cdots, E_n$ des sev de $E$ tels que \[
    \bigcup_{i=1}^nE_i=E
\]
Quitte à éliminer les redondances et supposer $n$ minimal, on peut faire en sorte (sans modifier l'union) que \[
    E_1\not\subset \bigcup_{j=2}^n E_j
\]
Ainsi, si $u\in E_1\setminus \bigcup_{i=2}^nE_i$ et $v\in E\setminus E_1$, l'ensemble $v+\K u$ est disjoint de $E_1$ et contient au plus un vecteur de chaque autre espace vectoriel (car si $\alpha_1\neq \alpha_2$ sont tels que $v+\alpha_1 u, v+\alpha_2 u \in E_i$, alors $u \in  E_i$ absurde). Ainsi, \[
    v+\K u= \bigcup_{j=1} ^n (E_j\cap (v + \K u))
\]
est un ensemble fini de cardinal au plus $n-1$, ce qui est absurde si  $\K$ est infini.

\paragraph{Application aux endomorphismes cycliques.} On note $f \in  \mathcal  L(E)$ pour $E$ un  $\C$-ev et on suppose que $f$ n'a qu'un nombre fini de sous espaces stables. On définit les espaces (stables) suivants: \[
    \forall  x \in  E, \qquad  E_{f,x}\defeq \Vect(f^{k}(x), k \in  \N)
\]
Supposons que $\forall  x \in  E, E_{f,x}\neq  E$. On construit alors la suite $(x_k)_{k \in  \N^\star}$ ainsi: \begin{itemize}
    \item $x_1 \in  E\setminus \{0\} $
    \item $ x_2 \in  E\setminus E_{f,x_1}$ et $E \neq  E_{f,x_1}\cup E_{f,x_2}$
    \item $\cdots $
    \item $x_{n+1} \in  E \setminus  (E_{f,x_1}\cup \cdots \cup E_{f,x_n})$ et $E \neq  E_{f,x_1}\cup \cdots \cup E_{f,x_{n+1}}$
\end{itemize}
Les $E_{f,x}$ sont stables par $f$ est deux à deux distincts puisqu'on peut construire la suite. On en a une infinité, c'est absurde donc il existe  $x \in  E$ tel que $E_{f,x}=E$. On dit alors que $f$ est  \textbf{cyclique}\index{endomorphisme cyclique}

Si $E$ est de dimension finie, alors on note  $r$ le plus grand entier tel que  $(x, \cdots , f^{r}(x))$ est libre. Cette famille constitue alors une base de $E_{f,x}=E$ (si on rajoute un autre vecteur de la partie génératrice, c'est une CL d'éléments de la famille libre exprimée). La matrice de $f$ dans cette base s'écrit  \[
    \mathcal  M_{\mathcal  B}(f)= \begin{pmatrix}
        0 & \cdots & \cdots & 0 & a_0 \\
        1 & 0 & \ddots & \vdots & \vdots \\
        0 & 1 & \ddots & \vdots & \vdots \\
        \vdots & \ddots & \ddots & \vdots & \vdots \\
        0 & \cdots & 0 & 1 & a_r
    \end{pmatrix}
\]
C'est une matrice compagnon.

\section{Somme simple, somme directe}

On note $E$ un $\K$-ev, $E_1,\cdots ,E_n$ des sev de $E$ et on pose  \[
    E_1+\cdots +E_n= \{x_1+\cdots +x_n, \quad \forall  i \in  \llbracket 1,n \rrbracket , x_i \in  E_i\}
\]
C'est un sev de $E$ en tant qu'image d'une application linéaire.

\begin{thmdef}
    \Hyp $E_1,\cdots ,E_n$ sev de $E$
    \begin{concenum}
    \item La somme $E_1+\cdots +E_n$ est dite \textbf{directe}\index{somme directe} si \[
            \forall  (x_1,\cdots ,x_n) \in  E_1\times \cdots \times E_n, \qquad  x_1+\cdots +x_n =0 \implies (x_1, \cdots , x_n)=0
        \]
        On notera alors $E_1\oplus \cdots \oplus E_n$
    \item C'est équivalent à ce que l'une des trois propositions suivantes soit vérifiée: \begin{enumerate}
        \item $\forall  j \in  \llbracket 2, n \rrbracket , \qquad  (E_1+\cdots+E_{j-1})\cap E_j= \{0\} $
        \item $\dim(E_1+\cdots +E_n)=\dim E_1+\cdots +\dim E_n$
        \item La concaténations de bases des $E_i$ est une base de la somme simple.
    \end{enumerate}
\end{concenum}
\end{thmdef}

\begin{proof}~
    \begin{itemize}
        \item (def $\implies a$) Facile
        \item $(a\implies c)$ $\mathcal  B$ génératrice de $E_1+\cdots +E_n$, $ \mathcal  B_{\ell }=(e_{i,\ell })_i$ base de $E_{\ell }$. On considère une CL nulle, on isole un coefficient: il est nul. On itère.
        \item $(c \implies b)$ Immédiat
        \item $(b \implies c)$ Une famille génératrice du cardinal de la dimension est une base
        \item $(c \implies \text{def})$ On décompose sur la base concaténée: tous les coefs sont nuls donc les  $x_i$ sont nuls.
    \end{itemize}
\end{proof}

\subsection{Application linéaire définie par morceaux}

\begin{thm}
    \Hyp $E_1, \cdots , E_n$ des sev de $E$

    \begin{concenum}
    \item  $E_1\oplus\cdots \oplus E_n$ si et seulement si \[
            \forall  x \in  E_1+\cdots +E_n, \exists !(x_1,\cdots ,x_n) \in  E_1\times \cdots \times E_n, \quad  x=x_1+\cdots +x_n
        \]
    \item Dans ce cas, \begin{enumerate}
        \item L'application \[
                \begin{matrix}
                    \varphi:& E_1\times \cdots \times E_n & \longrightarrow & E_1\oplus \cdots \oplus E_n \\
                            & (x_1, \cdots , x_n) & \longmapsto & x_1+\cdots +x_n
                \end{matrix}
            \]
            est un isomorphisme
        \item On a : \[
                \forall  x \in  E_1\oplus \cdots \oplus E_n, \forall  i \in  \llbracket 1, n \rrbracket , \exists !x_i \in  E_i, \quad  \quad  x-x_i \in  \bigoplus_{\substack{j=1\\j\neq i}}^n E_j
            \]
            On note $\pi_i(x)=x_i$
        \item Les  $\pi_i$ sont des projecteurs et  $\sum \pi_i$ est l'application identité sur  $E_1\oplus\cdots \oplus E_n$. On les appelle les \textbf{projecteurs associés à la décomposition}
    \end{enumerate}
\item $F$ un  $\K$-ev, $f_1 \in  \mathcal  L(E_1, F), \cdots , f_n \in  \mathcal  L(E_n,F)$. \[
    \exists !f \in  \mathcal  L(E_1\oplus \cdots \oplus E_n, F), \qquad f_{|E_i}=f_i \]
    et \[
        f=\sum_i f_i\circ \pi_i
    \]
\end{concenum}
 \end{thm}

 \begin{proof}
     Facile
 \end{proof}

 \subsection{Lemme de décomposition et d'isomorphisme}

 Si $E=F\oplus G$ et  $H$, alors  \[
     \begin{matrix}
         \psi:& \mathcal  L(E, H) & \longrightarrow & \mathcal  L(F, H)\times \mathcal  L(G, H) \\
              & f & \longmapsto & (f_{|F}, f_{|G})
     \end{matrix}
 \]
 est un isomorphisme.

 Si $\pi_1, \pi_2$ sont les projecteurs associés à $(F_1\times \{0\} )\oplus (\{ 0\}\times F_2 )$, alors \[
     \begin{matrix}
         \varphi:& \mathcal  L(E, F_1\times  F_2) & \longrightarrow & \mathcal  L(E, F_1)\times \mathcal  L(E, F_2)  \\
                 & f & \longmapsto & (\pi_1\circ f, \pi_2\circ f)
     \end{matrix}
 \]
 est aussi un isomorphisme.

 \subsection{Lemme de factorisation}

 \paragraph{Premier lemme}
 Soient $u \in  \mathcal  L(E, F), v \in  \mathcal  L(E, G)$ tels que $\Ker u \subset \Ker v$. On va montrer qu'il existe  $w \in  \mathcal  L(F, G)$ telle que $v=w\circ u$.

 On note $V$ un supplémentaire de  $\Ker u$ dans  $E$.  $u_{|V}$ est un isomorphisme de $V$ dans  $\Img u$. On peut prendre  $w_{|\Img u}=v_{|V}\circ (u_{|V}^{-1})$ et $w$ nulle sur un supplémentaire de  $\Img u$

 \paragraph{Second lemme}
 $u \in  \mathcal  L(E, G), v \in  \mathcal  L(F, G)$ et $\Img u \subset \Img v$. Il existe  $w \in  \mathcal  L(E, F)$ telle que $u=v\circ w$

 La démonstration est similaire.

 \section{Théorème du rang}

 \begin{thm}[Rang\index{theoreme du rang@théorème du rang}]
     \Hyp $E, F$ des  $\K$-ev et $f \in  \mathcal  L(E, F)$
     \begin{concenum}
     \item Si $V$ est un supplémentaire de  $\Ker f$ dans  $E$ alors  $f_{|V}$ est un isomorphisme de $V$ dans  $\Img f$
     \item  $\dim E<+\infty \implies  \dim(\Img f)<+\infty$ et $\dim E=\dim\Ker f+\rg f$
     \end{concenum}
 \end{thm}

 \begin{proof}
     Simple (sup)
 \end{proof}

\begin{rem}
Le rang est invariant par composition de son argument avec un isomorphisme: si $u,v$ sont des isomorphismes,  $\rg f=\rg f\circ v=\rg u\circ f$
\end{rem}

\begin{exo}
    $f,g\in \mathcal  L(E)$. Montrer que \[
        |\rg f-\rg g|\leq \rg(f+g)\leq \rg f+\rg g
    \] 
\end{exo}

\begin{proof}[Résolution]
    $(f+g)(E)\subset f(E)+g(E)$ d'où l'inégalité de droite. Puis  \[
        \rg f = \rg (f+g-g)\leq \rg (f+g)+\rg (-g)=\rg (f+g)+\rg g
    \] 
    et par symétrie des rôles, on a l'autre inégalité.
\end{proof}

\section{Monotonie des noyaux itérés, décomposition de Fitting}

\begin{res}[Propriété d'essoufflement des noyaux itérés\index{essoufflement (propriété des noyaux itérés)}]
On note $f$ un endomorphisme en dimension finie $n$. La suite \[
    \left(\Ker(f^k)\right)_k.
\]
est strictement croissante sur $d$ premiers termes (on peut avoir $d=0$) puis stationnaire. De plus, la suite $(\delta_n)_n$ définie par \[
    \forall k\in \N, \delta_k=\dim(\Ker(f^{k+1}))-\dim(\Ker(f^k))
\]
est décroissante (cette monotonie est appelée \textbf{propriété d'essoufflement}).
\end{res}

\begin{proof}
    La croissance simple de la suite des noyaux est directe: si $f^k(x)=0$ alors $f(f^k(x))=f(0)=0$. On a forcément croissance stricte sur les premiers termes jusqu'à atteindre une égalité (éventuellement la croissance stricte ne concerne aucun termes). Cette égalité est atteinte car la suite $(\dim(\Ker(f^k)))$ est une suite entière croissante majorée donc stationnaire. Il reste à montrer que dès qu'une égalité est atteinte, la suite devient constante. Supposons \[
    \Ker(f^k)=\Ker(f^{k+1})
\]
Alors, \[
    \Ker(f^{k+2})=\{x, \quad f(f^{k+1}(x))=0\}=\{x, \quad f(f^k(x))=0\}=\Ker(f^k)
\]
La suite est donc bien strictement croissante puis stationnaire. On va maintenant montrer la décroissance de $(\delta_n)_n$. On a, en appliquant le théorème du rang,
\begin{align*}
    \dim(\Img(f^k))&=\dim\left(\Ker\left(f\left|_{\Img(f^k)}\right.\right)\right)+\dim\left(\Img\left(f\left|_{\Img(f^k)}\right.\right)\right)\\
    &=\dim\left(\Ker\left(f\left|_{\Img(f^k)}\right.\right)\right)+\dim(\Img(f^{k+1}))
\end{align*}
d'où \[
    \delta_k=\dim(\Img(f^k))-\dim(\Img(f^{k+1}))=\dim\left(\Ker\left(f\left|_{\Img(f^k)}\right.\right)\right)=\dim(\Img(f^k)\cap \Ker(f)).
\]
Or la suite des images est décroissante donc $(\delta_k)_k$ décroît.
\end{proof}

\begin{res}[Décomposition de Fitting\index{Fitting (décomposition de -- )}]
    Si $r$ est tel que  $\Ker f^r=\Ker f^{r+1}$, alors  $E=\Ker f^r\oplus \Img f^r$
\end{res}

\begin{proof}
    Si $x \in  \Img f^r\cap \Ker f^r$ alors $x=f^r(t)$ et  $f^r(x)=0$ donc  $t \in  \Ker f^{2r}=\Ker f^r$ et $x=0$, ce qui donne la somme directe. Puis on a la bonne dimension avec le théorème du rang.
\end{proof}

\section{Formes linéaires}

\subsection{Rappels}

\begin{dfn}
    Si $E$ est un  $\K$-ev, on appelle \textbf{forme linéaire}\index{forme linéaire} sur $E$ une application de  $\mathcal  L(E, \K)\defeq E^\star$. On appelle  \textbf{hyperplan} le noyau d'une forme linéaire non nulle.
\end{dfn}

\begin{thm}
\Hyp $E$ un  $\K$-ev, $H$ un sev de  $E$
\Conc Il y a équivalence entre \begin{enumerate}
    \item $H$ est un hyperplan de  $E$
    \item  $\forall   a \in  E\setminus H, \quad  H\oplus \K a=E$
    \item $ \exists a \in  E\setminus H, \quad  H\oplus \K a=E$
    \item $\dim H=\dim E-1$ si  $E$ est de dimension finie
    \item  $ \exists  \varphi \in  \mathcal  L(E, \K)\setminus \{0\}, \quad  H=\Ker \varphi$
\end{enumerate}
\end{thm}

\begin{proof}~\\
    $(1 \iff 5)$ c'est la déf \\
        $(1\implies 2)$ Si $\varphi$ est une FL non nulle telle que  $\Ker\varphi=H$ et si  $a \in  E\setminus H$ alors $\varphi(a)\neq 0$ et  \[
            x=\left(x-\frac{\varphi(x)}{\varphi(a)}a\right)+\frac{\varphi(x)}{\varphi(a)}a
        \] 
        d'où $E=H+\K a$. L'intersection vaut $\{ 0\} $ donc la somme est directe. \\
        $(2\implies 3)$ Trivial\\
        $(3\implies 1)$ $\varphi_{H}=0$, $\varphi(a)=1$.
\end{proof}

\subsection{Base antéduale}

\begin{exo}
On note E un $\K$-ev de dimension n et $(\varphi_1,\cdots,\varphi_n)$ une base de $E^\star$. Montrer qu'il existe $(x_1,\cdots,x_n)\in  E^n$ tels que $\varphi_i(x_j) = \delta_{i,j}$
\end{exo}

\begin{proof}
On note
\begin{align*}
    \psi : E &\longrightarrow \K^n\\
  x &\longmapsto (\varphi_1(x),\cdots,\varphi_n(x))
\end{align*}
Si $\psi$ n'est pas surjective alors $\dim \Img(\psi)<n$ donc $\Img \psi$ est inclus dans un hyperplan de $\K^n$. Il existe $(a_1,\cdots,a_n)\in \K^n\setminus \{0\}$ tels que
\[\forall x \in E, a_1 \varphi_1(x) + \cdots + a_n\varphi_n(x)=0\]
donc $(\varphi_1,\cdots,\varphi_n)$ est liée, ce qui est absurde.  L'application $\psi$ est donc une AL surjective et $\dim E= \dim \K^n=n$ donc $\psi$ est un isomorphisme.

On note $x_1=\psi^{-1}(1,0,\cdots,0),\;x_2= \psi^{-1}(0,1,\cdots,0)\;,\cdots,\;x_n= \psi^{-1}(0,,\cdots,1)$ et la famille $(x_1,\cdots,x_n)$ convient.

C'est une base car c'est l'image par $\psi^{-1}$ de la base canonique de $\K^n$. Cette base s'appelle la \textbf{base antéduale} de $(\varphi_1,\cdots,\varphi_n)$

\end{proof}

\subsection{Base duale}

Si on note $(e_1,\cdots,e_n)$ est une base de $E$, alors pour chaque $i$, on note $e_i^\star$ la forme linéaire donnée par $e_i^\star(e_j) \defeq \delta_{i,j}$.

La famille $(e_1^\star, \cdots , e_n^\star)$ est une famille de $n$ vecteurs en dimension  $n$, qui est libre donc c'est une base, appelée  \textbf{base duale}  de $(e_1, \cdots , e_n)$


\subsection{Un exercice classique}

\begin{exo}
    Soit $E$ un $\K$-ev de dimension finie $n$. On note $\varphi_1,\cdots,\varphi_p \in E^\star$

Montrer que $ \varphi \in \Vect(\varphi_1,\cdots,\varphi_p)\iff \Ker \varphi_1 \cap\cdots \cap \Ker \varphi_p \subset \Ker \varphi$
\end{exo}

\begin{proof}[Résolution] ~\\
    $(\implies )$ Si $\varphi \in \Vect(\varphi_1,\cdots,\varphi_p)$ alors il existe $a_1,\cdots,a_p\in \K$ tels que:
    \[ \varphi = a_1 \varphi_1 + \cdots + a_p \varphi_p \]
    et $\forall x \in \Ker \varphi_1 \cap\cdots \cap \Ker \varphi_p$,
    \[
        \varphi(x) = a_1 \varphi_1(x) + \cdots + a_p \varphi_p(x)
                = 0 + \cdots + 0 = 0 
            \]
    donc $x \in \Ker \varphi$ d'où l'inclusion et le sens direct. \\
    $(\impliedby )$
    On suppose que $(\varphi_1,\cdots,\varphi_p)$ est libre, et on complète en une base $(\varphi_1,\cdots,\varphi_p,\varphi_{p+1},\cdots,\varphi_n)$.  Ainsi, il existe $(a_1,\cdots,a_n)\in \K^n$ tels que 
    \[ \varphi = a_1 \varphi_1 + \cdots + a_n \varphi_n  \]
    On note $(x_1,\cdots,x_n)$ la base antéduale de $(\varphi_1, \cdots , \varphi_n)$. Alors, $x_{p+1} \in \Ker \varphi_1 \cap\cdots \cap \Ker \varphi_p$ donc $x_{p+1} \in \Ker(\varphi)$ ce qui donne $\varphi(x_{p+1}) = 0 = a_{p+1}$.
    En recommençant  pour $x_{p+2},\cdots,x_{n}$, on trouve $a_{p+1}=\cdots=a_n=0$ d'où la conclusion.
    
    Dans le cas général, on extrait de $(\varphi_1, \cdots , \varphi_p)$ une base de $\Vect(\varphi_1, \cdots , \varphi_p)$ qu'on note $(\varphi_1, \cdots , \varphi_r)$ (on renomme si besoin). On a $\Ker\varphi_1\cap \cdots\cap \Ker \varphi_r\subset \Ker \varphi_{r+1},\cdots ,\Ker\varphi_p$

\end{proof}

\subsection{Intersection}

On note $E$ un $ \K$-ev, $\varphi_1,\cdots,\varphi_p\in E^\star$ et $H_i = \Ker \varphi_i$.  On va montrer que $\dim(H_1\cap\cdots\cap H_p)\geq n-p$ avec égalité si et seulement si $(\varphi_1,\cdots,\varphi_p)$ est libre. \begin{itemize}
    \item $\dim H_1 \geq  n-1$
    \item $\varphi_{2|H_1}$ est une FL sur $H_1$ donc  $\dim(\Ker \varphi_{2|H_1})=\dim(\Ker \varphi_2\cap \Ker \varphi_1)=\dim H_1-1\geq n-2$
    \item $\cdots$
\end{itemize}
Puis il y a égalité si et seulement si on a une égalité à chaque étape.
\begin{itemize}
    \item $\varphi_1 \neq 0$ donc $(\varphi_1)$ libre.
    \item  $H_1\not\subset H_2$ donc  $\varphi_2 \not\in\Vect(\varphi_1)$ et  $(\varphi_1, \varphi_2)$ libre
    \item $\cdots $
\end{itemize}

\section{Idéaux de \texorpdfstring{$\mathcal  L(E)$ }{L(E)}}

On note $E$ un $\K$-ev de dimension $n$. Quitte à se donner une base  $\mathcal  B$, on a $\mathcal  L(E) \cong\mathcal  M_n(\K)$ (c'est un isomorphisme d'algèbres).

$I$ est un idéal à gauche (resp. à droite) si  $(I, +)$ est un groupe et  $\forall M \in  I, \forall  A \in  \mathcal  M_n(\K), MA \in  I$ (resp. $AM \in  I$). Un idéal est bilatère si c'est un idéal à droite et à gauche.

\subsection{Idéaux bilatères}

$I=\{ 0 \}$ en est un, on suppose désormais que $I\neq\{0\}$.  Il existe donc $A\in I$ tel que $\rg(A)=r>0$ donc il existe $P,Q\in \mathrm{GL}_n(\K)$ telles que 
\[
A = P \Jr Q \text{ donc } J_r = P^{-1}AQ^{-1} \in I.
\]
Puis $J_1=J_r J_1 \in I$. Les matrices de rang $1$ sont équivalentes à $J_1$ donc sont dans $I$, ce qui est le cas des matrices de la base canonique $E_{i,j}$, donc $I=\mathcal  M_n(\K)$

Les idéaux bilatères de $\mathcal M_n(\K)$ sont donc $\{0\}$ et $\mathcal M_n(\K)$.


\subsection{Idéaux à gauche}

\begin{rem}
    Un idéal de $\mathcal  M_n(\K)$ est toujours un espace vectoriel.
\end{rem}

On note $(A_1,\cdots,A_r)$ une base de $I$ et $M \in I$ de rang maximal.  On a $M \cdot \mathcal M_n(\K) \subset I$. Soit $A\in I$, si $\Img A\subset \Img M$ alors il existe $B$ tel que $A=MB$ (lemme de factorisation) donc $A\in M \mathcal M_n(\K)$.

Soit $A\in I$. Par l'absurde si $\Img A \not \subset \Img M$ alors $M$ non inversible et il existe $X$ un vecteur colonne tel que $AX\not \in \Img M$. On note $(X_1,\cdots,X_n)$ une base d'un supplémentaire de $\Ker M$.
On note $(X_{r+1},\cdots,X_n)$ une base de $\Ker M$ et on note $B$ telle que.
\[
    \begin{cases}
BX_1=\cdots=BX_r=0 \\
BX_{r+1}=X \\
BX_{r+2}=\cdots=BX_n=0
    \end{cases}
\]

La matrice $M+AB$ est dans $I$ et son image contient $(MX_1,\cdots,MX_r,AX)$ donc $rg(M+AB)\geq r+1 >r$ absurde donc $\Img A \not \subset \Img M$.

Les idéaux à gauche de $\mathcal M_n(\K)$ sont docn les ensembles du type $A \mathcal M_n(\K)$ avec $A \in \mathcal M_n(\K)$.


\section{Matrices élémentaires}

\subsection{Rappels}

\begin{rem}[Produit de matrices $E_{i,j}$]
    Dans $ \mathcal  M_n(\K)$,
\[
E_{i,j}E_{k,l}=\delta_{j,k}E_{i,l}
\] 
\end{rem}

\begin{dfn}
    On définit les matrices élémentaires\index{matrices élémentaires} suivantes: \begin{itemize}
        \item Matrices de transposition: \[
                T_{i,j}=I_n-E_{i,i}-E_{j,j}+E_{i,j}+E_{j,i}=
                \begin{gmatrix}[p]
                    1          &        &   &        &   &        &   &        &   &        & \\
                               & \ddots &   &        &   &        &   &        &   &  (0)   & \\
                               &        & 1 &        &   &        &   &        &   &        & \\
                               &        &   & 0      &   & \cdots &   & 1      &   &        & \\
                               &        &   &        & 1 &        &   &        &   &        & \\
                               &        &   & \vdots &   & \ddots &   & \vdots &   &        & \\
                               &        &   &        &   &        & 1 &        &   &        & \\
                               &        &   & 1      &   & \cdots &   & 0      &   &        & \\
                               &        &   &        &   &        &   &        & 1 &        & \\
                               &  (0)   &   &        &   &        &   &        &   & \ddots & \\
                               &        &   &        &   &        &   &        &   &        & 1
                       \colops
                       \swap[i][j]37
                       \rowops
                       \swap[i][j]37
                \end{gmatrix}
        \] 
    \item Matrices de transvection: pour $i\neq j$, \[
            T_{i,j}(\lambda)=I_n+\lambda E_{i,j}= \begin{pmatrix}
        1 &   &        &         & \\
          & 1 &        & \lambda & \\
          &   & \ddots &         & \\
          &   &        & 1       & \\
          &   &        &         & 1
    \end{pmatrix}
    \] 
\item Matrices de dilataion: \[
    D_i(\lambda)=I_n+(\lambda-1)E_{i,i}= \begin{pmatrix}
        1   &        &   &         &   &        & \\
            & \ddots &   &         &   &        & \\
            &        & 1 &         &   &        & \\
            &        &   & \lambda &   &        & \\
            &        &   &         & 1 &        & \\
            &        &   &         &   & \ddots & \\
            &        &   &         &   &        & 1
    \end{pmatrix}
\] 
    \end{itemize}
\end{dfn}

\subsection{Actions des matrices élémentaires}

La multiplication à gauche par une matrice élémentaire agit sur les lignes, et la multiplication à droite agit sur les colonnes. \begin{itemize}
    \item La multiplication à gauche par $T_{i,j}$ fait $L_i \leftrightarrow L_j$
    \item La multiplication à gauche par $T_{i,j}(\lambda)$ fait $L_i \leftarrow L_i+\lambda L_j$
    \item La multiplication à gauche par $D_i(\lambda)$ fait  $L_i \leftarrow \lambda L_i$
    \item La multiplication à gauche par $E_{i,j}$ fait $L_k \leftarrow \delta_{k,i}L_j$
\end{itemize}
Les opérations sont identiques mais avec les colonnes pour la multiplication à droite

\subsection{Générateurs de \texorpdfstring{$\mathrm{GL}_n(\K)$ }{GLn(K)} et \texorpdfstring{$\mathrm{SL}_n(\K)$ }{SLn(K)}}

On va montrer que les éléments de $\mathrm{GL}_n(\K)$ s'écrivent comme un produit de transvections et d'une dilatation.

\begin{rem}
    L'inverse d'une transvection est une transvection: $T_{i,j}(\lambda)^{-1}=T_{i,j}(-\lambda)$
\end{rem}

On procède par récurrence pour montrer que si $A$ est inversible, il existe  $T_1, \cdots , T_r$ des transvections telles que \[
    T_1\cdots T_rA=D_n(\det A)
\] \begin{itemize}
    \item L'initialisation est trivialle: pour $n=1$, toutes les matrices sont des matrices de dilatation.
    \item Soit  $n> 1$. On suppose la propriété vraie au rang  $n-1$ et on note  $A \in  \mathrm{GL}_{n}(\K)$. On note $(a_{i,j})$ les coefficients de $A$. Quitte à multiplier par une transvection, on peut supposer  $a_{1,1}\neq 0$ et l'utiliser comme pivot: \begin{align*}
            \left( \begin{array}{c}
                    a_{1,1}  \\
                    \vdots \\
                    \vdots \\
                    a_{n,1}
            \end{array}
            \hspace{2em}\star\hspace{2em}
            \right) &\xrightarrow{\text{pivot avec} a_{1,1}} \left( \begin{array}{c|ccc}
                a_{1,1} & & \star &\\
            \hline 0 \\
            \vdots & &\star &\\
            0
        \end{array}
         \right) 
        \\
                    & \xrightarrow{L_2 \leftarrow L_2+L_1}\left( \begin{array}{c|ccc}
                a_{1,1} & & \star &\\
            \hline a_{1,1} \\
            \vdots & &\star &\\
            0
        \end{array}
         \right) 
        \\
                    & \xrightarrow{L_1 \leftarrow L_1+(1-a_{1,1})L_2}\left( \begin{array}{c|ccc}
                1 & & \star &\\
            \hline a_{1,1} \\
            \vdots & &\star &\\
            0
        \end{array}
         \right) 
        \\
                    & \xrightarrow{L_2 \leftarrow L_2-a_{1,1}L_1}\left( \begin{array}{c|ccc}
                1 & & \star &\\
            \hline 0 \\
            \vdots & &\star &\\
            0
        \end{array}
         \right) 
        \\
                    & \xrightarrow{\;\;\mathrm{HR}\;\;}\left( \begin{array}{c|cccc}
                1 & &  &\star \\
                \hline 0 & 1\\
            \vdots & &\ddots &\\
            \vdots & & & 1 \\
            0 & & & & \det A
        \end{array}
         \right) 
        \\
                    & \xrightarrow{C_i \leftarrow C_i-a_{1,i}C_1}\left( \begin{array}{ccccc}
                1 & &  & \\
             & 1\\
             & &\ddots &\\
             & & & 1 \\
             & & & & \det A
        \end{array}
         \right) 
     \end{align*}
\end{itemize}
d'où la conclusion.

\begin{exo}
    Montrer que $\mathrm{SL}_n(\K)=\ker (\det)= \{A \in  \mathrm{GL}_n(\K), \det A=1\} $ est engendré par les matrices de transvection
\end{exo}

\begin{proof}[Résolution]
On vient de le voir.
\end{proof}

\section{Matrices}

\subsection{Rappels}

$\mathcal  B=(e_1, \cdots , e_p)$, $\mathcal  F=(f_1, \cdots , f_n)$ des bases de $E, F$ et  $f \in  \mathcal  L(E, F)$. On note $\mathcal  M_{\mathcal  B, \mathcal  F}(f)$ la matrice de taille $n\times p$ dans laquelle la colonne  $i$ est le vecteur des coordonées de  $f(e_i)$ dans le base  $\mathcal  F$

Si $\mathcal  B'=(e_1', \cdots , e_p')$ est une base de $E$ alors on définit la matrice de passage  $P_{\mathcal  B, \mathcal  B'}$ par  \[
    P_{\mathcal  B, \mathcal  B'}\defeq \mathcal  M_{\mathcal  B, \mathcal  B'}(f)
\] 
où $f:e_i\longmapsto e_i'$. Si $x$ s'écrit  $X_{\mathcal  B}$ dans $\mathcal  B$ et $X_{\mathcal  B'}$ dans $\mathcal B'$, alors \[
X_{\mathcal  B}=P_{\mathcal  B, \mathcal B'}X_{\mathcal  B'}
\] 

\subsection{Interprétation combinatoire}

On note $G=(S, A)$ un graphe simple orienté avec  $|S|=n$. La matrice d'adjacence  $M \in  \mathcal  M_n(\K)$ est la matrice de terme général $(\1_A((i, j)))_{i, j \in  \llbracket 1, n \rrbracket }$ 

\begin{exo}
    Montrer que $[A^m]_{i,j}$ est le nombre de chemins de longueur $m$ de  $i$ vers  $j$. On pourra procéder par récurrence sur  $m$.
\end{exo}

\subsection{Crochet de Lie}

On s'intéresse au commutateur comme crochet de Lie dans $\mathcal  M_n(\K)$: $[A, B]\defeq AB-BA$

 \begin{exo}
Montrer l'identité de Jacobi: \[
    [A, [B, C]]+[B, [C, A]]+[C, [A, B]]=0
\]
\end{exo}

\begin{exo}
    Si $A$  et  $[A, B]$ commutent, alors  \[
        \forall  k \in  \N^\star, \qquad [A^k,B]=k[A, B]A^{k-1}
    \] 
\end{exo}
\begin{proof}
    Par récurrence en écrivant $AB=[A, B]+BA$
\end{proof}

\begin{exo}
    Montrer que si $A \in  \mathcal  M_n(\C)$ est de trace nulle, alors  il existe $B, C$ tels que  $A=[B, C]$
\end{exo}
\begin{proof}[Résolution]
    On commence par le cas où $A$ est de diagonale nulle. On pose  $B=\diag(1, \cdots , n)$. Si $C$ existe alors  \[
        A=[B, C] \underset{\text{calcul}}\iff \forall  i, j, \quad  A_{i,j}=(i-j)C_{i,j}
    \] 
    donc la matrice $C$ existe bien (on prend $0$ pour la diagonale)

 
    On va maintenant montrer que toute matrice de trace nulle est semblable à une matrice de diagonale nulle, ce qui conclura. On procède par récurrence sur la dimension. \begin{itemize}
        \item L'initialisation est évidente
        \item $A \in \mathcal  M_{n+1}(\C)$, $\Tr(A)=0$. Il y a deux cas:  \begin{itemize}
            \item $ \forall  X \in  \mathcal  M_{n+1,1}(\C), \quad  (X, AX)$ liée. Donc $A=\lambda I_{n+1}=0$.
            \item $ \exists  X \in  \mathcal  M_{n+1,1}(\C), \quad  (X, AX)$ libre.
        \end{itemize}
        Il n'y a que le second cas à traiter. On complète $(X, AX)$ en une base  $\mathcal  B$ et la matrice de $A$ dans cette base s'écrit  \[
            \left( \begin{array}{c|ccc}
                    0 & & \star& \\
                    \hline 1\\
                    0 & &  \\
                    \vdots&\phantom{0}&A' \\
                    0
            \end{array}
             \right) 
        \] 
        et $\Tr(A')=\Tr(A)$. Puis l'hypothèse de récurrence permet de conclure (considérer les matrices $\hat{P}=\diag(1, P)$ pour $P \in  \mathcal  M_n(\C)$ matrice de passage)
    \end{itemize}
\end{proof}

\section{Déterminants}

\subsection{Rappels}

On note $\mathcal  B=(e_1, \cdots , e_n)$ une base de $E$ et  $\mathcal  F=(f_1, \cdots , f_n)$ une famille de vecteurs de $E$. On note  $f_{i,j}$ la coordonnée de $f_j$ sur  $e_i$ dans $\mathcal  B$. \[
    \det_{\mathcal  B}(\mathcal  F)=\sum_{\sigma \in  \mathfrak S_n}\epsilon(\sigma)f_{1,\sigma(1)}\cdots x_{n, \sigma(n)}
\] 

\begin{thm}
    \Hyp $E$ un  $\K$-ev de base $\mathcal  B=(e_1, \cdots , e_n)$
    \Conc $\det_{\mathcal  B}$ est l'unique forme $n$-linéaire alternée dont l'image de $\mathcal  B$ est $1$
\end{thm}

\begin{prop}
\Hyp $\mathcal  B, \mathcal  C$ des bases de $E$
\Conc  $\det_{\mathcal  C}=\det_C(\mathcal  B)\det_{\mathcal  B}$
\end{prop}

\begin{defprop}
    \Hyp $\mathcal  B$ est une base de $E$ un $\K$-ev de dimension finie,  $f,g \in  \mathcal  L(E)$
    \begin{concenum}
    \item $\det f$ est l'unique scalaire tel que  \[
            \forall  x_1, \cdots , x_n \in  E, \qquad  \det_{\mathcal  B}(f(x_1), \cdots , f(x_n))=\det f\times \det_{\mathcal  B}(x_1, \cdots , x_n)
    \] 
\item $\det(f \circ g)=\det f \times \det g$
\item  $f \in  \mathrm{GL}_n(\K) \iff  \det f \neq 0$
    \end{concenum}
\end{defprop}

\subsection{Déterminants classiques}

\paragraph{Vandermonde}

Pour $(\lambda_1, \cdots , \lambda_n) \in  \K$, on note \[
    V(\lambda_1, \cdots , \lambda_n)= \begin{vmatrix}
        1 & \lambda_1 & \cdots & \lambda_1^{n-1} \\
        1 & \lambda_2 & \cdots & \lambda_2^{n-1} \\
        \vdots & & & \vdots \\
        1 & \lambda_n & \cdots & \lambda_n^{n-1}
    \end{vmatrix}
\] 
On montre par récurrence que \[
    V(\lambda_1, \cdots , \lambda_n)=\prod_{i<j}(\lambda_j-\lambda_i)
\] 
\begin{itemize}
    \item Initialisation claire
    \item On se place dans le cas où les $\lambda_i$ sont deux à deux distincts (car sinon deux lignes sont identiques, le déterminant et nulle et la formule reste vraie). On pose\[
            P(X)=V(\lambda_1, \cdots , \lambda_n, X)= \begin{vmatrix}
                1 & \lambda_1 & \cdots & \lambda_1^{n-1} & \lambda_1^n\\
        1 & \lambda_2 & \cdots & \lambda_2^{n-1}  & \lambda_2^n\\
        \vdots & & & &\vdots \\
        1 & \lambda_n & \cdots & \lambda_n^{n-1} & \lambda_n^n \\
        1 & X & \cdots & X^{n-1} & X^n
    \end{vmatrix}
    \] 
    $P$ est un polynôme de degré  $n$, qui admet  $\lambda_1, \cdots , \lambda_n$ pour racines distinctes donc il s'écrit \[
        P(X)=\mu(X-\lambda_1)\cdots (X-\lambda_n)
    \] 
    et en développant par rapport à la dernière colonne, on identifie \[
        \mu=V(\lambda_1, \cdots , \lambda_n)
    \] 
    donc \[
        V(\lambda_1, \cdots , \lambda_{n+1})=P(\lambda_{n+1})=\prod_{i<j}(\lambda_j-\lambda_i)
    \] 
\end{itemize}

\paragraph{Déterminant tridiagonal}

\[
D_n = \begin{vmatrix}
a & c & 0 & \dots & 0\\
b & a & c & \ddots & 0\\
0 & b & a & \ddots & 0\\
\vdots & \ddots & \ddots &\ddots &\vdots \\
0 & 0 & 0 & \dots & a 
\end{vmatrix}
\] 
On trouve facilement \[
D_{n+2}=aD_{n+1}-bcD_n
\] 
C'est une suite qui satisfait une relation de récurrence linéaire d'ordre $2$, on connait  $D_1=a, D_0=1$ donc sait en trouver une forme close.

\paragraph{Déterminant circulant}

On appelle \textbf{déterminant circulant} le déterminant de la matrice suivante:
\[  D = \begin{pmatrix} a_0 & a_1 & \cdots & a_{n-1} \\ a_{n-1} & a_0 & \cdots & a_{n-2} \\ \vdots & \vdots & \ddots & \vdots \\ a_1 & a_2 & \cdots & a_0 \end{pmatrix} .\]
On va montrer que
\[ \det D = \prod_{k=0}^{n-1} P(\omega^k)\]
où $\omega$ est une racine primitive $n$-ième de l'unité et $\displaystyle P = \sum_{k=0}^{n-1} a_k X^k$.  

Pour $i\in\llbracket 0, n-1\rrbracket$, on pose
\[V_i = \begin{pmatrix} 1 \\ \omega^{i} \\ \vdots \\ \omega^{i(n - 1)} \end{pmatrix}.\]
Remarquons alors que $DV_i = P(\omega^i)V_i$. En posant la matrice $Q = (V_0, V_1,\dots, V_{n-1} )$ il vient \linebreak$DQ = Q\times \diag(P(1), P(\omega),\dots, P(\omega^{n-1}) $ donc nous avons \[\det(D)\det(Q) = \det(Q)\prod_{i = 0}^{n - 1}P(\omega^i).\]
Et le déterminant de $Q$ est non nul ($\det Q = V(1, \omega, \dots, \omega^{n-1})$ avec $V$ le déterminant de Vandermonde et les $\omega^i$ sont deux à deux distincts) ce qui conclut la preuve. 

\subsection{Comatrice}

\begin{dfn}
    Le \textbf{mineur} d'indice $i,j$ de  $A \in  \mathcal  M_n(\K)$ est $\det(A')$ où  $A'$ est la matrice  $A$ privée de la ligne  $i$ et de la colonne  $j$ . Si $i=j$, on dit que c'est un mineur principale. On note  $\Delta_{i,j}(A)$ les mineurs de $A$.

    On appelle  \textbf{comatrice}\index{comatrice} de $A$ la matrice  $\Com(A)=((-1)^{i+j}\Delta_{i,j}(A))$. Les coefficients de cette matrice sont appelés \textbf{cofacteurs}.
\end{dfn}

\begin{thm}
    \Hyp $A \in  \mathcal  M_n(\K)$
    \begin{concenum}
    \item $ \forall  i \in  \llbracket 1, n \rrbracket , \qquad  \displaystyle \det A=\sum_{j=1}^n (-1)^{i+j}\Delta_{i,j}(A)$
    \item $ \forall  j \in  \llbracket 1, n \rrbracket , \qquad  \displaystyle \det A=\sum_{i=1}^n (-1)^{i+j}\Delta_{i,j}(A)$
    \item $A\Com(A)^\top = \Com(A)^\top A=\det A I_n$
    \end{concenum}
\end{thm}

% TODO: taper cette partie ? Flemme monumentale de typeset les blocs
% \section{Matrices et déterminants par blocs}

\section{Système d'équation linéaire}

On appelle système linéaire d'inconnues $x_1, \cdots , x_p$ un système du type \[
\begin{cases}
    a_{1,1}x_1+\cdots +a_{1,p}x_p=b_1\\
    \vdots \\
    a_{n,1}x_1+\cdots +a_{n,p}x_p=b_p
\end{cases}
\] 

\subsection{Interprétation matricielle}

On pose $A=(a_{i,j}) _{\substack{1\leq i\leq n\\1\leq j\leq p}}$, $\displaystyle B= \begin{pmatrix}
    b_1\\ \vdots \\ b_n
\end{pmatrix} $ et $\displaystyle X= \begin{pmatrix}
    x_1 \\ \vdots \\ x_p
\end{pmatrix}$. $(x_1, \cdots , x_p)$ est solution si et seulement si $AX=B$.

Il y a une unique solution si  $A$ est inversible, c'est à dire  $n=p$ et  $A \in  \mathrm{GL}_n(\K)$. On dit alors que c'est un système de Cramer et $X=A^{-1}B$ est l'unique solution.

Dans le cas général, il y a une solution si et seulement si $B \in  \Img (A)$. Dans ce cas là, pour $X_0$ solution, l'ensemble des solutions est  \[
    \{X \in \mathcal M_{p,1}, \qquad  X-X_0 \in  \Ker(A)\} 
\] 


\subsection{Interprétation géométrique}

On note $\varphi_i: x \longmapsto a_{i,1}x_1+\cdots +a_{i,p}x_p$ des formes linéaires. Les équations $\varphi_i(x)=b_i$ sont des équations d'hyperplans affines. Ainsi,  $x$ est solution  si et seulement si $x$ dans  l'intersection des hyperplans d'équations  $\varphi_i(x)=b_i$

\subsection{Interprétation vectorielle}

Si $C_1, \cdots , C_p$ sont les colonnes de $A$, alors  \[
    x \text{ solution } \iff  x_1C_1+\cdots +x_pC_p=B
\]
Dans le cas d'un système de Cramer, pour $x$ solution, \[
    \det(B, C_2, \cdots , C_n)=\det(x_1C_1+\cdots +x_nC_n, C_2, \cdots , C_n)=x_1 \det A
\] 
donc  \[
    x_1= \frac{\det(B, C_2, \cdots , C_n)}{\det A}.
\]
De même, \[
    x_j= \frac{\det(C_1, \cdots , C_{j-1}, B, C_{j+1},\cdots ,C_n)}{\det A}
\] 

\endchapter

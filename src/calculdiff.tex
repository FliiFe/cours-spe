\ifsolo
~

\vspace{1cm}

\begin{center}
    \textbf{\LARGE Calcul différentiel} \\[1em]
\end{center}
\tableofcontents
\else
\minitoc
\fi
\thispagestyle{empty}

\ifsolo \newpage \setcounter{page}{1} \fi
\section{Notions, Définitions}

$E$ est un $\mathbb R$-e.v de dimension $n$ (donc isomorphe à $\mathbb R^n$), $F$ un sev de $E$ et $\Omega$ un ouvert de $E$.

\begin{rem}
    Pour $a\in\Omega, v\in E, \exists \epsilon>0, \forall t\in ]-\epsilon, \epsilon[, a+tv\in\Omega$ (par définition d'un ouvert)
\end{rem}

\begin{dfn}
    On note $f:\Omega \longrightarrow F$. On dira que $f$ possède une dérivée\index{dérivée partielle} en $a\in\Omega$ selon $v\in E$ si \[
        \lim_{\substack{t\to 0\\ t\neq 0}}\frac{f(a+tv)-f(a)}t
    \] existe. On note cette limite $D_vf(a)$.

    Si $\mathcal B=(e_1,\cdots, e_n)$ base de $E$ et $f$ a une dérivée en $a\in\Omega$ selon $e_{i_0}$ alors on appelle $i_0$-ième dérivée partielle $D_{e_{i_0}}f(a)$, notée \[
        \frac{\partial f}{\partial x_{i_0}}(a)\qquad \text{ ou }\qquad \partial_{i_0}f(a)
    \]
\end{dfn}

\begin{rem}
    Pour $\mathcal B=(e_1, \cdots, e_n)$ on note $x\underset{\mathcal B}{\longleftrightarrow}(x_1, \cdots, x_n)$ et on a $f(x)=f(x_1e_1+\cdots + x_ne_n)$ qu'on note abusivement $f(x_1, \cdots, x_n)$. \[
        \frac{f(x+te_1)-f(x)}{t}=\frac{f(x_1+t, x_2, \cdots, x_n)-f(x_1, \cdots, x_n)}{t}
    \]
    Pour calculer $\partial_if(a)$, on fait comme si les autres variables étaient constantes et on dérive par rapport à $x_i$.
\end{rem}

\begin{rem}
    $D_v$ est linéaire
\end{rem}

\begin{rem}
    Avoir une dérivée en $a$ n'implique pas la continuité en $a$. Contre-exemple: \[
        f(x, y)=\begin{cases}
            \frac{y^2}x &\text{ si }x\neq 0\\ y & \text{ sinon}
        \end{cases}
    \]
\end{rem}

\section{Différentielle}

\begin{defprop}
    \Hyp $f:\Omega\longrightarrow F$, $a\in \Omega$. \begin{concenum}
    \item On dira que $f$ est différentiable\index{différentielle} en $a$ s'il existe $\varphi\in\mathcal L(E, F), \epsilon>0$ tels que \[
            \forall h\in\mathcal B_o(0, \epsilon), f(a+h)=f(a)+\varphi(h)+o_0(h)
        \]
        Dans ce cas $\varphi$ est unique et se note $\diff f_a$
    \item Si $f$ est différentiable en $a$ alors $f$ est continue en $a$
    \item Si $f$ est différentiable en $a$ alors $f$ a une dérivée en $a$ selon tout $v$ et \[
            D_vf(a)=df_a(v)
        \]
    \end{concenum}
\end{defprop}

\begin{proof} ~
    \begin{enumerate}
        \item Si $\varphi, \psi$ conviennent, $\varphi(tv)-\psi(tv)=o_0(tv)$ ie \[
                \varphi(v)-\psi(v)=o_0(v)\xrightarrow[t\to 0]{}0
            \]
            d'où $\varphi\equiv \psi$.
        \item $f(a+h)=f(a)+\varphi(h)+o_0(h)\xrightarrow[h\to 0]{}f(a)$
        \item \[
                \frac{f(a+tv)-f(a)}{t}=\diff f_a(v)+o_0(v)\xrightarrow[\substack{t\to 0\\t\neq 0}]{}\diff f_a(v)
            \]
    \end{enumerate}
\end{proof}

\begin{thmdef}
    ~ \begin{enumerate}
        \item Soit $f\in\mathcal L(E, F)$ et $a\in E$. La fonction $f$ est différentiable en $a$ et $df_a=f$.
        \item Si $\mathcal B=(e_1, \cdots, e_n)$ est une base de $E$ et si $\pi_i$ est l'application $i$-ième coordonnée alors $\forall a\in E, \diff \pi_i=\pi_i$ se dépend pas de $a$. On note cette application $\diff x_i$.
        \item Si $\mathcal B$ est une base de $E$ et $f$ est différentiable en $a$ alors \[
                \diff f_a=\sum_{i=1}^n\frac{\partial f}{\partial x_i}(a)\diff x_i
            \]
    \end{enumerate}
\end{thmdef}

\begin{proof} ~
    \begin{enumerate}
        \item $f(a+h)=f(a)+f(h)$
        \item Ok
        \item $h=h_1e_1+\cdots +h_ne_n$, \[
                \diff f_a(h)=h_1\diff f_a(e_1)+\cdots +h_n\diff f_a(e_n)=\sum_{i=1}^n\frac{\partial f}{\partial x_i}(a)\diff x_i
            \]
    \end{enumerate}
\end{proof}

\begin{rem}
    Pour une fonction $f:\mathbb R\longrightarrow F$, $f'(a)=\diff f_a(1)$
\end{rem}

\section{Calculs de différentielles}

\subsection{Quelques exemples}
\begin{ex} Étude en $0$ de
    \[
        f(x, y)=\begin{cases}
            \frac{y^2}x &\text{ si }x\neq 0\\ y & \text{ sinon}
        \end{cases}
    \]
\end{ex}

\[
    \frac{\partial f}{\partial x}(0, 0)=0 \qquad\qquad \frac{\partial f}{\partial y}(0, 0)=f(0, 1)=1
\]
donc \[
    \frac{\partial f}{\partial x}(0, 0) \diff x+ \frac{\partial f}{\partial y}(0, 0)\diff y
\]
existe mais $f$ est non différentiable car non $\mathcal C^0$

\begin{ex} Calcul de la différentielle de
    {$f:A\in\mathcal M_n(\mathbb R)\longmapsto A^2$}
\end{ex}

\[
    f(A+H)=f(A)+AH+HA+H^2
\]
et pour une norme sous-multiplicative, $\|H^2\|\leq \|H\|^2=o_0(H)$ donc \[
    \diff f_A(H)=AH+HA
\]

\begin{ex} Calcul de la différentielle de
    {$f:M\in GL_n(\mathbb R)\longmapsto M^{-1}$}
\end{ex}

On fixe $M\in GL_n(\mathbb R)$. Alors $\exists \epsilon >0 / \mathcal B_o(M, \epsilon)\subset GL_n(\mathbb R)$. Pour $H$ dans $\mathcal B_o(0, \epsilon)$, \[
    M+H\in\mathcal B_o(M, \epsilon)
\]
et \[
    \left((M+H)^{-1}-M^{-1}\right)(M+H)=I_n-M^{-1}(M+H)=-M^{-1}H
\] donc
\begin{align*}
    (M+H)^{-1}-M^{-1}&=-M^{-1}H(M+H)^{-1}\\&=-M^{-1}HM^{-1}+M^{-1}Ho_0(1)\\&=\underbrace{-M^{-1}HM^{-1}}_{\diff f_M(H)}+o_0(H)
\end{align*}

\begin{exo} Calcul de la différentielle de
    {$f:A\in\mathcal M_n(\mathbb R)\longmapsto A^\intercal A$}
\end{exo}

\needspace{5\baselineskip}
\subsection{Interprétation matricielle}

\begin{thmdef}
    On note $f:\Omega\to \mathbb R^m$, $\Omega\subset \mathbb R^n$, on écrit $f=(f_1, \cdots, f_m)$. On suppose que $f$ est différentiable en $a\in\Omega$ et on note $\mathcal C_n$ (resp. $\mathcal C_m$) la base canonique de $\mathbb R^n$ (resp. $\mathbb R^m$). Alors: \begin{enumerate}
        \item On appelle matrice jacobienne\index{jacobienne (matrice -- )} de $f$ en $a$ la matrice \[
                J_f(a)=\left(\frac{\partial f_i}{\partial x_j}(a)\right)_{\substack{1\leq i\leq n\\ 1\leq j\leq n}}
            \]
        \item Si $x\underset{\mathcal C_n}{\longleftrightarrow}X$ alors \[
                \diff f_a(x)\underset{\mathcal C_m}\longleftrightarrow J_f(a)X
            \]
    \end{enumerate}
\end{thmdef}

\begin{proof}
    \[
        \diff f_a(x)=\sum_{i=1}^n\underbrace{\frac{\partial f}{\partial x_i}(x)}_{\displaystyle \left(\frac{\partial f_1}{\partial x_i}(x), \cdots, \frac{\partial f_m}{\partial x_i}(x)\right)}\overbrace{\diff x_i(x)}^{=x_i} \qquad \underset{\mathcal C_n}{\longleftrightarrow}\qquad \begin{pmatrix}
            \sum_{i=1}^n\limits \frac{\partial f_1}{\partial x_i}(a)x_i \\
            \vdots \\
            \sum_{i=1}^n\limits \frac{\partial f_m}{\partial x_i}(a)x_i
        \end{pmatrix}=J_f(a)X
    \]
\end{proof}

\section{Opérations sur les différentielles}

\begin{prop}
    Si $f,g:\omega\subset E\to F$ sont différentiables en $a$, alors pour $\lambda\in\mathbb R$, $\lambda f+g$ est différentiable en $a$ et $\diff (\lambda f+g)_a=\lambda df_a+\diff g_a$ \index{différentielle!opérations}
\end{prop}

\begin{prop}
    \Hyp $B:E_1\times E_2\to F$ bilinéaire, $f_1:\Omega_1\to E_1$ différentiable en $a_1$, $f_2:\Omega_2\to E_2$ différentiable en $a_2$.
    \Conc $f=B(f_1, f_2)$ est différentiable en $a=(a_1, a_2)$ et \[\diff f_a(h_1, h_2)=B(\diff {f_1}_{a_1}(h_1), f_2(a_2))+B(f_1(a_1), \diff {f_2}_{a_2}(h_2)) \]
\end{prop}

\begin{proof}
    On développe $f(a+h)=B(f_1(a_1+h_1), f_2(a_2+h_2))$
\end{proof}

\begin{ex} ~
    \begin{itemize}
        \item $(E, (\; |\; ))$ euclidien, $\varphi:(x, y)\longmapsto (x|y)$ \[
                \diff \varphi_{(x, y)}(h, k)=(h|y)+(x|k)
            \]
        \item $\varphi = \det $. On note \[
                A = (C_1 \cdots C_n) \quad H=(H_1 \cdots H_n)
            \]
            alors \[
                \det(A+H)=\det(A)+\det(H_1, C_2, \cdots, C_n)+\cdots + \det(C_1, \cdots, C_{n-1}, H_n)+o_0(H)
            \]
            On en déduit \begin{align*}
                \diff \varphi_A(H) &= \sum_{j=1}^n \det(C_1, \cdots, H_i, \cdots, C_n)\\
                                   &= \sum_{j=1}^n\sum_{i=1}^n(-1)^{i+j}h_{i, j}\Delta_{i, j}(A) \\
                                   &= \Tr(\Com(A)^\intercal H)
            \end{align*}
    \end{itemize}
\end{ex}

\section{Composition des fonctions différentielles}

\begin{prop}
    \Hyp $f : \Omega \subset E \to F$ différentiable en $a$ telle que $f(\Omega)\subset \Omega'$, $g : \Omega'\subset F \to G$ différentiable en $f(a)$
    \Conc $g\circ f$ est différentiable en $a$ et $\diff (g\circ f)_a=\diff g_{f(a)}\circ \diff f_a$ \index{différentielle!composition}
\end{prop}

\begin{proof}
    On a \[
        g(f(a)+k)=g(f(a))+\diff g_{f(a)}(k)+o(k)
    \]
    et \[
        f(a+h)=f(a)+\underbrace{\diff f_a(h)+o(h)}_{k}
    \]
    donc \[
        g\circ f(a+h)=g(f(a))+\diff g_{f(a)}(\diff f_a(h)+o_0(h)) + \underbrace{o(\underbrace{\diff f_a(h)+o_0(h)}_{o(h)})}_{o(h)}
    \]
    d'où \conc
\end{proof}

\begin{rem}
    $E=\mathbb R^n$, $F=\mathbb R^m$, $G=\mathbb R^p$, $f=(f_1, \cdots, f_m):E\to F$, $g=(g_1, \cdots, g_p):F\to G$, $\mathcal C_k$ base canonique de $\mathbb R^k$.

    Si $x \underset{\mathcal C_n}\longleftrightarrow X$ alors $\diff f_a(x)\underset{\mathcal C_m}\longleftrightarrow J_f(a)X$ et $\diff (g\circ f)_a(x)\underset{\mathcal C_p}\longleftrightarrow J_{g\circ f}(a)X\underset{\mathcal C_p}\longleftrightarrow J_g(f(a))J_f(a)X$. C'est vrai pour tout $X$ donc \[
        \underbrace{J_{g\circ f}(a)}_{\in\mathcal M_{p,n}} = \underbrace{J_g(f(a))}_{\in\mathcal M_{p,m}}\underbrace{J_f(a)}_{\in \mathcal M_{m,n}}
    \]
    On va identifier le coefficient $i, j$. \begin{align*}
        & \left[ J_{g\circ f}(a) \right]_{i, j}=\sum_{k=1}^m \left[ J_g(f(a)) \right]_{i, k} \left[ J_f(a) \right]_{k, j} \\
        \iff & \frac{\partial (g_i\circ f)}{\partial x_j} (a)=\sum_{k=1}^m \frac{\partial g_i}{\partial y_k}(f(a)) \frac{\partial f_k}{\partial x_j} (a) \\
        \iff & \colorboxed{red}{\partial_j (g_i\circ f)(a)=\sum_{k=1}^m\partial_kg_i(f(a))\partial_jf_k(a)} \quad \text{ \emph{(règle de la chaîne)} }
    \end{align*}

    En particulier, si $p = n = 1$, $\varphi:t\in \mathbb R\to g(f_1(t), \cdots, f_m(t))$ alors \[
        (g\circ f)'(a)=\sum_{k=1}^m \frac{\partial g}{\partial y_k}(f(a))f'_k(a)
    \]
\end{rem}

\begin{rem}
    Si $f:\Omega\subset E\to \mathbb R$ est différentiable en $a$ et $f(a)\neq 0$ alors $\frac 1f$ est différentiable en $a$ car $x\longmapsto \frac 1x$ est différentiable en $f(a)$
\end{rem}

\begin{rem}
    Si $f:\Omega\subset E\to F$ est $a\in\Omega$, $v\in E$ alors $\varphi:t \longmapsto f(a+tv)$ est définie au $\mathcal V(0)$ et $\varphi'(t)=\diff f_{a+tv}(v)$
\end{rem}

\needspace{5cm}
\section{Applications de classe \texorpdfstring{$\mathcal C^1$}{C1}}

\begin{thmdef}
    \Hyp $f:\Omega\subset E\longrightarrow F$
    \begin{concenum}
    \item On dira que $f$ est $\mathcal C^1$ sur $\Omega$ si $f$ est différentiable sur $\Omega$ et si $a\in\Omega \longmapsto \diff f_a\in\mathcal L(E, F)$ est $\mathcal C^0$
    \item Une fonction $\mathcal C^1$ est continue
    \item On note $\mathcal B=(e_1, \cdots, e_n)$ une base de $E$, $\mathcal C=(u_1, \cdots, u_m)$ une base de $F$ et $f=(f_1, \cdots, f_m)$. Il y a équivalence entre: \begin{enumerate}
        \item $f$ est $\mathcal C^1$ sur $\Omega$
        \item Les $f_i$ ont toutes leurs dérivées partielles sur $\Omega$ et les $\partial_jf_i$ sont $\mathcal C^0$ sur $\Omega$
    \end{enumerate}
\item On note $\mathcal C^1(\Omega, F)$ l'ensemble de ces fonctions
\end{concenum}
\end{thmdef}

\begin{proof}~
    \begin{enumerate}
        \setcounter{enumi}{1}
    \item Les fonctions $\mathcal C^1$ sont différentiables donc continues.
    \item $(a) \implies (b)$: \[
            J_f(a)=\mathcal M_{\mathcal B, \mathcal C}(\diff f_a)= \left( \frac{\partial f_i}{\partial x_j} (a) \right)_{\substack{1\leq i\leq m\\1\leq j\leq n}}
        \]
        est continue par rapport à $a$ par hypothèse.

        $(b) \implies (a)$: Il suffit de montrer que $f$ est différentiable sur $\Omega$. On commence par le cas $F=\mathbb R$. \begin{align*}
            f(a+h)-f(a) = \phantom{+} & f(a_1+h_1, \cdots, a_n+h_n)-f(a_1, a_2+h_2, \cdots, a_n+h_n) \\
            + & f(a_1, a_2+h_2, \cdots, a_n+h_n)-f(a_1, \cdots, a_i, a_{i+1}+h_{i+1}, \cdots, a_n+h_n) \\
            + & f(a_1, \cdots, a_i, a_{i+1}+h_{i+1}, \cdots, a_n+h_n) + \cdots \\
            + & f(a_1, \cdots, a_{n-1}, a_n+h_n)-f(a_1, \cdots, a_n)
        \end{align*}
        et chaque ligne est fonction d'une seule variable, donc le théorème des accroissements finis donne
        \begin{align*}
            \exists c_{i,h}\in]a_i,a_i+h_i[, \quad f(a+h)-f(a)=\phantom+ &
            h_1\frac{\partial f}{\partial x_1}(c_{1,h}, a_2+h_2, \cdots, a_n+h_n)\\
            + & h_2 \frac{\partial f}{\partial x_2}(a_1, c_{2,h},a_3+h_3,\cdots, a_n+h_n)\\
            + & \hspace{2cm}\vdots \\
            + & \underbrace{h_n \frac{\partial f}{\partial x_n} (a_1, \cdots, a_{n-1}, c_{n,h})}_{\longrightarrow h_n \frac{\partial f}{\partial x_n}(a) \text{ donc } =h_n \frac{\partial f}{\partial x_n}(a)+o(1) } \\
            =\phantom + & \underbrace{h_1 \frac{\partial f}{\partial x_1}(a)+\cdots + h_n \frac{\partial f}{\partial x_n} (a) }_{\text{AL}}  + \underbrace{h_1o(1)+\cdots h_no(1)}_{o(h)}
        \end{align*}
        donc $f$ est différentiable sur $\Omega$. Si $F\neq R$ on raisonne coordonnée par coordonnée.
\end{enumerate}
\end{proof}

\begin{prop} ~
    \begin{enumerate}
        \item $\mathcal C^1(\Omega, F)$ est un $\mathbb R$-e.v.
        \item Si $\mathcal C$ base de $F$ est $f:\Omega\longrightarrow F$ donnée par $f=(f_1, \cdots, f_n)$ (dans $\mathcal C$) alors il y a équivalence entre \begin{enumerate}
            \item $f$ est $\mathcal C^1$
            \item Les $f_i$ sont toutes $\mathcal C^1$
        \end{enumerate}
    \item Si $f:\Omega\longrightarrow F$ et $g: \Omega'\supset f(\Omega)\longrightarrow G$ sont $\mathcal C^1$ alors $g\circ f$ aussi
\end{enumerate}
\end{prop}

\begin{proof}
    1. et 2. sont faciles, 3. est déjà vu.
\end{proof}

\begin{thm}
    \Hyp $f:\Omega\subset E\longrightarrow \mathbb R$, $a,b\in\Omega$ tels que $[a, b]\subset \Omega$
    \begin{concenum}
    \item \[ f(b)-f(a)=\int_0^1\mathrm df_{(1-t)a+tb}(b-a)\diff t\]
    \item Si $\nop \diff f_x\nop\leq M$ pour $x\in[a, b]$ alors $|f(b)-f(a)|\leq M\|b-a\|$
    \end{concenum}
\end{thm}

\begin{proof}
    \begin{enumerate}
        \item $\varphi:t\longmapsto f(a+t(b-a))$ est telle que $\varphi'(t)=df_{(1-t)a+tb}(b-a)$ et \[
                f(b)-f(a)=\varphi(1)-\varphi(0)=\int_0^1\varphi'
            \]
        \item \[
                |f(b)-f(a)|\leq \int_0^1\|\diff f_{(1-t)a+tb}(b-a)\|\diff t\leq M\int_0^1\|b-a\|\diff t
            \]
    \end{enumerate}
\end{proof}

\section{Étude de régularité}

\subsection{\texorpdfstring{$f:(x, y)\longmapsto \min(x^2, y^2)$}{f(x, y)=min(x², y²)}}

\begin{center}
    \includegraphics{src/figures/calculdiff-regularite-min-carres.pdf}
\end{center}

D'abord, \[
    f(x, y)=\frac{x^2+y^2-|x^2-y^2|}2
\]
donc $f$ est $\mathcal C^0$. On note $\Delta$ le fermé $\{(x, y)\in\mathbb R^2, x^2=y^2\}$, $\mathbb R^2\setminus \Delta$ est ouvert et $f\in \mathcal C^1(\mathbb R^2\setminus \Delta, \mathbb R)$ comme composée de fonctions $\mathcal C^1$.

Il reste à savoir si $f$ est différentiable sur $\Delta$ et si éventuellement la différentielle est continue.

\begin{align*}
    f(x+h, x+k)-f(x,x) &= \frac{(x+h)^2+(x+k)^2-|(x+h)^2-(x+k)^2|}{2}-x^2 \\
                       &= x(h+k)+\frac{h^2+k^2}2 - \left|x(h+k)+ \frac{h^2-k^2}{2}  \right| \\
\end{align*}
Pour $k=0$, \[
    \frac{f(x+h, x)-f(x, x)}h=x+\frac h2-\frac{\left|xh+\frac{h^2}2\right|}h\xrightarrow[h\to0\pm]{}x\pm|x|
\]

Donc $f$ n'a pas de dérivée partielle par rapport à $x$ en $(x, x)$ pour $x\in\Delta\setminus\{0\}\defeq\Delta^\star$ (idem en $(x, -x)$) donc n'est pas différentiable sur les points de $\Delta$.

En $(0, 0)$, $f$ est différentiable car $0\leq f(x, y)\leq x^2+y^2=o(\|(x, y)\|_2)$ donc $f(x, y)=f(0, 0)+0+o((x, y))$ et $\diff f_{(0, 0)}=0$

Ainsi $(x, y)\longmapsto \diff f_{(x, y)}$ est définie sur $\mathbb R^2\setminus \Delta^\star$. Elle est continue sur $\mathbb R^2\setminus \Delta$, puis on va montrer qu'elle est continue en $(0, 0)$. On note $(x_0, y_0)\in\mathbb R^2\setminus \Delta$ tels que $x_0^2>y_0^2$ donc au voisinage de $(x_0, y_0)$, $f(x, y)=y^2$ et $\diff f_{(x_0, y_0)}=2y_0\diff y$ puis sinon $\diff f_{(x_0, y_0)}=2x_0\diff x$. Dans tous les cas, $\diff f_{(x, y)}\xrightarrow[(x, y)\to 0]{}0=\diff f_{(0, 0)}$ donc la différentielle est continue sur son domaine de définition.

\subsection{Domaine de continuité}

On pose \[
    f:(x, y)\longmapsto \begin{cases}
        \dfrac{x^2y^2}{x^2+y^2} &\text{ si } (x, y)\neq (0, 0) \\
        0 &\text{ sinon }
    \end{cases}
\]

\begin{center}
    \includegraphics{src/figures/calculdiff-regularite-fonction-continue.pdf}
\end{center}

$f$ est clairement continue sur $\mathbb R^2\setminus\{(0, 0)\}$ puis $x^2y^2\leq (x^2+y^2)^2$ donc $0\leq f(x)\leq x^2+y^2=o((x, y))$ donc $f$ continue en $0$

\subsection{Une fonction différentiable non \texorpdfstring{$\mathcal C^1$}{C1}}

On pose \[
    f:(x, y)\longmapsto \begin{cases}
        \displaystyle (x^2+y^2)\sin \left( \frac{1}{\sqrt{x^2+y^2}}  \right) &\text{ si } (x, y)\neq (0, 0) \\
        0 &\text{ sinon }
    \end{cases}
\]

\begin{center}
    \includegraphics{src/figures/calculdiff-regularite-diff-non-c1.pdf}
\end{center}

La fonction est clairement $\mathcal C^1$ sur $\mathbb R^2\setminus\{0\}$ puis $f(x, y)=o_0((x, y))$ donc $\diff f_0=0$ et $f$ est différentiable. Il reste à voir si la différentielle est continue en $(0, 0)$.

Puis, \[
    \frac{\partial f}{\partial x} (x, y)= \begin{cases}
        \displaystyle 2x\sin \left( \frac{1}{\sqrt{x^2+y^2}} \right)-\frac x{\sqrt{x^2+y^2}}\cos \left( \frac{1}{\sqrt{x^2+y^2}}  \right) &\text{ si } (x, y)\neq (0, 0) \\ 0 &\text{ sinon }
    \end{cases}
\]
et \[
    \frac{\partial f}{\partial x} (x, 0)\xnrightarrow[x\to 0]{} 0
\]
donc $f$ n'est pas $\mathcal C^1$

\subsection{Expression intégrale}

On considère $g:\mathbb R^2\to \mathbb R$ de classe $\mathcal C^1$ et \[
    F:x\longmapsto \int_0^xg(x, t)\diff t
\]

On va montrer que $F$ est $\mathcal C^1$ et on va calculer $F'$.
On a \[
    F(x) \underset{t=ux}=\int_0^1g(x, ux)\diff u
\]
On note donc $h: (x, u) \longmapsto g(x, ux)$ et $F$ est $\mathcal C^1$ par théorème de dérivation sous le signe somme puis \begin{align*}
    F'(x)&=\int_0^1 g(x, ux)\diff u+x\int_0^1\frac{\partial g}{\partial x}(x, ux)\diff u+x\int_0^1\frac{\partial g}{\partial t}(x, ux)\diff u \\
         &= \int_0^1\frac{\partial}{\partial t}(ug(x, ux))+x\int_0^1 \frac{\partial g}{\partial x} (x, ux)\diff u \\ &= g(x, x)+\int_0^x \frac{\partial g}{\partial x}(x, t)\diff t
\end{align*}

\section{Caractérisation des fonctions constantes}

On se place dans $E$ et on note $\Omega$ un ouvert connexe par arcs de $E$, et $f:\Omega \to F$. On note $\mathcal B=(e_1, \cdots, e_n)$ une base de $E$. On va montrer que $f$ est constante sur $\Omega$ si et seulement si $f$ est $\mathcal C^0$ et a toutes ses dérivées partielles nulles.

\begin{itemize}
    \item ($\implies$) Direct
    \item ($\impliedby$) Par hypothèse, $\partial_if =0$ est continue donc $f$ est $\mathcal C^1$ donc pour $[a, b]\subset \Omega$, \[
            f(b)-f(a)=\int_0^1\diff f_{(1-t)a+tb}(b-a)\diff t=0\implies f(b)=f(a)
        \]

        Vu ce qui précède, $f$ est constante sur toutes les boules invertes incluses dans $\Omega$. On prend $a\in\Omega$ et on note \[
            \mathcal E=\{x\in\Omega, f(x)=f(a)\}.
        \]
        Cet ensemble est un fermé en tant que préimage par une application continue ($f$) par le fermé $\{f(a)\}$. C'est aussi un ouvert car pour $x_0\in\mathcal E$, $f$ est constante sur une boule ouverte centrée en $x_0$ donc par définition de $\mathcal E$, cette boule est dans $\mathcal E$.

        On note $h=\1_{\mathcal E}$ de sorte que $h^{-1}(\{0\})=\Omega\setminus \mathcal E$, $h^{-1}(\{1\})=\Omega$, $h^{-1}(\emptyset)=\emptyset$. Ainsi, $h$ est continue car la préimage d'un fermé est un fermé (induit). Ainsi, $h(\Omega)$ est connexe par arcs donc $h(\Omega)=\{1\}$ car $\mathcal E$ non vide et $\Omega=\mathcal E$
\end{itemize}


\textbf{Autre méthode:} Il suffit de montrer que $\Omega$ est connexe par arcs polygonaux. On fixe $a\in\Omega$ et on note $\mathcal E=\{x\in\Omega, \; \text{il existe un arc polygonal entre }a \text{ et } x\}$. C'est un ouvert (même justification que dans la méthode d'avant). Si $(\alpha_n)$ est une suite de $\mathcal E$ convergente vers $\alpha\in\Omega$, alors APCR $\alpha_n$ est dans une boule incluse dans $\Omega$ centrée en $\alpha$, puis il suffit de relier $\alpha_n$ et $\alpha$ dans $\Omega$ pour conclure que $\alpha\in\mathcal E$. Ainsi c'est un ouvert fermé de $\Omega$ donc c'est $\Omega$ (déjà fait)

\section{Extrema -- Gradient}

Dans cette partie, $f:\Omega\to\mathbb R$. On suppose $(E, (\;|\;))$ euclidien.

\begin{thmdef}
    \Hyp $f:\Omega\to\mathbb R$ différentiable en $a\in\Omega$
    \begin{concenum}
    \item $\diff f_a$ est une forme linéaire
    \item Il existe un unique $w\in E$ tel que \[
            \forall x\in E, \qquad \diff f_a(x)=(w|x)
        \]
        Ce vecteur est appelé gradient\index{gradient} de $f$ en $a$ et se note $\nabla f(a), \overrightarrow{\mathrm{grad}} f(a), \dots$
    \item Si $\mathcal B=(e_1, \cdots, e_n)$ est une base ON de $E$ alors \[
            \nabla f(a)=\sum_{i=1}^n\partial_i f(a)e_i
        \]
    \end{concenum}
\end{thmdef}

\begin{proof}~
    \begin{enumerate}
        \item Trivial
        \item C'est une application du théorème de représentation de Riesz\index{Riesz (théorème de représentation de -- )}
        \item \[
                \diff f_a(x)=\sum_{i=1}^n\partial_if(a)\underbrace{\diff x_i(x)}_{(e_i|x)}
            \]
    \end{enumerate}
\end{proof}

\begin{rem}[Interprétation géométrique]
    On note $u\in E$ unitaire. \[ f(a+tu)=f(a)+\diff f_a(tu)+o_0(t)=f(a)+t (\nabla f(a)|u) +o_0(t).\] Pour $a$ fixé tel que $\nabla f(a)\neq 0$, $(\nabla f(a)|u)$ est maximal pour $u=\dfrac{\nabla f(a)}{\|\nabla f(a)\|}$ et dans ce cas $(\nabla f(a)|u)=\|\nabla f(a)\|$
\end{rem}

\begin{rem}[Égalité des accroissements finis\index{accroissements finis!égalité}]
    On note $f:\Omega\to E$ différentiable, et $[a, b]\subset \Omega$. On note \[ g:t\in[0, 1]\longmapsto f((1-t)a+tb)\in\mathbb R \]
    \begin{itemize}
        \item $g$ est continue sur $[0, 1]$, dérivable sur $]0, 1[$ (car dérivable sur $[0, 1]$)
        \item On applique l'égalité des accroissements finis à $g$: \[
                \exists t_0\in ]0, 1[, \quad g(1)-g(0)=g'(t_0)
            \]
            soit encore \[
                f(b)-f(a)=\diff f_{(1-t_0)a+t_0b}(b-a)=(\nabla f(\underbrace{(1-t_0)a+t_0b}_{\defeq c})\;|\;b-a)
            \]
    \end{itemize}
    Ainsi, il existe $c\in ]a, b[$ tel que $f(b)-f(a)=\diff f_c(b-a)$
\end{rem}

\begin{thmdef}
    \Hyp $f:\Omega\subset E\to \mathbb R, a\in \Omega$, $f$ différentiable en $a$.
    \begin{concenum}
    \item On dira que $f$ a un point critique\index{point critique} en $a$ si $\diff f_a=0$ (i.e. si $\nabla f(a)=0$ avec un produit scalaire)
    \item Si $a$ est un extremum local de $f$ alors $a$ est un point critique de $f$.
    \end{concenum}
\end{thmdef}

\begin{proof}
    \begin{enumerate}
        \setcounter{enumi}{1}
    \item On note $u\in E$, $\varphi: t\longmapsto f(a+tu)$ est définie sur un intervalle du type $]-\varepsilon, \varepsilon[$, et $\varphi$ atteint un extremum local en $0$. Ainsi, $\varphi'(0)=0=\diff f_a(u)$ vrai pour tout $u$ donc $\diff f_a=0$
\end{enumerate}
\end{proof}

\begin{ex}
    On pose \[
        f:(x, y)\longmapsto (x^2+y^2)\exp(x^2-y^2)
    \]
    Cette fonction est $\mathcal C^1$ et si $(x, y)$ est un extremum alors \[
        \begin{cases}
            \displaystyle \frac{\partial f}{\partial x}(x, y)=\exp(x^2-y^2)2x(1+(x^2+y^2)) =0 \\[1em]
            \displaystyle \frac{\partial f}{\partial y}(x, y)=\exp(x^2-y^2)2y(1-(x^2+y^2)) =0 \\
            \end{cases} \iff \begin{cases}
            x=0 \\ y(1-y^2)=0
            \end{cases} \iff \begin{cases}
            x=0\\
            y\in \llbracket -1, 1\rrbracket
        \end{cases}
    \]
    Il reste à vérifier si les trois points sont bien des extrema. \begin{itemize}
        \item $(0, 0)$ est un minimum global
        \item On fait l'étude en $(0, 1)$. \[
                \underbrace{f(h, 1+k)-f(0, 1)}_{\Delta(h, k)}=2e^{-1}(h^2-k^2)+o_0(\|(h, k)\|)
            \]
            et $\Delta(h, 0)>0$ pour $h>0$ assez petit, $\Delta(0, k)<0$ pour $k>0$ assez petit donc $\Delta$ change de signe au voisinage de $(0, 1)$ donc $(0, 1)$ n'est pas un extremum (c'est un point selle).
        \item De même, $(0, -1)$ n'est pas un extremum.
    \end{itemize}

    \begin{center}
        \includegraphics{src/figures/calculdiff-point-critique-ex.pdf}
    \end{center}

\end{ex}

\begin{exo}
    On note $S=\{x\in\mathbb R^n, \quad \|x\|=1\}$ la sphère unité. On suppose $f:\mathbb R^n\to \mathbb R$ différentiable sur son domaine de définition et constante sur $S$. Montrer qu'il existe $x_0\in\mathbb R^n$ tel que \[
        \|x\|<1 \qquad\text{ et }\qquad \diff f_{x_0}=0
    \]
\end{exo}

\begin{proof}[Résolution]
    $\mathcal B_f(0, 1)$ est un compact donc $f$ est bornée et atteint ses bornes (son minimum en $\alpha$ et son maximum en $\beta$ par exemple) sur cette boule.
    \begin{itemize}
        \item Si $f(\alpha)=f(\beta)$ alors $f$ est constante et $x_0=0$ convient
        \item Si $f(\alpha)<f(\beta)$ alors $\alpha$ ou $\beta$ n'est pas dans $S$ et ce point convient.
    \end{itemize}
\end{proof}

\begin{center}
    \includegraphics{src/figures/calculdiff-rolle.pdf}
\end{center}

\begin{rem}
    C'est une généralisation possible du théorème de Rolle\index{Rolle!généralisation}. Plus généralement, si $K$ est un compact d'intérieur non vide de $\mathbb R^n$ tel que $f:\mathbb R^n\to\mathbb R$ différentiable est constante sur $\partial K=K\setminus \mathring K$, alors il existe $x\in\mathring K$ tel que $\diff f_x=0$
\end{rem}

\section{Application à la géométrie}

\begin{dfn}
    On note $X\subseteq E$ non vide, $x\in X$ et $v\in E$. On dira que $v$ est un vecteur tangent à $X$ en $x$ s'il existe un $\varepsilon>0$ et $\gamma\in\mathcal D^1(]-\varepsilon, \varepsilon[, X)$ tel que $\gamma (0)=x$, $\gamma'(0)=v$
\end{dfn}

\begin{ex}~
    \begin{enumerate}
        \item On note $\mathcal C$ le cercle unité (dans le plan) et $(x_0, y_0)\in\mathcal C$. \begin{itemize}
            \item Si $(\alpha, \beta)$ tangent à $\mathcal C$ en $(x_0, y_0)$ alors il existe $\gamma$ dérivable telle que $\gamma(0)=(x_0, y_0)$ et en dérivant la relation $(\gamma(t)\;|\;\gamma(t))=1$ on a $2(\gamma(t)\;|\;\gamma'(t))=0$ donc $((x_0, y_0)\;|\;(\alpha, \beta))=0$
            \item Si $(x_0, y_0)\perp (\alpha, \beta)$, on note $(x_0, y_0)=(\cos\theta, \sin\theta)$ et on pose \[
                \begin{matrix}
                    \gamma : & [-1, 1] & \longrightarrow & \mathcal C\hspace{3.5cm} \\
                              & t & \longmapsto & (\cos(\theta+tu), \sin(\theta+tv))
                \end{matrix}
                \]
                et $\gamma$ est dérivable, $\gamma(0)=(x_0, y_0)$, et $\gamma'(0)=(-uy_0, vx_0)$ colinéaire à $(\alpha, \beta)$ d'où l'équivalence.
        \end{itemize}
        La droite $(x_0, y_0)^{\perp}$ est l'ensemble des vecteurs tangents en $(x_0, y_0)$. L'espace tangent associé est la droite affine $\mathcal D=(x_0, y_0)+(x_0, y_0)^{\perp}$
        \begin{center}
            \includegraphics{src/figures/calculdiff-espace-tangent-cercle.pdf}
        \end{center}

    \item $E=\mathbb R^3$, $f:\mathbb R^2\to \mathbb R$ différentiable, $X=\{(x, y, f(x, y)), \quad x,y\in\mathbb R\}$. Le vecteur $(\alpha, \beta, \lambda)$ est tangent à $X$ en $(x_0, y_0, f(x_0, y_0))$ si et seulement si \[
            \lambda= \frac{\partial f}{\partial x} (x_0, y_0)\alpha + \frac{\partial f}{\partial y}(x_0, y_0)\beta \tag{$\star$}
        \]
        En effet, si $(\alpha, \beta, \nu)$ tangent alors on note $\gamma$ l'arc associé et $z(t)-f(x(t), y(t))=0$ donc en dérivant \[
            z'(t)- \frac{\partial f}{\partial x} (x(t), y(t))x'(t)- \frac{\partial f}{\partial y} (x(t), y(t))y'(t)
        \]
        donc on a bien $(\star)$. Puis, si $(\alpha, \beta, \lambda)$ satisfait $(\star)$ alors \[
            \gamma: t\in]-1, 1[\longmapsto (x_0+t\alpha, y_0+t\beta, f(x_0+t\alpha, y_0+t\beta))
        \]
        convient.

    \item \emph{Cas des lignes de niveau}. On note $f:\Omega\to \mathbb R$, $X_a=f^{-1}(a)$. On suppose $X_a$ non-vide et $\gamma:~]-\varepsilon, \varepsilon[ \to X_a$ dérivable avec $\gamma(0)=t_0$, $\gamma'(0)=v$. En dérivant la relation $f\circ gamma=a$ on a \[\forall x\in ]-\varepsilon; \varepsilon[, \quad \diff f_{\gamma(x)}\circ \gamma'(x)=0\]
        donc pour $x=0$, $\diff f_{t_0}(v)=0=(\nabla f(t_0)\;|\;v)$
    \end{enumerate}
\end{ex}

\section{Calcul de vecteurs tangents}

\subsection{Vecteurs tangents à $O_n(\mathbb R)$ en $I_n$}

On veut déterminer les vecteurs tangents à $X=O_n(\mathbb R)$ en $I_n$. On se donne \[
    \gamma: ]-\varepsilon; \varepsilon[ \longrightarrow O_n(\mathbb R)
\]
dérivable telle que $\gamma (0)=I_n$. \begin{itemize}
    \item $\forall x\in ]-\varepsilon; \varepsilon[, \qquad \gamma(t)\in O_n(\mathbb R)$, i.e. $\gamma(t)^{\top}\gamma(t)=I_n$. En dérivant: \[
            \gamma'(t)^\top\gamma(t)+\gamma(t)^\top\gamma'(t)=0\implies \gamma'(0)^\top+\gamma(0)=0 \implies \gamma'(0)\in \mathcal{AS}_n(\mathbb R)
        \]
    \item On note $A\in\mathcal M_n(\mathbb R)$ antisymétrique et $\gamma:t\in ~]-1, 1[~\longmapsto e^{tA}$ de sorte que $\gamma(t)\in O_n(\mathbb R)$ (facile à vérifier en écrivant l'exponentielle comme somme d'une série), $\gamma(0)=I_n$ et $\gamma'(0)=A$ donc $A$ est tangent à $X$ en $I_n$.
\end{itemize}

\subsection{Vecteurs tangents à $\mathrm{GL}_n(\mathbb R)$ en $I_n$}

$\mathrm{GL}_n(\mathbb R)$ est un ouvert donc tous les vecteurs sont tangents à $I_n$.

\subsection{Vecteurs tangents à $\mathrm{SL}_n(\mathbb R)$ en $I_n$}

On note $\gamma$ un chemin qui satisfait les bonnes hypothèses, $\gamma(0)=I_n$, $\det \circ \gamma =1$ donc \[
    \diff (\det)_{\gamma(t)}\circ \gamma'(t)=\Tr(\Com(\gamma(t))^\top \gamma'(t))=0 \implies \Tr(\gamma'(0))=0
\]

On note maintenant $A$ une matrice de trace nulle et \[
    \gamma: t\longmapsto \frac{I_n+tA}{\sqrt[n]{\det(I_n+tA)}}
\]
est définie au voisinage de $0$ et convient car \[
    \gamma'(t)=\frac{A}{\sqrt[n]{\det(I_n+tA)}}+(I_n+tA) \left( - \frac{1}{n} \Tr(\Com(I_n+tA)^\top A)\det(I_n+tA)^{- \frac{n+1}{n} } \right)
\]
donc $\gamma'(0)=A$.

\subsection{Vecteurs tagents à la surface d'équation $\frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2}=1$}

On cherche les vecteurs tangents à $X=\{(x, y, z), \; \frac{x^2}{a^2}+\frac{y^2}{b^2}+\frac{z^2}{c^2}=1\}$ en $M_0=(x_0,y_0,z_0)\in X$.

On note $\gamma :~]-\varepsilon, \varepsilon[ ~\longrightarrow X$ différentiable telle que $\gamma(0)=M_0$
On a \[
    \forall |t|<\varepsilon, \qquad \frac{x(t)^2}{a^2} + \frac{y(t)^2}{b^2} + \frac{z(t)^2}{c^2}=1
\]
donc en dérivant, \[
    \frac{2xx'}{a^2}+ \frac{2yy'}{b^2} + \frac{2zz'}{c^2} =0
\]
donc $\displaystyle\gamma'(0)\perp \left( \frac{x_0}{a^2}, \frac{y_0}{b^2} , \frac{z_0}{c^2}   \right)$. La réciproque est vraie: si on note $(\alpha, \beta, \lambda)$ perpendiculaire à $\displaystyle x\defeq\left( \frac{x_0}{a^2}, \frac{y_0}{b^2} , \frac{z_0}{c^2}   \right)\in X$, alors $x\neq 0$ donc par exemple $z_0>0$ et \[
    \gamma:t\longmapsto \left(x_0+t\alpha, y_0+t\beta, c^2\sqrt{1- \dfrac{(x_0+t\alpha)^2}{a^2}-\dfrac{(y_0+t\beta)^2}{b^2}}\right)
\]
convient.

\section{Espace tangent}

\begin{defprop}
    \Hyp $X\subset E, a\in X$
    \begin{concenum}
    \item On note $V_a(X)$ l'ensemble des vecteurs tangents à $X$ en $a$.
    \item L'espace tangent\index{espace tangent} à $X$ en $a$ est $T_a(X)=a+V_a(X)$
    \item Si $f:\Omega \subset E\to \mathbb R$ et si $a\in \Omega$ n'est pas un point critique alors $X=f^{-1}(\{f(a)\})$ vérifie \begin{enumerate}
        \item $V_a(X)=\nabla f(a)^\perp$
        \item $T_a(X)=a+\nabla f(a)^\perp$
    \end{enumerate}
    En particulier, si $E=\mathbb R^3$ et $X=f^{-1}(\{\lambda\})$ et $a=(x_0, y_0, z_0)\in X$ point non critique de $f$ alors $T_a(X)$ est donné par l'équation cartésienne \[
        \frac{\partial f}{\partial x} (a)(x-x_0)+ \frac{\partial f}{\partial y} (a)(y-y_0)+ \frac{\partial f}{\partial z} (a)(z-z_0)=0
    \]
    \end{concenum}
\end{defprop}

\begin{proof}
    Admise
\end{proof}

\section{Dérivée partielle d'ordre supérieur}

\begin{dfn}
    On note $f:\Omega\subset E\to F$, $n\geq 2$, $\mathcal B=(e_1, \cdots, e_p)$ base de $E$. On définit récursivement $\mathcal C^n(\Omega, F)$ par \[
        f\in\mathcal C^n(\Omega, F)\iff f \text{ différentiable sur }\Omega\text{ et }\forall i\in\llbracket 1, p\rrbracket, \; \partial_i f\in\mathcal C^{n-1}(\Omega, F)
    \]
\end{dfn}

\begin{notation}
    \[
        \frac{\partial}{\partial x_k} \left( \frac{\partial f}{\partial x_i}  \right) (a)= \frac{\partial f}{\partial x_k\partial x_i}=\partial^2_{k,i}f(a) 
    \]
    et \[
        \frac{\partial}{\partial x_{i_1}} \left( \frac{\partial }{\partial x_{i_2}} \left( \cdots \left( \frac{\partial f}{\partial x_{i_p}}  \right)\right)  \right)(a)=\partial^p_I f(a), \qquad I=(i_1, \cdots, i_p)
    \]
\end{notation}

\begin{thm}[Schwarz]\index{Schwarz (théorème de -- )}
    \Hyp $f\in \mathcal C^2(\Omega, F)$, $a\in \Omega$, $\mathcal B=(e_1, \cdots, e_p)$ base de $E\supseteq \Omega$.
    \Conc Pour $i, j\in\llbracket 1, p\rrbracket$,  \[
        \frac{\partial^2f}{\partial x_i\partial x_j} (a)= \frac{\partial^2f}{\partial x_j\partial x_i} (a)
    \]
\end{thm}

\begin{proof}
    Admis
\end{proof}

\section{Exemples}

\subsection{Identité d'Euler}

On note $f:\mathbb R^n\setminus \{0\}\to \mathbb R$ différentiable. On dira que $f$ est $\alpha$-homogène si \[\forall x, t\in\mathbb R^n\times \mathbb R, \quad x\neq 0 \implies f(tx)=t^\alpha f(x)\]

On va montrer que $f$ est $\alpha$-homogène si et seulement si \[
    \sum_{i=1}^nx_i\partial_if(x)=\alpha f(x)
\]

\begin{itemize}
    \item $(\implies)$ On a $f(tx)=t^\alpha f(x)$ donc \[
            \diff d_{tx}(x)=\alpha t^{\alpha-1}f(x)\underset{t=1}\implies \diff f_x(x)=\alpha f(x)
        \]
    \item $(\impliedby)$ On note $h(t)=t^{-\alpha}f(tx)$ de sorte que \[
            h'(t)=-\alpha t^{-\alpha-1}f(tx)+t^{-\alpha}\diff f_{tx}(x)=t^{-\alpha-1}(-\alpha f(tx)+\diff f_{tx}(tx))=0
        \] par hypothèse donc $h(t)=h(1)=f(x)$ ce qui conclut.
\end{itemize}

\subsection{Calcul du laplacien en coordonnées polaires}

On note $f:\mathbb R^2\to \mathbb R $ de classe $\mathcal C^2$, et \[
    \begin{matrix}
        f^\star :&\mathbb R_+\times [0, 2\pi]&\longrightarrow & \mathbb R \\
                 &\phantom{\mathbb R_+\times \;\;}(\rho, \theta) &\longmapsto &f(\rho\cos\theta, \rho\sin\theta)
    \end{matrix}
\]

On appelle laplacien\index{laplacien} de $f$ la fonction \[
    \Delta f: (x, y)\longmapsto \frac{\partial^2f}{\partial x^2}(x, y)+ \frac{\partial^2 f}{\partial y^2}(x, y)  
\]

On veut calculer $(\Delta f)^\star$ en fonction de $f^\star$.

\begin{itemize}
    \item \[
            \frac{\partial f^\star}{\partial \rho} (\rho, \theta)= \left( \frac{\partial f}{\partial x}  \right)^\star (\rho, \theta)\cos \theta+ \left( \frac{\partial f}{\partial y}  \right)^\star (\rho, \theta)\sin\theta
        \]
    \item \[
            \frac{\partial f^\star}{\partial \theta}(\rho, \theta)= \left( \frac{\partial f}{\partial x}  \right)^\star (\rho, \theta)(-\rho\sin\theta)+ \left( \frac{\partial f}{\partial y}  \right)^\star (\rho, \theta)\rho\cos\theta
        \]
    \item \[
        \frac{\partial^2 f^\star}{\partial \rho^2} (\rho, \theta)= \left( \frac{\partial^2f}{\partial x^2}  \right)^\star (\rho, \theta)\cos^2\theta + \left( \frac{\partial^2f}{\partial x\partial y}  \right)^\star (\rho, \theta)\cos\theta\sin\theta \]\[ + \left( \frac{\partial^2 f}{\partial y\partial x}  \right)^\star(\rho, \theta)\sin\theta\cos\theta + \left( \frac{\partial^2f}{\partial y^2}  \right)(\rho, \theta)\sin^2\theta
        \]
    \item \begin{align*}
            \frac{\partial^2f^\star}{\partial \theta^2} (\rho, \theta) &=
            -\rho \left( \frac{\partial f}{\partial x}  \right)^\star(\rho, \theta)\cos \theta -\rho \left( \frac{\partial f}{\partial y}  \right)^\star (\rho, \theta)\sin\theta \\
                                                                       &\phantom{= } + \left( \frac{\partial ^2f}{\partial x^2}  \right)^\star (\rho, \theta)\rho^2\sin^2\theta+ \left( \frac{\partial ^2f}{\partial y\partial x}  \right)^\star (\rho, \theta)(-\rho^2\sin\theta\cos\theta) \\ 
                                                                       &\phantom{= } + \left( \frac{\partial^2f}{\partial x\partial y}  \right)^\star (\rho, \theta)(-\rho^2\sin\theta\cos\theta)+ \left( \frac{\partial^2f}{\partial y^2}  \right)^\star (\rho, \theta)\rho^2\cos^2\theta
    \end{align*}
\end{itemize}

Donc pour $\rho \neq 0$, \[
    \frac{1}{\rho^2} \frac{\partial^2f^\star}{\partial\theta^2} (\rho, \theta)+ \frac{\partial^2f}{\partial \rho^2} (\rho, \theta)=(\Delta f)^\star (\rho, \theta)-\frac1\rho\frac{\partial f^\star}{\partial \rho}(\rho, \theta)
\]
d'où \[
    (\Delta f)^\star (\rho, \theta)=\frac1{\rho^2}\frac{\partial^2f^\star}{\partial\theta^2}(\rho, \theta)+\frac{\partial^2f^\star}{\partial \rho^2}(\rho, \theta)+\frac1\rho \frac{\partial f^\star}{\partial \rho}(\rho, \theta)
\]

\begin{exo}
    Déterminer les $g\in\mathcal C^2(\mathbb R_+^\star, \mathbb R)$ tel que $f:(x_1, \cdots, x_n)\in\mathbb R^n\setminus \{0\}\longmapsto g(x_1^2+ \cdots+ x_n^2)$ est harmonique ($\Delta f=0$)
\end{exo}

\begin{proof}[Résolution]
    Soit $g$ solution. Alors \[
        \frac{\partial f}{\partial x_i}(x)=g'(x_1^2+\cdots+x_n^2) 2x_i
    \]
    et \[
        \frac{\partial ^2 f}{\partial x_i^2}(x)=g''(x_1^2+\cdots+x_n^2)4x_i^2+2g'(x_1^2+\cdots +x_n^2) 
    \]
    donc \[
        \Delta f=0\iff \forall x\in\mathbb R^n\setminus \{0\}, \quad \sum_{i=1}^ng''(\|x\|^2)4x_i^2+2g'(\|x\|^2)=0 \iff 4\|x\|^2g''(\|x\|^2)+2g'(\|x\|^2)n=0
    \]
    donc \[
        \Delta f=0\iff \forall r>0, \quad 4rg''(r)+2ng'(r)=0\iff \cdots \iff \exists A, B, \quad g:r\longmapsto B+\frac{A}{r^{n/2-1}}
    \]
\end{proof}

\section{Régression linéaire}

On dispose de données expérimentales $(x_{i, 1}, \cdots, x_{i, p}, y_i)$ avec $i\in\llbracket 1, n\rrbracket$. On sait que les vraies valeurs $(\bar x_{i, 1}, \cdots, \bar x_{i, p}, \bar y_i)$ vérifient une relation du type \[
    \bar y_i=\beta_1\bar x_{i, 1}+\cdots+\beta_p\bar x_{i, p}
\]

On veut connaitre les $\beta_i$ à l'aide des mesures effectuées. On note: \[
    X=(x_{i, j})_{\substack{1\leq i\leq n\\ 1\leq j\leq p}} \qquad \bar X=(\bar x_{i, j})_{\substack{1\leq i\leq n\\ 1\leq j\leq p}} \qquad \Gamma= \begin{pmatrix}
        \beta_1\\ \vdots\\\beta_p
    \end{pmatrix} \qquad Y= \begin{pmatrix}
        y_1\\\vdots\\ y_n
    \end{pmatrix}\qquad \bar Y= \begin{pmatrix}
        \bar y_1\\\vdots\\ \bar y_n
    \end{pmatrix}
\]
de sorte que $\bar Y=\bar X\Gamma$. On connait $X$ et $Y$, on veut approcher $\Gamma$. On va trouver $B$ qui minimise $\|Y-XB\|$ et on va montrer que si $\rg X=p$ alors il y a une unique solution minimale donnée par \[
    B=(X^\top X)^{-1}X^\top Y
\]

\begin{itemize}
    \item On note $f:B\longmapsto \|Y-BX\|^2$ de sorte que \[
            f(B+H)=\|Y\|^2-2(Y\;|\;XB)-2(Y\;|\;XH)+\|XB\|^2+2(XB\;|\;XH)+\underbrace{\|XH\|^2}_{o_0(H)}
        \]
        donc \[
            \diff f_B(H)=2(XB-Y\;|\;XH)=-2(X^\top Y\;|\; H)+2(X^\top XB\;|\;H)
        \]
    \item $B$ point critique $\iff \forall H, (-X^\top Y+X^\top XB\;|\;H)=0 \iff X^\top XB=X^\top Y$
    \item Si $p=\rg X$ alors $\dim\Ker X=0$ et $X$ est une matrice injective. \[
            B\in \Ker(X^\top X)\iff X^\top XB=0\implies B^\top X^\top XB=0\implies \|XB\|^2=0\implies B=0
        \]
        donc $X^\top X$ est inversible.
    \item $B$ point critique $\iff B=(X^\top X)^{-1}X^\top Y$
\end{itemize}

\section{Exemple d'équation aux dérivées partielles}

On va déterminer les solution $f\in\mathcal C^1(\mathbb R_+^\star\times \mathbb R, \mathbb R)$ tels que \[
    x \frac{\partial f}{\partial y} (x, y)-y \frac{\partial f}{\partial x} (x, y)=k f(x, y)
\]

Pour $r\in\mathbb R^\star$, $\theta\in\left]-\frac\pi2,\frac\pi2\right[$, $f^\star(r, \theta)=f(r\cos\theta,r\sin\theta)$. Donc: \[
    \frac{\partial f^\star}{\partial \theta} (r, \theta)=-r\sin\theta \left(\frac{\partial f}{\partial x}\right)^\star (r, \theta)+r\cos \theta \left(\frac{\partial f}{\partial y}\right)^\star (r,\theta)=kf^\star(r,\theta)
\]

d'où \[
    f \text{ solution }\iff \forall r>0, \exists a(r)\;/\; \forall \theta \in \left]-\frac\pi2,\frac\pi2\right[,\quad f^\star(r\theta)=a(r)\exp(k\theta)
\]
donc les solutions sont de la forme $f(x, y)=a \!\!\,\left( \sqrt{x^2+y^2} \right)\exp \left( k\arctan \left( \frac yx \right) \right)$, qui convient si elle est $\mathcal C^1$ (i.e. si $a$ est $\mathcal C^1$)

\section{Méthodes de gradient}


On note $J:\mathbb R^n\to \mathbb R$ minorée $\mathcal C^1$ et on suppose qu'elle atteint sa borne inférieure en un point $x^\star$.

\textbf{Idée:} On va construire une suite $(x_n)$ telle que $J(x_n)\to J(x^\star)$. On a \[
    J(x_n+tv)=J(x_n)+t(\nabla J(x_n)\;|\; v)+o_0(tv)
\]
et on va prendre $v=-\nabla J(x_n)$ de sorte que $x_{n+1}=x_n-t_n\nabla J(x_n)$, $t_n>0$ à choisir

\subsection{Méthode de gradient conjugué}

\index{gradient!méthode de gradient conjugué}

On note $A\in S_p^{++}(\mathbb R)$, $b\in\mathbb R^p$ et \[
    \begin{matrix}
        J: & \mathbb R^p & \longrightarrow & \mathbb R \\
           & x & \longmapsto & \dfrac12(Ax\;|\;x)-(b\;|\;x)
    \end{matrix}
\]
La fonction $J$ est $\mathcal C^1$ et \[
    J(x)\geq \frac 12 \underbrace{\lambda_1}_{\min \Sp(A)>0}\|x\|^2-\|b\|\cdot \|x\| \implies J(x)\xrightarrow[\|x\|\to +\infty]{}+\infty
\]
par conséquent, $J$ atteint son minimum (i.e. $x^\star$ existe) car atteint sur un compact du type $\mathcal B_f(0, R)$.

On a \[
    \diff J_x(h)=(Ax-b\;|\;h)
\]
et $\diff J_{x^\star}=0$ donc $x^\star=A^{-1}b$

On va mettre en place une méthode de descente de gradient. \[
    J(x+t\nabla J(x))=J(x)+ t (Ax-b\;|\;\nabla J(x))+\frac12t^2(A\nabla J(x)\;|\;\nabla J(x))
\]
Cette fonction de $t$ atteint un minimum lorsque \[
    (Ax-b\;|\;\nabla J(x))+t(A\nabla J(x)\;|\; \nabla J(x))=0 \iff t=-\frac{(Ax-b\;|\;\nabla J(x))}{(A\nabla J(x)\;|\; \nabla J(x))}
\]

Or $\diff J_x(h)=(Ax-b\;|\;h)=(\nabla J(x)\;|\; h)$ donc $\nabla J(x)=Ax-b$. On construit alors $(x_n)$ comme suit: \begin{itemize}
    \item $x_0$ est quelconque
    \item $x_{n+1}=x_n+t_n\nabla J(x)$ avec \[
            t_n = \begin{cases}
                -\dfrac{\|Ax_n-b\|^2}{(A(Ax_n-b)\;|\;Ax_n-b)} &\text{ si } Ax_n-b\neq 0 \\[1em]
                \hspace{2cm}0 &\text{ sinon }
            \end{cases}
        \]
\end{itemize}

On a \[
    J(x_{n+1})=J(x_n) + t_n\|Ax_n-b\|^2 +\frac12t_n^2(A(Ax_n-b)\;|\;Ax_n-b)
\]
soit encore \[
    J(x_{n+1})=J(x_n) -\frac{\|Ax_n-b\|^4}{2(A(Ax_n-b)\;|\;Ax_n-b)} \leq J(x_n)
\]
si $x_n\neq x_{n+1}$. On a égalité si $x_n=x_{n+1}$.  
Si $(x_n)$ ne prend pas la valeur $x^\star$ alors $(J(x_n))$ est décroissante minorée donc converge. On a alors $Ax_n-b\longrightarrow 0$ donc $x_n\longrightarrow x^\star$.

\subsection{Méthode de gradient à pas optimal}
\index{gradient!méthode de gradient à pas optimal}

Soit $f:\mathbb R^n\to \mathbb R$ de classe $\mathcal C^1$ telle que \[
    \exists \alpha > 0, \forall u, v\in\mathbb R^n, (\nabla f(u)-\nabla f(v)\;|\; u-v)\geq \alpha\|u-v\|^2
\]

\begin{enumerate}
    \item On va établir que \[
            \forall u, v\in\mathbb R^n, f(u)\geq f(v)+(\nabla f(u)\;|\;v-u)+\frac\alpha2\|v-u\|^2
        \]
        et en déduire que $f$ a un minimum global.

        \begin{align*}
            f(v)-f(u) &= \int_0^1\diff f_{(1-t)v+tu}(v-u)\diff t \\
                      &= \int_0^1(\nabla f((1-t)v+tu)\;|\; v-u) \diff t \\ 
                      &\geq \int_0^1(\nabla f(u)\;|\;v-u)+\alpha(1-t)\|v-u\|^2\diff t \\
                      &=(\nabla f(u)\;|\;v-u)+\frac{\alpha\|v-u\|^2}2
        \end{align*}
        puis \[
            f(v)\geq f(u)-\|\nabla f(u)\|\cdot\|v-u\|+\frac\alpha2\|v-u\|^2\xrightarrow[\|v\|\to+\infty]{}+\infty
        \]
        Donc $f$ possède un minimum global (car pour une boule fermée centrée en $0$ assez grande, la borne inférieure globale est la borne inférieure sur cette boule compacte donc c'est un minimum par continuité de $f$).
    \item On va montrer l'unicité du minimum.

        Si $x^\star$ et $v^\star$ sont tels que $f(x^\star)=f(v^\star)=\min f$ alors \[
            \nabla f(v^\star)=\nabla f(v^\star)=0\implies 0\geq \alpha\|x^\star-v^\star\|^2\implies x^\star=v^\star
        \]
    \item On souhaite approcher $x^\star$.
        \begin{enumerate}
            \item Soit $u\in\mathbb R^n\setminus\{x^\star\}$. On va montrer que $\varphi_u:t\longmapsto f(y+t\nabla f(u))$ admet un minimum global.

                On a \[
                    \varphi_u'(t)=\diff f_{u+t\nabla f(u)}(\nabla f(u))=(\nabla f(u+t\nabla f(u))\;|\;\nabla f(u))
                \]
                et \begin{align*}
                    (t-s)(\varphi_u'(t)-\varphi_u'(s))&=(\nabla f(\underbrace{u+t\nabla f(u)}_{u'})-\nabla f(\underbrace{u+s\nabla f(u)}_{v'})\;|\;\underbrace{(t-s)\nabla f(u)}_{u'-v'}) \\
                                                      &\geq \alpha\|v'-u'\|^2=\alpha(t-s)^2\|\nabla f(u)\|^2\geq 0
                \end{align*}
                donc $\phi_u'$ est croissante et $\varphi_u$ est convexe. Puis, \begin{align*}
                    \varphi_u'(t)-\varphi_u'(s)\geq \alpha(t-s)\|\nabla f(u)\|^2 &\xrightarrow[t\to+\infty]{} +\infty \\ &\xrightarrow[s\to-\infty]{}+\infty
                \end{align*}
                donc $\phi_u'$ est croissante surjective dans $\mathbb R$ d'où l'unicité et l'existence du minimum de $\varphi_u$.
            \item On considère le schéma numérique suivant: \[
                    \begin{cases}
                        u_0\in\mathbb R^n\\
                        u_{p+1}=u_p &\text{ si }u_p=x^\star \text{ (on suppose que cela n'arrive pas) }\\
                        u_{p+1}=u_p+t_p\nabla f(u_p) &\text{ sinon }
                    \end{cases}
                \]
                où $t_p$ minimise $\varphi_{u_p}$. On va montrer que $(u_k)$ converge vers $x^\star$.

                \[
                    \varphi'_{u_k}(t_k)=0\implies (\nabla f(u_{k+1})\;|\;\nabla f(u_k))=0
                \]
                d'où \[
                    f(u_k)\geq f(u_{k+1})+\underbrace{(\nabla f(u_{k+1})\;|\; u_k-u_{k+1})}_{=0}+\frac\alpha2\|u_k-u_{k+1}\|^2
                \]
                donc \[
                    f(u_k)-f(u_{k+1})\geq 0
                \]
                donc $(f(u_k))_k$ est décroissante minorée donc converge. Puis \[
                    \underbrace{f(u_k)-f(u_{k+1})}_{\longrightarrow 0}\geq \frac\alpha2\|u_k-u_{k+1}\|^2
                \]
                or $(u_k)$ bornée car sinon $(f(u_k))$ non convergente. On note $R$ un majorant de $(\|u_k\|)$ et $\nabla f$ est uniformément continue sur $\mathcal B_f (0, R+)$ donc \[
                    \nabla f(u_{k+1})-\nabla f(u_k)=\nabla f(u_k+u_{k+1}-u_k)-\nabla f(u_k) \xrightarrow[k\to+\infty ]{} 0
                \]
                or \[
                    \|\nabla f(u_{k+1})-\nabla f(u_k)\|^2=\|\nabla f(u_{k+1})\|^2+\|\nabla f(u_k)\|^2\xrightarrow[k\to+\infty]{}0
                \]
                donc $\|\nabla f(u_k)\|\to 0$. Enfin, \[
                    (\underbrace{\nabla f(u_k)}_{\to 0}-\underbrace{\nabla f(x^\star)}_{=0}\;|\;u_k-x^\star)\geq \alpha\|u_k-x^\star\| ^2
                \] d'où $u_k\to x^\star$
        \end{enumerate}
\end{enumerate}

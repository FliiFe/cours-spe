\ifsolo
    ~

    \vspace{1cm}

    \begin{center}
        \textbf{\LARGE Espaces préhilbertiens réels} \\[1em]
    \end{center}
    \tableofcontents
\else
    \chapter{Espaces préhilbertiens réels}

    \minitoc
\fi
\thispagestyle{empty}

\section{Rappels}

Dans tout le chapitre on ne considèrera que des $ \R$-espaces vectoriels, sauf mention contraire.

\begin{dfn}
    On appelle \textbf{produit scalaire}\index{produit scalaire} sur $E$ une forme bilinéaire symétrique définie positive. Un $ \R$-espace vectoriel $E$ muni d'un produit scalaire  $ \scalar{\;}{\;} $ se note $(E, \scalar{\;}{\;} )$. Si $E$ est de dimension finie, on dit qu'il est  \textbf{euclidien}\index{espace euclidien}. Sinon, il est dit \textbf{préhilbertien}\index{espace préhilbertien réel}. On dit que $x$ et  $y$ sont  \textbf{orthogonaux} si $ \scalar{x}{y} =0$
\end{dfn}

\begin{defprop}
    \Hyp $(E, \left\langle\; \middle| \; \right\rangle )$ un espace préhilbertien réel

    \begin{concenum}
    \item Pour $x \in E$, l'application $x\longmapsto \sqrt{\scalar{x}{x}} $ est une norme. On l'appelle norme associée à $ \scalar{\;}{\;} $
    \item $\forall  x, y \in  E, \quad  \|x+y\|^2 = \|x\|^2 + 2 \scalar{x}{y} +\|y\|^2$
    \item $\displaystyle\forall  x, y \in  E, \quad  \scalar{x}{y} =\frac{1}{2}\left( \|x+y\|^2 - \|x\|^2 -\|y\|^2 \right) =\frac{1}{4} (\|x+y\|^2 -\|x-y\|^2 )$
    \item $\forall  x, y \in  E, \quad  \|x+y\|^2 +\|x-y\|^2 =2 (\|x\|^2 +\|y\|^2 )$ (identité du parallélogramme)
    \end{concenum}
\end{defprop}

\begin{proof}
On verra une preuve du premier point plus tard. Le reste est facile.
\end{proof}

\begin{dfn}
    Si $A\subset E$ où  $(E, \scalar{\;}{\;} )$ est préhilbertien et $A\neq \emptyset$, on définit \begin{enumerate}
        \item $A^\perp= \left\{  x \in  E \suchthat \forall  a \in  A, \scalar{x}{a} =0 \right\} $
        \item $A\perp B \iff  \forall  a \in  A, \forall  b \in  B, \scalar{a}{b} =0$
    \end{enumerate}
\end{dfn}

\begin{rem}
$\forall  a \in  A, \varphi_a:x \longmapsto \scalar{a}{x} \in E^\star$ et \[
A^\perp = \bigcap_{a \in  A}  \Ker \varphi_a
\] 
\end{rem}

\section{Produits scalaires usuels}

\subsection{Produits scalaires sur \texorpdfstring{$\R^n$ }{R\^{}n}}

On prend $x=(x_1, \cdots , x_n)$ et $y=(y_1, \cdots , y_n)$. Le produit scalaire canonique est définit par \[
    \scalar{x}{y} =\sum_{i=1}^nx_iy_i
\] 

\subsection{Produit scalaire à poids sur \texorpdfstring{$\R[X]$ }{R[X]}}

On se donne une fonction poids $f \in  \mathcal  C(I, \R_+)\setminus \left\{ 0 \right\} $ telle que $\forall  n \in  \N, t\longmapsto t^nf(t)$ est intégrable sur l'intervalle $I$. Dans ce cas on peut définir \[
    \scalar{P}{Q} =\int_I f(t)P(t)Q(t)\diff t
\] 

\subsection{Produit scalaire dans \texorpdfstring{$\mathcal  L_c^2(I, \R)$}{les fonctions continues de carré intégrable}}

On a $\forall  f,g \in  \mathcal  L_c^2(I, \R), \qquad  |fg|\leq \dfrac{f^2+g^2}{2}$ donc $fg$ est intégrable sur  $I$ et on peut poser  \[
\scalar{f}{g} =\int_Ifg
\] 

\begin{rem}
    On peut définir un produit scalaire à poids $w \in  \mathcal  C(I, \R_+^\star)$ sur l'ensemble \[ \mathcal  L_{c,w}^2= \left\{  f \in  \mathcal  C(I, \R) \suchthat wf^2 \text{ intégrable } \right\} \]
\end{rem}

\section{Projections orthogonales}

\begin{thmdef}
    \Hyp $(E, \scalar{\;}{\;} )$ préhilbertien réel, $F$ un sev de  $E$ de dimension finie et  $(e_1, \cdots , e_n)$ une base orthonormée de $F$
     \begin{concenum}
     \item $F^\perp$ est un sev de  $E$
     \item  $E=F\oplus F^\perp$
     \item  On appelle \textbf{projection orthogonale}\index{projection orthogonale} sur $F$ l'application linéaire définie par  \[\forall  x \in  E, \quad  \pi_F(x)= \scalar{e_1}{x}e_1+\cdots +\scalar{e_n}{x} e_n \]
     \item $(F^\perp)^\perp$ si  $E$ est euclidien
    \end{concenum}
\end{thmdef}

\begin{proof}~
\begin{enumerate}
    \item C'est la linéarité du produit scalaire
    \item  On se donne $x \in E$, et on pose $y= \scalar{x}{e_1} e_1+\cdots +\scalar{x}{e_n} e_n$. On a \[
            \forall  i \in  \llbracket 1, n \rrbracket, \quad   \scalar{x-y}{e_i} = \scalar{x}{e_i} - \sum_{i=j}^n \scalar{y}{e_j} \scalar{e_i}{e_j} =0
    \] 
    donc $x-y \in  F^\perp$ et $x=y+(x-y)$ d'où la somme directe. Puis la somme est directe car  $F\cap F^\perp = \left\{  0 \right\} $ 
    \stepcounter{enumi}
\item $F\subset (F^\perp)^\perp$ et même dimension finie.
\end{enumerate}
\end{proof}

\begin{defprop}
    \Hyp $(E, \scalar{\;}{\;} )$ préhilbertien, $p \in  \mathcal  L(E)$ projecteur
    \begin{concenum}
    \item On dira que $p$ est un \textbf{projecteur orthogonal}\index{projection orthogonale} si $\Ker p\perp \Img p$
    \item Il y a équivalence entre  \begin{enumerate}
        \item $p$ est un projecteur orthogonal
        \item $p$ est \textbf{autoadjoint}:  $\forall  x, y \in  E, \quad  \scalar{p(x)}{y}= \scalar{x}{p(y)}  $
    \end{enumerate}
    \end{concenum}
\end{defprop}

\begin{proof}~\\
    $(a\implies b)$ $x, y \in  E$ \[
        \scalar{p(x)}{y} = \scalar{p(x)}{y-p(y)+p(y)} =\scalar{p(x)}{p(y)} =\scalar{p(y)}{p(x)} =\scalar{x}{p(y)} 
    \] 
    $(b\implies a)$ $x \in  \Ker p, y \in  \Img p$, \[
        \scalar{x}{y} = \scalar{x}{p(y)} = \scalar{p(x)}{y}=0
    \]
\end{proof}

\begin{exo}
    Montrer que dans $E$ préhilbertien, un projecteur $p$ tel que  $\forall  x \in  E, \quad  \|p(x)\|\leq \|x\|$ est orthogonal
\end{exo}

\begin{proof}[Résolution]
Soit $x \in  \Ker p, y \in  \Img p, t \in  \R$. \[
    \|p(y)\|^2 =\|p(y+tx)\|^2 \leq \|y+tx\| \iff  \|p(y)\|^2=\|y\|^2  \leq \|y\|^2 +2t \scalar{y}{x} + \|x\|^2 t ^2 
\] 
donc  \[
    0\leq 2t \scalar{y}{x} + \|x\|^2 t^2
\] 
d'où $\scalar{x}{y} =0$ ($\Delta\leq 0$)
\end{proof}

\section{Distance d'un point à un sous-espace}

\begin{thmdef}
    \Hyp $(E, \scalar{\;}{\;} )$ préhilbertien réel et $F$ sev de $E$
    \begin{concenum}
    \item Soit $a \in  E$. On pose \[
            d(a, F)=\inf_{x \in  F} \|x-a\|
    \] 
\item En dimension finie, $d(a, F)=\|a-\pi_F(a)\|=\|\pi_{F^\perp}(a)\|$
    \end{concenum}
\end{thmdef}

\begin{proof}
    2. $\|a-x\|^2 =\|a-\pi_F(x)\|^2 + \|\pi_F-x\|^2 \geq \|a-\pi_F(a)\|^2 $
\end{proof}

\begin{ex}
Déterminons \[
    d=\inf_{a, b \in  \R}\int_0^1 (\ln x-ax-b)^2 \diff x
\] 
On se place sur $E=\mathcal  L_c^1(]0, 1], \R)$ avec le produit scalaire usuel. On note $F=\R_1[X]$ de dimension finie, et on a \[
    d=d(\ln, F)^2
\] 
On cherche $a, b \in  \R$ tels que $\ln(x)-ax-b\perp F$. C'est équivalent à  \[
\begin{dcases}
    \scalar{\ln x - ax - b}{1} =0 \\
    \scalar{\ln x - ax - b}{x} =0
\end{dcases}
\iff  \int_0^1(\ln x-ax-b)\diff x=\int_0^1 x\ln x-ax^2 -bx\diff x=0
\] 
soit encore $a=3$ et  $b=-\dfrac{5}{2}$. On en déduit $d=\dfrac{1}{4}$
\end{ex}

\subsection{Méthode de Gauss.}
On note $(E, \scalar{\;}{\;} )$ euclidien, $x=(x_1, \cdots , x_p)$ une famille de $p$ vecteurs. On appelle \textbf{matrice de Gram}\index{matrice de Gram} la matrice \[
    G(x)= \begin{pmatrix}
        \scalar{x_1}{x_1} & \cdots & \scalar{x_1}{x_p} \\
        \vdots & \ddots & \vdots \\
        \scalar{x_p}{x_1} & \cdots  & \scalar{x_p}{x_p} 
    \end{pmatrix}
\] 
On note \[
T= \begin{pmatrix}
    t_1 \\ \cdots \\ t_p
\end{pmatrix} \in  \mathcal  M_{p,1}(\R)
\] 
de sorte que \begin{align*}
    T^\top G(x)T&=(t_1, \cdots , t_p) \begin{pmatrix}
        \scalar{x_1}{x_1} & \cdots & \scalar{x_1}{x_p} \\
        \vdots & \ddots & \vdots \\
        \scalar{x_p}{x_1} & \cdots  & \scalar{x_n}{x_p} 
    \end{pmatrix}\begin{pmatrix}
    t_1 \\ \cdots \\ t_p
\end{pmatrix} \\ &= \|t_1x_1+\cdots +t_px_p\|^2 
\end{align*}
Si $x$ est libre, alors \[
    G(x)T=0 \implies T^\top G(x)T=0 \implies \|t_1x_1+\cdots +t_px_p\|^2 =0 \implies T=0
\] 
donc $G(x) \in  \mathrm{GL}_n(\R)$. On suppose $G(x)$ inversible.  \[
    t_1x_1+\cdots +t_px_p=0 \implies G(x)T=0=0\implies T=0
\] 
donc $x$ est libre. On a donc $\rg(G(x))=p \iff  \rg(x)=p$. Supposons que $\rg(x)=m$ et quitte à permutter, on suppose  $(x_1, \cdots , x_m)$ libre.

Pour $k\geq m+1$, on écrit \[
    x_k=\sum_{j=1}^m \alpha_{k,j}x_j
\] 
Avec des transvections (qui ne modifient pas le rang) on se ramène, en partant de $G(x)$, à  \[
\begin{pmatrix}
    \scalar{x_1}{x_1} & \cdots  & \scalar{x_1}{x_m} & 0 & \cdots  & 0 \\
    \vdots &  & \vdots & \vdots & & \vdots\\
    \scalar{x_p}{x_1} &\cdots & \scalar{x_p}{x_m} &0 & \cdots & 0
\end{pmatrix}
\] 
qui est de rang $m$ (on peut en extraire $G(x_1, \cdots , x_m)$ donc $\rg(G(x))\geq m$ et l'autre inégalité est évidente). On a donc toujours $\rg (G(x))=\rg(x)$.


\paragraph{Lien entre les déterminants de Gram et les distances.}
On note $F$ un sev de  $E$ de base  $(x_1, \cdots , x_p)$. On note $u \in  E$ et $\pi_F(u)=\lambda_1 x_1+\cdots +\lambda_px_p$.
\[
    \det(G(x, u)) = \begin{vmatrix}
        \scalar{x_1}{x_1} & \cdots  & \scalar{x_1}{x_p} & \scalar{x_1}{u} \\
        \vdots & \ddots & \vdots & \vdots \\
        \scalar{x_p}{x_1} & \cdots  & \scalar{x_p}{x_p} & \scalar{x_p}{u} \\
        \scalar{u}{x_1} & \cdots  & \scalar{u}{x_p}  &\scalar{u}{u} 
    \end{vmatrix}
\] 
On fait $C_{p+1}\leftarrow C_{p+1}-\lambda C_1-\cdots -\lambda_p C_p$ et idem sur les lignes, on obtient  \[
\begin{vmatrix}
    \scalar{x_1}{x_1} & \cdots  & \scalar{x_1}{x_p} & \scalar{x_1}{u-\pi_F(u)} \\
        \vdots & \ddots & \vdots & \vdots \\
        \scalar{x_p}{x_1} & \cdots  & \scalar{x_p}{x_p} & \scalar{x_p}{u-\pi_F(u)} \\
        \scalar{u-\pi_F(u)}{x_1} & \cdots  & \scalar{u-\pi_F(u)}{x_p}  &\scalar{u-\pi_F(u)}{u-\pi_F(u)} 
    \end{vmatrix}=\|u-\pi_F(u)\|^2 \det(G(x))
\] 
donc \[
    d(u, F)^2 = \frac{\det(G(x, u))}{\det(G(x))}
\] 

\subsection{Projection sur un compact convexe}

\paragraph{Existence et unicité du projeté.}
On note $(E, \scalar{\;}{\;} )$ euclidien, $C$ un compact convexe. On va montrer que  \[
\forall  a \in  F, \quad  \exists  ! c \in  C, \qquad  \inf_{x \in  C}\|x-a\|=\|c-a\|
\] 
On note $\Delta$ l'inf et  $(c_n) \in  C^{\N}$ une suite telle que \[
\|c_n-a\| \xrightarrow[n\to+\infty]{}\Delta
\] 
Une telle suite existe et $C$ est compact donc on peut supposer (quitte à extraire) que c'est une suite convergente de limite  $x \in  C$, ce qui montre l'existence.

Si $c, c' \in  C$ conviennent, on note $u=c-a$ et  $v=c'-a$. L'identité du parallélogramme donne  \[
    \|x+y\|^2 + \|u-v\|^2 =2(\|u\|^2 + \|v\|^2 )
\] 
donc \[
    \|c-c'\|^2 +\underbrace{4 \|\frac{c+c'}{2}-a\|}_{\geq 4\Delta^2}=4\Delta^2
\] 
et $\|c-c'\|\leq 0 \implies c=c'$

\begin{rem}
    Si on ne suppose pas que $C$ est compact, alors l'unicité reste vraie mais pas l'existence. Mais, si  $(c_n) \in  C^{\N}$ est minimisante, \[
        \|c_n-c_m\|^2 =\underbrace{2(\|c_n-a\|^2 +\|c_m-a\|^2 )}_{\leq 4\Delta^2+\epsilon \text{ APCR }}-\underbrace{4 \|\frac{c_n+c_m}{2}-a\|}_{\geq 4\Delta^2} \leq \epsilon\text{ PARC }
    \] 
    donc $(c_n)$ est une suite de Cauchy, qui converge (car $E$ est un espace de Banach). Si $C$ est fermé,  $(c_n)$ converge vers un élément  $c \in  C$
\end{rem}

\begin{notation}
    On note $c=\pi_C(a)$
\end{notation}

\paragraph{Caractérisation du projeté.}
On va maintenant montrer qu'il y a équivalence entre \begin{enumerate}
    \item $z=\pi_C(a)$
    \item $\forall  y \in  C, \quad  \scalar{y-z}{a-z} \leq 0$
\end{enumerate}

$(1\implies 2)$ Pour $t \in  [0, 1]$, on a $(1-t)z+ty \in  C$ pour $y \in  C$. Puis \[
    \|a-z\|^2 \leq \|a-(1-t)z+ty\|^2 =\|a-z\|^2 +2t \scalar{a-z}{z-y} +t^2 \|z-y\|^2 
\] 
donc \[
2 \scalar{a-z}{y-z} \leq t \|z-y\|^2 
\] 
et $t \to  0$ permet de conclure.

$(2 \implies  1)$ $\forall  t \in  [0, 1], \forall  y \in  C,$ \[
    t\scalar{a-z}{y-z} \leq t \|y-z\|^2 \implies   \|a-z\|^2 \leq  \|a-y\|^2 
\] 
par le raisonnement inverse, en prenant $t=1$.

\paragraph{Caractère lipschitzien.} 
Pour $x, x' \in  E,$ \[
    \scalar{\pi_C(x')-\pi_C(x)}{x-\pi_C(x)} \leq 0 \qquad  \text{ et } \qquad  \scalar{\pi_C(x)-\pi_C(x')}{x'-\pi_C(x')} \leq 0
\] 
donc en ajoutant les deux inégalités \[
    \|\pi_C(x')-\pi_C(x)\|^2 \leq \scalar{\pi_C(x')-\pi_C(x)}{x'-x} \underset{\text{CS}}\leq \|\pi_C(x')-\pi_C(x)\|\times \|x'-x\|
\] 
d'où le caractère $1$-lipschitzien (on divise par $\|\pi_C(x')-\pi_C(x)\|$ si c'est non nul, le cas nul est évident)

\subsection{Procédé de Gram-Schmidt}

\index{Gram-Schmidt}
On note $E$ un espace préhilbertien réel,  $(f_1, \cdots , f_n)$ une famille libre. On va montrer qu'il existe une famille $(e_1, \cdots , e_n)$ orthogonale telle que $\forall  i \leq n, \Vect(e_1, \cdots , e_i)=\Vect(f_1, \cdots , f_i)$.

Principe: \begin{itemize}
    \item On pose $e_1=f_1$
    \item On suppose  $e_1, \cdots , e_i$ construits. On cherche $e_{i+1}$ de la forme \[
    e_{i+1}=\lambda_{i,1}e_1+\cdots +\lambda_{i,i}e_i+f_{i+1}
    \] 
    La condition $e_{i+1}\perp e_1, \cdots , e_i$ impose \[
        \scalar{e_{i+1}}{e_1} = \cdots =\scalar{e_{i+1}}{e_i} =0 \iff  \begin{dcases}
            0=\lambda_{i,1} \scalar{e_1}{e_i} + \scalar{f_{i+1}}{e_1} \\
            \cdots \\
            0=\lambda_{i,i}\scalar{e_i}{e_i} +\scalar{f_{i+1}}{e_i} 
        \end{dcases} \implies \lambda_{i,k}=- \frac{\scalar{f_{i+1}}{e_k} }{\scalar{e_k}{e_k} }
    \] 
    et ces valeurs conviennent.
\end{itemize}

\begin{rem}
La matrice de passage entre une famille libre et son orthogonalisée de Gram-Schmidt est triangulaire supérieure, et sa diagonale ne contient que la valeur propre $1$.
\end{rem}

\section{Inégalité de Hadamard}

On note $\mathcal  F=(f_1, \cdots , f_p)$ une famille libre de $E$, et  $\mathcal  E=(e_1, \cdots , e_p)$ la famille orthonormée construite par le procédé de Gram-Schmidt.

On note $P_{\mathcal  F, \mathcal  E}=\mathcal  M_{\mathcal  F, \mathcal  E}(\id)$ de sorte que \[
    f_j=\sum_{i=1}^p \left( P_{\mathcal  F, \mathcal  E} \right) _{i,j}e_i \qquad  \text{ et } \qquad  e_j=\sum_{i=1}^p \left( P_{\mathcal  E, \mathcal  F} \right) _{i,j}f_i
\] 
On note $T=P_{\mathcal  E, \mathcal  F}  $, et on a \begin{align*}
    \left( T^\top G(\mathcal  F)T \right) _{i,j}&=\sum_{1\leq k,l\leq p} (T^\top)_{i,k}G(\mathcal  F)_{k,l}T_{l,j} \\
                                                &= \sum_{1\leq k,l\leq p} T_{k,i} \scalar{f_k}{f_l} T_{l,j} \\
                                                &= \scalar{\sum_{k=1}^pT_{k,i}f_k}{\sum_{l=1}^pT_{l,j}f_l} =\scalar{e_i}{e_j} =\delta_{i,j}
\end{align*}
donc \[
    T^\top G(\mathcal  F)T=I_p \qquad \text{ et }\qquad \det(G(\mathcal  F))=\frac{1}{(\det T)^2}=\prod_{i=1}^p \scalar{f_i}{e_i} ^2 \underset{\text{CS}}\leq \prod_{i=1}^p \|f_i\|^2 
\] 
Avec égalité si et seulement si $(f_1, \cdots , f_p)$ est orthogonale, i.e. si $e_i=\dfrac{f_i}{\|f_i\|^2 }$ 

Prenons $A \in  \mathcal M_n(\R)$ inversible, on note $C_1, \cdots  ,C_n$ ses colonnes. On a \[
A^\top A= \begin{pmatrix}
    C_1^\top \\ \vdots \\ C_n^\top
\end{pmatrix} (C_1, \cdots , C_n)= (\underbrace{C_i^\top C_j}_{\scalar{C_i}{C_j} })_{i,j}
\] 
donc \[
    \det(A^\top A)=\det(A)^2\leq \prod_{i=1}^n \|C_i\|^2
\] 
ce qui reste vrai pour $A$ non inversible. On a donc finalement toujours  \[
    \det(A)^2\leq \prod_{n=1}^{n} \|C_i\|^2
\] 

\section{Famille totale}

\begin{dfn}
    On se place dans $(E, \scalar{\;}{\;} )$ un espace préhilbertien réel de dimension infinie. On dira que $(e_k)_{k \in \N} \in E^{\N}$ est totale si c'est une famille orthonormale et si \[
        \bar{\Vect\left( (e_k)_{k \in  \N} \right) }=E
    \] 
\end{dfn}

\begin{thm}
    \Hyp $(e_k)_{k \in  \N}$ famille totale, $E_n=\Vect(e_1, \cdots , e_n)$, $\pi_n$ projecteur orthogonal sur $E_n$
     \begin{concenum}
     \item $\forall  x \in  E, \quad  \|x-\pi_n(x)\|^2 \xrightarrow[n\to+\infty]{}0$
     \item Égalité de Parseval: \index{Parseval (égalité de -- )}\[
     \forall  x \in  E, \|x\|^2 = \sum_{k=0}^{+\infty} \scalar{x}{e_k} ^2 
     \] 
    \end{concenum}
\end{thm}

\begin{proof}~
\begin{enumerate}
    \item $E= \bar{\Vect((e_k)_k)}$. On note $x \in  E$, $\epsilon>0$. Il existe $y \in  \Vect((e_k)_k)$ tel que $\|x-y\|\leq \epsilon$ et il existe $N \in  \N$ tel que $\forall  n\geq N, y \in  E_n$ donc $\|x-\pi_n(x)\|\leq \|x-y\|\leq \epsilon$
    \item \[
            \|x\|^2 = \|x-\pi_n(x)\|^2 + \|\pi_n(x)\|^2 
    \]
    et \[
        \pi_n(x)=\sum_{k=0}^n \scalar{x}{e_k} e_k \quad \text{ donc } \quad  \|\pi_n(x)\|^2 =\sum_{k=0}^n \scalar{x}{e_k} ^2 .
    \] 
    Finalement, \[
        \|x\|^2 = \underbrace{\|x-\pi_n(x)\|^2}_{\xrightarrow[n\to+\infty]{}0} +\|\pi_n(x)\|^2 \xrightarrow[n\to+\infty]{} \sum_{n=0}^{+\infty} \scalar{x}{e_n} ^2 
    \] 
\end{enumerate}
\end{proof}

\begin{rem}
    \begin{itemize}
        \item On peut écrire \[
                \pi_n(x)= \sum_{n=0}^{n} \scalar{x}{e_n} e_n \xrightarrow[n\to+\infty]{}x \qquad  \text{ donc } \qquad  x= \sum_{n=0}^{+\infty} \scalar{x}{e_n} e_n
        \] 
    \item On se donne $(e_k)$ orthonormale. Pour  $x \in  E$, on pose $f_n:x\longmapsto \|x-\pi_n(x)\|$. Si $(e_k)$ est totale alors pour chaque  $x$,  $f_n(x) \xrightarrow[\hspace{1em}]{}0$. Si pour chaque $x$,  $f_n(x) \xrightarrow[\hspace{1em}]{}0$, alors $ \pi_n(x) \xrightarrow[\hspace{1em}]{}x$ et $x \in  \bar{ \Vect((e_k)_k) }$ donc la famille est totale.
    \end{itemize}
\end{rem}

\section{Les séries de Fourier}

On note $f \in  \mathcal  C^0_{2\pi}(\R, \C)$, et on définit \[
    c_n(f)= \frac{1}{2\pi}\int_0^{2\pi} f(t)e^{-int}\diff t=\frac{1}{2\pi}\int_{-\pi}^\pi f(t)e^{-int}\diff t
\] 
et \[
    S_n(f):x \longmapsto \sum_{k=-n}^{n} c_k(f)e_k(x) \qquad  \text{ où } \qquad e_k:x \longmapsto e^{ikx}
\] 
On introduit le produit hermitien \[
    \pscalar{f}{g}=\frac{1}{4\pi} \int_0^{2\pi} \bar{f}g
\] 

Quelques remarques: \begin{itemize}
    \item $c_n(f)\xrightarrow[n \to  +\infty]{}0$ (Riemann-Lebesgue)
    \item  $ \pscalar{e_k}{e_l}=\delta_{k,l} $
    \item Si $k \in  \llbracket -n, n \rrbracket $, \[
            \pscalar{f-S_n(f)}{e_k} = \pscalar{f}{e_k} - \pscalar{\sum_{l=-n}^{n} c_l(f)e_l}{e_k}= \pscalar f{e_k}-\sum_{l=-n}^n \bar{c_l(f)} \pscalar{e_l}{e_k} =0
    \] 
\end{itemize}
En particulier, $\pscalar{f-S_n(f)}{S_n(f)} =0$ et \[
    \pscalar{f}{f} =\pscalar{f-S_n(f)}{f-S_n(f)} + \pscalar{S_n(f)}{S_n(f)} \geq \pscalar{S_n(f)}{S_n(f)} = \sum_{k=-n}^{n} |c_k(f)|^2 \text{ CV }
\] 

\subsection{Théorème de Féjer}\index{Féjer (théorème de -- )}

On va montrer que \[
    \sigma_n(f):x \longmapsto \frac{1}{n+1}\sum_{k=0}^nS_k(f)(x)
\] 
converge uniformément vers $f \in  \mathcal  C^0_{2\pi}(\R, \C)$

\paragraph{Un lemme.}
Pour $x \not \in  2\pi\Z$, \[
    S_n(x)= \sum_{k=-n}^{k=n} e^{ikx}=e^{-inx} \frac{1-e^{i(2n+1)x}}{1-e^{ix}}= \frac{\sin\left( \frac{2n+1}{2}x \right) }{\sin\left( \frac{x}{2} \right) }
\] 
et cette expression se prolonge par continuité sur $2\pi\Z$. On note $s_n=\sigma_n(\id)$ de sorte que \[
    s_n(x)=\frac{1}{n+1}\Im\left( \frac{e^{i\frac{x}{2}}}{\sin \frac{x}{2}}  \sum_{k=0}^{n} e^{ikx}\right) =\frac{1}{n+1} \frac{\sin^2 (\frac{n+1}{2}x)}{\sin^2 \frac{x}{2}}
\] 
qu'on prolonge encore sur $2\pi\Z$.

\paragraph{Preuve du théorème.}
On a: \[
    \sigma_n(f)(x)=\frac{1}{2\pi}\int_0^{2\pi} \frac{f(t)}{n+1}\sum_{l=0}^n\sum_{k=-l}^l e^{ik(x-t)} \diff t=\frac{1}{2\pi}\int_0^{2\pi} f(t)s_n(x-t)\diff t
\] 
Pour $f\equiv 1$, on a  $\sigma_n(f)\equiv 1$ donc  \[
    \frac{1}{2\pi}\int_0^{2\pi} s_n(x-t)\diff t=1
\] 
Pour $f \in  \mathcal  C^0_{2\pi}(\R, \C)$ quelconque, on a \[
    \sigma_n(f)(x)-f(x)=\frac{1}{2\pi}\int_0^{2\pi}(f(t)-f(x))s_n(x-t)\diff t
\] 
Soit $\epsilon>0$.  $f$ est continue périodique donc uniformément continue (Heine + périodicité) sur  $\R$ et il existe $\delta>0$ tel que  $|u-v|\leq \delta \implies  |f(u)-f(v)|\leq \epsilon'$. Dans ce cas, \[
    \sigma_n(f)(x)-f(x)=\frac{1}{2\pi} \int_{-\pi}^\pi (f(x-u)-f(x))s_n(u)\diff u
\] 
et on peut séparer l'intégrale en trois intégrales adaptées à l'écriture $[-\pi,\pi]=[-\pi,-\delta]\cup[-\delta, \delta]\cup [\delta, \pi]$. Sur $[-\delta, \delta]$,  \[
    \left| \frac{1}{2\pi}\int_{-\delta}^\delta (f(x-u)-f(x))\underbrace{s_n(u)}_{\geq 0}\diff u\right|\leq \frac{1}{2\pi}\int_{-\delta}^\delta \underbrace{|f(x-u)-f(x)|}_{\leq \epsilon'} s_n(u)\diff u\leq \epsilon'
\] 
et sur $[\delta, \pi]$ (le troisième intervalle se traite identiquement), \[
    \left| \frac{1}{2\pi}\int_{\delta}^\pi (f(x-u)-f(x)) s_n(u)\diff u \right|\leq \frac{2 \|f\|_{\infty}}{2\pi} \frac{\pi-\delta}{n+1} \cdot \frac{1}{\sin^2  \frac{\delta}{2}} \xrightarrow[n\to+\infty]{}0
\] 
donc $\leq \epsilon'$ APCR. Ainsi, APCR, $\|\sigma_n(f)-f\|_\infty \leq 3\epsilon'=\epsilon$ donc \[
    \sigma_n(f) \xrightarrow[\R]{\text{CVU}}f
\] 

\subsection{Weierstrass trigonométrique}

Pour tout $f$, $\sigma_n(f)$ est un polynôme trigonométrique. Le théorème de Féjer permet alors de montrer le théorème de Weierstrass trigonométrique\index{Weierstrass trigonométrique (théorème de -- )}: toute fonction continue $2\pi$-périodique est limite d'une suite de polynômes trigonométriques. Ainsi, \begin{itemize}
    \item dans $ \C, \quad  \bar{\Vect(e_k, k \in  \Z)}=\mathcal  C^0_{2\pi}(\R, \C)$
    \item dans $ \R, \quad  \bar{\Vect(\sin\circ (k\id), \cos \circ (k\id), k \in  \Z)}=\mathcal  C^0_{2\pi}(\R, \R)$
\end{itemize}
Dans les deux cas, la famille est orthogonale dénombrable. On obtient une famille totale en normalisant.

\section{Théorème de Müntz-Szász}
\index{Muntz-Szasz@Müntz-Szász (théorème de -- )}

On se place dans $E=\mathcal  C^0([0, 1], \R)$ muni du produit scalaire usuel \[
\scalar{f}{g} =\int_0^1fg
\] 
On note $\mathcal  F=(t \longmapsto t^{\lambda_i})_{i \in  \N}$ pour $(\lambda_i)_{i \in  \N}$ une suite strictement croissante qui tend vers $+\infty$\footnote{Il s'agit d'une version plus faible du théorème pour lequel on ne suppose pas $\lambda_i \to  +\infty$}

On note $E_n=\Vect(t\longmapsto t^{\lambda_i}, i \in  \llbracket 1, n \rrbracket )$ et $d_n(t^k)=d(t^k,E_n)$.

On va montrer que $\bar{\Vect(\mathcal  F)}=\mathcal  C^0([0, 1], \R)$ si et seulement si $\sum\frac{1}{\lambda_i}$ diverge. Puisque $\bar{\R[X]}=\mathcal  C^0([0, 1], \R)$, on va en fait montrer que $\forall  k \in \N, t^k \in  \bar{\Vect(\mathcal  F)} \iff  \sum \frac{1}{\lambda_i}$ DV.

On a $\scalar{t^{\lambda_i}}{t^{\lambda_j}} = \dfrac{1}{\lambda_i+\lambda_j+1}$ donc \[
    G_n\defeq G(\mathcal  F_n)= \begin{pmatrix}
        \displaystyle \frac{1}{\lambda_1+\lambda_1+1} & \cdots  & \displaystyle\frac{1}{\lambda_1+\lambda_n+1} \\
        \vdots & \ddots & \vdots \\
        \displaystyle \frac{1}{\lambda_n+\lambda_1+1} & \cdots  & \displaystyle\frac{1}{\lambda_n+\lambda_n+1}
    \end{pmatrix}
\] 
On va calculer son déterminant. On pose \[
    \Delta_n(X)= \begin{vmatrix}
        \displaystyle \frac{1}{\lambda_1+\lambda_1+1} & \cdots  & \displaystyle\frac{1}{\lambda_1+X+1} \\
        \vdots & \ddots & \vdots \\
        \displaystyle \frac{1}{\lambda_n+\lambda_1+1} & \cdots  & \displaystyle\frac{1}{\lambda_n+X+1}
    \end{vmatrix}= \frac{P_n(X)}{(\lambda_1+X+1)\cdots (\lambda_n+X+1)}
\] 
avec $\deg P_n\leq n-1$. Pour $x=\lambda_1, \cdots , \lambda_{n-1}$, $P_n(x)$ est nul car deux colonnes sont identiques. On a donc $P_n(X)=A_n(X-\lambda_1)\cdots (X-\lambda_{n-1})$ où $A_n=\dom P_n$. Une décomposition en éléments simples donne \[
    \frac{A_n(X-\lambda_1)\cdots (X-\lambda_{n-1})}{(\lambda_1+X+1)\cdots (\lambda_n+X+1)}= \frac{A_n(-\lambda_{n}-1-\lambda_1)\cdots (-\lambda_n-1-\lambda_{n-1})}{(\lambda_1-\lambda_n)\cdots (\lambda_{n-1}-\lambda_n)(\lambda_n+X+1)}+\text{ autres pôles }
\] 
or en décomposant par rapport à la dernière colonne, \[
    \Delta_n(X)=\frac{\Delta_{n-1}(X)}{\lambda_n+X+1}+\text{ autres pôles }
\] 
donc par unicité des décompositions en éléments simples, \[
    \Delta_{n-1}= \frac{A_n(-1)^{n-1}(\lambda_1+\lambda_n+1)\cdots (\lambda_{n-1}+\lambda_n+1)}{(\lambda_n-\lambda_{n-1})\cdots (\lambda_n-\lambda_1)(-1)^{n-1}}
\] 
soit encore \[
    \Delta_n(X)= \frac{(\lambda_n-\lambda_{n-1})\cdots (\lambda_n-\lambda_1)}{(\lambda_1+\lambda_n+1)\cdots (\lambda_{n-1}+\lambda_n+1)}\Delta_{n-1} \frac{(X-\lambda_1)\cdots (X-\lambda_{n-1})}{(\lambda_1+X+1)\cdots (\lambda_n+X+1)}
\] 
et si on note $\delta_n=\Delta_n(\lambda_n)$, on a  \[
    \delta_n=\frac{(\lambda_n-\lambda_1)^2 \cdots (\lambda_n-\lambda_{n-1})^2 }{(\lambda_1+\lambda_n+1)^2 \cdots (\lambda_{n-1}+\lambda_n+1)^2 }\cdot \frac{\Delta_{n-1}}{2\lambda_n+1} %=\frac{\displaystyle\prod_{i<j}(\lambda_j-\lambda_i)^2 }{\displaystyle\prod_{i<j}(\lambda_j+\lambda_i+1)^2 \prod_{i}(2\lambda_i+1)}
\] 
et d'après la formule de Gram, \[
    d_n(t^k)^2 = \frac{\det(G(t^{\lambda_1}, \cdots , t^{\lambda_n}, t^k))}{\delta_n}= \frac{(k-\lambda_1)^2\cdots (k-\lambda_n)^2 }{(k+\lambda_1+1)^2 \cdots (k+\lambda_n+1)^2 } \cdot \frac{1}{2k+1}
\] 
Il s'agit de voir si $d_n(t^k) \xrightarrow[n\to+\infty]{}0$. On écrit \[
    d_n(t^k)^2 =\frac{1}{2k+1} \left( \cfrac{1-\cfrac k{\lambda_1}}{1+\cfrac{k+1}{\lambda_1}} \right) ^2  \cdots 
    \left( \cfrac{1-\cfrac k{\lambda_n}}{1+\cfrac{k+1}{\lambda_n}} \right) ^2
\] 
de sorte que \[
    \ln(d_n(t^k)^2 )= \ln\left( \frac{1}{2k+1} \right) + 2 \sum_{i=1}^{n} \ln\left( \cfrac{1-\cfrac k{\lambda_i}}{1+\cfrac{k+1}{\lambda_i}} \right) 
\] 
or \begin{align*}
    \ln\left( \cfrac{1-\cfrac k{\lambda_i}}{1+\cfrac{k+1}{\lambda_i}} \right) &= \ln \left( \left( 1-\frac{k}{\lambda_i} \right)  \left( 1-\frac{k+1}{\lambda_i}+o\left( \frac{1}{\lambda_i} \right)  \right) \right) \\ &=\ln\left( 1-\frac{2k+1}{\lambda_i}+o\left( \frac{1}{\lambda_i} \right)  \right) \\&\underset{+\infty}\sim -\frac{2k+1}{\lambda_i}
\end{align*}
donc \[
    d_n(t^k) \xrightarrow[n\to+\infty]{}0 \iff  \sum \frac{1}{\lambda_i}\text{ DV }
\] 

\section{Endomorphismes symétriques}

\begin{defprop}
    \Hyp $(E, \scalar{\;}{\;} )$ est un espace préhilbertien réel
    \begin{concenum}
    \item On dira que $f \in  \mathcal  L(E)$ est \textbf{symétrique}\index{endomorphisme symétrique} ou \textbf{autoadjoint}\index{endomorphisme autoadjoint} si $\forall  x, y \in  E, \scalar{f(x)}{y} = \scalar{x}{f(y)} $ 
    \item On note $\mathrm S(E)$ l'ensemble des endomorphismes symétriques de  $E$
    \item  $\mathrm S(E)$ est un sev de  $\mathcal  L(E)$
    \end{concenum}
\end{defprop}

\begin{ex}
Les projecteurs orthogonaux sont symétriques
\end{ex}

\begin{rem}
    Si $f \in  \mathrm  S(E)$ et $\mathcal  B=(e_1, \cdots , e_n)$ est une base orthonormée de $E$, alors  \[
        \mathcal  M_{\mathcal  B}(f)= \begin{pmatrix}
            \scalar{f(e_1)}{e_1} & \cdots  & \scalar{f(e_n)}{e_1} \\
            \vdots & \ddots & \vdots \\
            \scalar{f(e_1)}{e_n} & \cdots  & \scalar{f(e_n)}{e_n} 
        \end{pmatrix} \in  \mathrm S_n(\R)
    \] 
\end{rem}

\begin{prop}
    \Hyp $f \in  \mathrm S(E)$ en dimension finie, $F$ un sev stable par  $f$
    \Conc  $F^\perp$ est stable par  $f$
\end{prop}

\begin{proof}
     \[
         \forall  x \in  F, \forall  y \in  F^\perp, \quad  \scalar{x}{f(y)} =\scalar{f(x)}{y} =0 \qquad  \text{ ie } f(y) \in  F^\perp
    \] 
\end{proof}

\begin{thm}[Spectral\index{théorème spectral}]
    \Hyp $E$ euclidien,  $f \in  \mathrm  S(E)$
    \begin{concenum}
    \item $\chi_f(X)$ est scindé sur  $ \R$
    \item Les $E_{\lambda}(f)$ sont en somme directe orthogonale
    \item $\displaystyle E=\bigoplus_{\lambda \in  \Sp(f)}E_{\lambda}(f)$. En particulier, $f$ est diagonalisable dans une base orthonormée.
    \end{concenum}
\end{thm}
\begin{proof}~
\begin{enumerate}
    \item On note $\mathcal  B$ une base orthonormée de $E$,  $A=\mathcal  M_{\mathcal  B}(f) \in  \mathrm S_n(\R)$. Soit $ \lambda \in  \Sp_{\C}(A)$ et $X \in \C^n$ un vecteur propre associé. On a $\bar{X}^\top AX=\lambda \bar{X}^\top X\in \R_+$ Puis, $X^\top A \bar{X}=\bar{\lambda} \bar{X}^\top X = \bar{\bar{X}^\top AX}$ donc $ \lambda=\bar\lambda$ et $\lambda \in  \R$. On a donc $\Sp_{\C}(A)\subset \R$. Puis, $\chi_A$ est scindé en tant que polynôme complexe, et a ses racines dans $\R$ donc scindé dans $\R[X]$.
    \item Si $x \in  E_{\lambda}(f), y \in  E_{\mu}(f)$, \[
            \scalar{f(x)}{y} =\lambda \scalar{x}{y} = \scalar{x}{f(y)} = \mu \scalar{x}{y} \implies \lambda=\mu \text{ ou } \scalar{x}{y} =0
    \] 
\item On note $F$ la somme directe des espaces propres, et on suppose  $F\neq E$.  $F$ est stable par  $f$ donc  $F^\perp$ aussi,  $E=F\oplus F^\perp$, et  $f_{|F^\perp}$ est un endomorphisme symétrique en dimension finie. Son polynôme caractéristique est scindé, donc admet une racine et cette racine est une valeur propre, ce qui est absurde car $F^\perp$ ne contient aucun espace propre. Donc,  $F=E$, ce qui conclut.
\end{enumerate}
\end{proof}

\begin{rem}
    Cela montre que les matrices réelles symétriques sont diagonalisables dans une base orthonormée pour le produit scalaire usuel de $ \mathcal  M_{n,1}(\R)$.
\end{rem}

\section{Groupe orthogonal}

On se place dans $E$ un espace euclidien.

 \begin{thmdef}
\begin{enumerate}
    \item $f \in  \mathcal  L(E)$ est un automorphisme \textbf{orthogonal}\index{automorphisme orthogonal} si il satisfait l'une des deux deux propriétés équivalentes suivantes \begin{enumerate}
        \item $ \forall  x, y \in  E, \quad  \scalar{f(x)}{f(y)}= \scalar{x}{y}$
        \item $\forall  x \in  E, \quad  \|f(x)\|=\|x\|$
    \end{enumerate}
\item On note $\mathrm  O(E)$ l'ensemble des automorphisme orthogonaux. C'est un sous-groupe de $(\mathrm {GL}(E), \circ)$
\end{enumerate}
\end{thmdef}

\begin{proof}
\begin{enumerate}
    \item Le sens direct est évident avec $x=y$, et dans l'autre on applique la formule de polarisation
    \item L'injectivité est directe avec (b) donc  $\mathrm O(E)\subset \mathrm {GL}(E)$. Si $f, g$ sont orthogonaux alors  \[
            \forall  x \in  E, \quad  \|f^{-1}\circ g(x)\|=\|f\circ f^{-1}\circ g(x)\|=\|g(x)\|=\|x\|
    \] 
    donc $f^{-1}\circ g \in \mathrm O(E)$. L'identité est dans $\mathrm  O(E)$.
\end{enumerate}
\end{proof}

\begin{thm}
    \Hyp $f \in  \mathcal  L(E)$
    \Conc Il y a équivalence entre \begin{enumerate}
        \item $f \in  \mathrm   O(E)$
        \item  $ \forall  \;\mathcal  B \text{ ON }, \quad  f(\mathcal B) \text{ ON }$
        \item  $ \exists  \;\mathcal  B \text{ ON }, \quad  f(\mathcal B) \text{ ON }$
    \end{enumerate}
\end{thm}

\begin{proof}~\\
    $(1\implies 2)$ $ \scalar{f(e_i)}{f(e_j)} = \scalar{e_i}{e_j} =\delta_{i,j}$\\
    $(2\implies 3)$ direct car il existe des bases ON\\
    $(3\implies 1)$ En décomposant sur $\mathcal  B$, on trouve bien $\scalar{f(x)}{f(y)} = \scalar{x}{y} $
\end{proof}

\begin{thmdef}
\begin{enumerate}
    \item $A \in  \mathcal  M_n(\R)$ est dite orthogonale si $A^\top A=AA^\top=I_n$. On note  $\mathrm  O_n(\R)$ l'ensemble des matrices orthogonales.
    \item $\mathrm  O_n(\R)$ est un sous-groupe de $\mathrm {GL}_n(\R)$
    \item Si $f \in  \mathcal  L(E)$, il y a équivalence \begin{enumerate}
        \item $f \in  \mathrm  O(E)$
        \item $\forall  \; \mathcal  B \text{ ON }, \quad  \mathcal M_{\mathcal  B}(f) \in  \mathrm  O_n(\R)$
        \item $\exists  \; \mathcal  B \text{ ON }, \quad  \mathcal M_{\mathcal  B}(f) \in  \mathrm  O_n(\R)$
    \end{enumerate}
\end{enumerate}
\end{thmdef}

\begin{proof}
~\\ $(a\implies b)$ Si $A=\mathcal  M_{\mathcal  B}(f)$ pour $\mathcal  B$ ON, alors on calcule \[ A^\top A=( \scalar{C_i}{C_j}  )_{1\leq i,j\leq n}=( \scalar{f(e_i)}{f(e_j)}  )_{1\leq 1,j\leq n} \]\\
$(b\implies c)$ direct \\
$(c\implies a)$ Si $A^\top A=I_n$, alors on trouve bien  $\scalar{f(e_i)}{f(e_j)} =\delta_{i,j}$, donc $f(\mathcal  B)$ est ON.
\end{proof}

\begin{rem}
    Une matrice orthogonale est telle que $\det(A)^2=1 \iff  \det A=\pm1$
\end{rem}

\begin{rem}
    Avec le produit scalaire $\scalar{M}{N} = \Tr(M^\top N)$ sur $\mathcal  M_n(\R)$, on a $\forall  A \in  \mathrm  O_n(\R), \quad  \scalar{A}{A} = \|A\|^2 =n$ donc $\mathrm  O_n(\R)$ est borné. Puis si \[
    f:M\longmapsto M^\top M-I_n
    \] 
    alors $f$ est continue et  $\mathrm  O_n(\R)=f^{-1}(\left\{  0 \right\} )$ donc $\mathrm  O_n(\R)$ est compact.
\end{rem}

\section{Groupe spécial orthogonal}

\begin{thmdef}
    \begin{enumerate}
        \item $f \in  \mathrm  O(E)$ est une \textbf{rotation}\index{rotation} si $\det f=1$. On note  $\mathrm {SO}(E)$ l'ensemble des rotations de $\mathrm  O(E)$
        \item C'est un sous-groupe de $\mathrm O(E)$, qu'on appelle \textbf{groupe spécial orthogonal}\index{groupe spécial orthogonal}.
        \item Identiquement, $\mathrm {SO}_n(\R)$ est le sous-groupe de $\mathrm  O_n(\R)$ constitué des matrices orthogonales de déterminant $1$
    \end{enumerate}
\end{thmdef}

\begin{proof}
    $\mathrm  {SO}(E)=\ker f$ où $f:A\longmapsto \det A$ donc c'est un groupe.
\end{proof}

\subsection{Orientation}

Si $\mathcal  B$ et $\mathcal B'$ sont des bases orthonormées de $E$, alors  $P_{\mathcal  B, \mathcal  B'}=\mathcal M_{\mathcal  B}(h)$ avec $h:e_i\longmapsto e_i'$, alors $h \in  \mathrm O(E)$ car l'image d'une base ON est ON. On a donc $\det P_{\mathcal  B, \mathcal  B'}=\det h=\pm1$ et $P_{\mathcal  B, \mathcal  B'} \in  O_n(\R)$

\begin{dfn}
\begin{enumerate}
    \item Deux bases orthonormées ont même \textbf{orientation} si la matrice de passage de l'une à l'autre est celle d'une rotation ($\det P=1$)
    \item La relation $\mathcal  B \; \mathcal  R\; \mathcal  B' \iff  \det P_{\mathcal  B, \mathcal B'}=1$ est une relation d'équivalence sur l'ensemble des bases orthonormées, il y a deux classes d'équivalences
    \item Un espace euclidien est dit \textbf{orienté} lorsqu'on a choisi l'une de ces deux classes, qui sera alors la classe des bases orthonormées \textbf{directes} 
    \item Si $H$ est un hyperplan,  $u$ un vecteur unitaire orthogonal à  $H$, et  $\mathcal  B=(e_1, \cdots , e_{n-1})$ une base orthonormée de $H$, alors $\mathcal  B$ est directe si $(e_1, \cdots , e_{n-1}, u)$ est une base directe de $E$. On dira que  $H$ est orienté par  $u$.
\end{enumerate}
\end{dfn}

\begin{prop}
    \Hyp $(E, \scalar{\;}{\;} )$ euclidien orienté, $u \in  \mathcal  L(E)$
    \Conc Il y a équivalence entre \begin{enumerate}
        \item $u \in  \mathrm {SO}(E)$
        \item $\forall \;\; \mathcal  B, \quad  u(\mathcal  B)$ orthonormée directe
        \item $\exists \;\; \mathcal  B, \quad  u(\mathcal  B)$ orthonormée directe
        \item $\forall \;\; \mathcal  B$ orthonormée directe, $\mathcal M_{\mathcal  B}(u) \in  \mathrm  {SO}_n(\R)$
        \item $\exists \;\; \mathcal  B$ orthonormée directe, $\mathcal M_{\mathcal  B}(u) \in  \mathrm  {SO}_n(\R)$
    \end{enumerate}
\end{prop}

\begin{proof}
Facile
\end{proof}

\subsection{Le cas du plan}

On se place dans $E=\R^2 $ muni du produit scalaire canonique et de l'orientation usuelle. \begin{align*}
    A= \begin{pmatrix}
        a&b\\c&d
    \end{pmatrix} \in  \mathrm O_2(\R) & \iff  \begin{dcases}
    a^2 + c^2 = 1 \\ b^2 + d^2 = 1 \\ ab+cd=0
    \end{dcases} \\
    & \iff \exists \theta, \theta' \in  \R , \begin{dcases}
        a = \cos \theta & c=\sin \theta \\ b=\sin \theta' & d=\cos \theta' \\ ab+cd=0
    \end{dcases} \\
    & \iff  \exists \theta, \theta' \in  \R , \begin{dcases}
        a = \cos \theta & c=\sin \theta \\ b=\sin \theta' & d=\cos \theta' \\ \theta+\theta'\equiv 0\pmod\pi
    \end{dcases} \\
    &\iff  \exists  \theta \in  \R, \quad  A = \begin{pmatrix}
        \cos \theta & -\sin \theta \\ \sin \theta & \cos \theta
    \end{pmatrix}
    \quad  \text{ ou } \quad A = \begin{pmatrix}
        \cos \theta & \sin \theta \\ \sin \theta & -\cos \theta
    \end{pmatrix}
\end{align*}
et \[
    A \in  \mathrm {SO}_2(\R) \iff  A= \begin{pmatrix}
        \cos \theta & -\sin \theta \\
        \sin \theta & \cos \theta
    \end{pmatrix}\defeq R_{\theta}
\] 
On a par ailleurs $R_{\theta}R_{\theta'}=R_{\theta+\theta'}$ donc $\mathrm {SO}_2(\R)$ est abélien

\begin{prop}
    Soit $u \in  \mathrm {SO}(\R^2)$. La matrice $\mathcal  M_{\mathcal B}(u)$ est indépendante de la base orthonormée directe choisie et \[
        \exists  !\theta \in  [0, 2\pi[, \quad  \mathcal  M_{\mathcal  B}(u)=R_{\theta}
    \] 
    On dit alors que $u$ est la rotation d'angle $\theta$
\end{prop}

\begin{proof}
    $\mathrm {SO}_2(\R)$ est abélien donc les matrices de passage n'ont pas d'influence.
\end{proof}

\begin{rem}[Angle orienté\index{angle orienté}]
    Pour $x, y \in  E$ ($E$ euclidien orienté de dimension  $2$) et $\mathcal  B$ orthonormée directe, on a (calculer explicitement) \[
        \scalar{x}{y} ^2 + \det_{\mathcal  B}(x, y)^2 = \|x\|^2 \|y\|^2 
    \] 
    donc si $x, y\neq 0$, il existe un unique $\theta \pmod{2\pi}$ tel que \[
    \begin{dcases}
        \scalar{x}{y} = \|x\|\|y\|\cos \theta \\
        \det_{\mathcal  B}(x, y)= \|x\|\|y\|\sin \theta
    \end{dcases}
    \] 
    Ce réel $\theta$ est alors appelé angle orienté  $\widehat{(x, y)}$
\end{rem}

\section{Produit vectoriel}
\index{produit vectoriel}

\begin{thm}[Théorème de représentation de Riesz\index{Riesz (théorème de représentation de -- )}]
\Hyp $E$ euclidien
\Conc $\forall \varphi \in  E^\star, \quad  \exists  ! a \in  E, \quad  \forall  x \in  E, \quad  \varphi(x)=\scalar{a}{x} $
\end{thm}

\begin{proof}
    \[
    \begin{array}{rrcl}
        \psi:& E & \longrightarrow &  E^\star \\
             & a & \longmapsto & \displaystyle (x \longmapsto \scalar{a}{x} )
    \end{array}
    \] 
    Est une AL injective et $\dim E=\dim E^\star$ donc c'est un isomorphisme.
\end{proof}

\begin{thmdef}
\Hyp $E$ est un espace euclidien orienté de dimension  $3$
 \begin{concenum}
 \item $u, v, w \in  E$. La quantité $\det_{\mathcal  B}(u,v,w)$ ne dépend pas de $ \mathcal  B$ OND. On la note $[u, v, w]$
 \item  $u, v \in  E. \exists !w \in  E, \quad  \forall  x \in  E, [u, v, x]= \scalar{w}{x} $. On note alors $w=u\wedge v$
 \item  $(u,v)\longmapsto u\wedge v$ est bilinéaire antisymétrique
 \item $u\wedge v=0 \iff  u,v$ colinéaires
 \item $u\wedge v\perp u,v$
 \item Si  $(e_1, e_2, e_3)$ OND alors  $e_1\wedge e_2=e_3, e_2\wedge e_3=e_1, e_3\wedge e_1=e_2$
 \item Si  $(e_1, e_2, e_3)$ ON alors cette base est directe  si et seulement si $e_1\wedge e_2=e_3$
\end{concenum}
\end{thmdef}
